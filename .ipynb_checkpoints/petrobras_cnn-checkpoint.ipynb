{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels already calculated\n",
      "Total number of features 447\n",
      "train_split = 0.7\n",
      "Shape of x, y train/cv/test (2783, 447) (2783,) (1193, 447) (1193,) (995, 447) (995,)\n",
      "('volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_17', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_23', 'rsi_24', 'rsi_25', 'rsi_26', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_21', 'mfi_22', 'mfi_23', 'mfi_24', 'mfi_25', 'mfi_26', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_13', 'roc_14', 'roc_15', 'roc_16', 'roc_17', 'roc_18', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_20', 'cmf_21', 'cmf_23', 'cmf_24', 'cmf_25', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_18', 'cmo_19', 'cmo_20', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_24', 'cmo_25', 'cmo_26', 'close_sma_6', 'close_sma_7', 'close_sma_8', 'open_sma_6', 'open_sma_7', 'open_sma_8', 'open_sma_9', 'hma_0', 'hma_8', 'hma_9', 'hma_10', 'hma_11', 'hma_12', 'hma_13', 'hma_14', 'hma_15', 'hma_16', 'hma_17', 'hma_18', 'hma_19', 'hma_20', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_11', 'trix_12', 'trix_13', 'trix_14', 'trix_15', 'trix_16', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 103 104 105 107 108 109 111 112 113 114 115\n",
      " 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133\n",
      " 134 153 154 155 156 216 224 225 226 227 228 229 230 231 232 233 234 235\n",
      " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
      " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289\n",
      " 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307\n",
      " 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325\n",
      " 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343\n",
      " 344 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
      " 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397\n",
      " 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415\n",
      " 416 417 418 419 420 421 422 423 424 425 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 445 446]\n",
      "****************************************\n",
      "330 ('volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_24', 'rsi_25', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_22', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_17', 'roc_18', 'roc_19', 'roc_21', 'roc_23', 'roc_24', 'roc_25', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_22', 'cmf_23', 'cmf_25', 'cmf_26', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_17', 'cmo_19', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_24', 'cmo_25', 'cmo_26', 'open_sma_12', 'ema_17', 'hma_8', 'hma_9', 'hma_10', 'hma_11', 'hma_12', 'hma_13', 'hma_14', 'hma_15', 'hma_16', 'hma_17', 'hma_18', 'hma_19', 'hma_20', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_11', 'trix_12', 'trix_13', 'trix_14', 'trix_15', 'trix_16', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'bb_9', 'bb_10', 'bb_11', 'bb_12', 'bb_13', 'bb_14', 'bb_15', 'bb_16', 'bb_17', 'bb_18', 'bb_19', 'bb_20', 'bb_21', 'bb_22', 'bb_23', 'bb_24', 'bb_25', 'bb_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  5   6   7   8   9  10  11  12  13  14  15  16  19  20  21  22  24  25\n",
      "  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44\n",
      "  45  46  47  48  49  50  51  52  54  55  56  57  58  59  60  61  62  64\n",
      "  69  70  71  72  73  74  75  76  77  78  80  81  82  84  86  87  88  90\n",
      "  91  92  93  94  96  97  98 100 101 102 103 106 107 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 122 124 126 127 128 129 130 131 159 185 224\n",
      " 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\n",
      " 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
      " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
      " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296\n",
      " 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
      " 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
      " 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368\n",
      " 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386\n",
      " 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404\n",
      " 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 441 442 443 444 445 446]\n",
      "common selected featues 300 ['mfi_13', 'kst_11', 'kdjk_7', 'rsv_15', 'kst_25', 'cci_18', 'eom_16', 'cci_22', 'kst_8', 'wr_15', 'rsi_15', 'mfi_9', 'kst_9', 'dmi_23', 'roc_23', 'cmf_19', 'fi_15', 'trix_9', 'eom_14', 'rsv_9', 'kdjk_18', 'rsi_16', 'trix_6', 'cmo_8', 'trix_8', 'kst_16', 'kst_24', 'wr_18', 'dpo_17', 'dpo_22', 'trix_11', 'rsv_8', 'hma_9', 'rsv_13', 'wr_8', 'cmo_17', 'cci_26', 'dpo_6', 'hma_11', 'dmi_16', 'dmi_7', 'hma_10', 'cmo_12', 'wr_23', 'mfi_20', 'dpo_10', 'dpo_26', 'rsv_11', 'eom_17', 'cmf_16', 'kst_20', 'kst_23', 'cmf_23', 'roc_9', 'roc_13', 'dpo_19', 'kst_15', 'rsi_9', 'rsi_20', 'cci_15', 'cci_19', 'kdjk_9', 'wr_16', 'mfi_22', 'bb_7', 'fi_17', 'kst_13', 'kdjk_22', 'rsi_14', 'eom_13', 'wr_7', 'kdjk_6', 'roc_19', 'trix_19', 'cci_24', 'dpo_23', 'dmi_24', 'fi_9', 'rsv_12', 'rsv_21', 'roc_11', 'fi_25', 'rsi_7', 'cci_25', 'wr_13', 'cmo_11', 'cmo_14', 'mfi_7', 'dpo_9', 'dpo_12', 'kst_6', 'fi_24', 'kdjk_20', 'wr_21', 'dpo_18', 'fi_13', 'cci_20', 'kst_17', 'wr_22', 'cmo_21', 'dpo_15', 'dpo_20', 'cci_7', 'rsi_12', 'wr_26', 'kdjk_13', 'eom_24', 'dpo_25', 'eom_25', 'dpo_24', 'fi_11', 'cmf_13', 'trix_12', 'hma_14', 'kst_10', 'rsi_11', 'eom_22', 'trix_15', 'dpo_14', 'cci_9', 'rsi_25', 'trix_18', 'hma_8', 'cci_10', 'dmi_22', 'cmo_22', 'trix_17', 'fi_22', 'rsv_16', 'trix_13', 'wr_11', 'dmi_19', 'fi_21', 'trix_26', 'fi_26', 'rsv_6', 'rsv_20', 'wr_25', 'cmo_10', 'kdjk_12', 'eom_21', 'roc_10', 'fi_12', 'dmi_10', 'kdjk_10', 'eom_23', 'hma_12', 'rsi_21', 'mfi_18', 'rsi_22', 'cmo_19', 'kdjk_11', 'hma_18', 'trix_14', 'wr_10', 'fi_7', 'fi_18', 'bb_8', 'kdjk_24', 'hma_16', 'mfi_17', 'rsv_18', 'trix_24', 'cmo_13', 'cci_16', 'kdjk_14', 'rsv_19', 'kdjk_26', 'hma_17', 'fi_6', 'rsv_10', 'eom_15', 'cmo_15', 'dmi_11', 'cmf_17', 'trix_23', 'dpo_7', 'dmi_6', 'cmo_6', 'dmi_21', 'rsv_17', 'wr_19', 'kst_18', 'dmi_13', 'kst_21', 'rsi_24', 'fi_8', 'rsv_26', 'kdjk_23', 'trix_21', 'mfi_8', 'cci_17', 'hma_15', 'trix_10', 'mfi_6', 'dpo_21', 'bb_6', 'hma_20', 'wr_6', 'rsv_22', 'eom_26', 'hma_13', 'rsi_19', 'roc_18', 'eom_20', 'wr_12', 'fi_19', 'mfi_19', 'cmf_9', 'kst_19', 'rsv_7', 'trix_22', 'fi_23', 'roc_6', 'cci_6', 'kdjk_21', 'cmf_14', 'kst_7', 'roc_21', 'cmf_6', 'dmi_12', 'dmi_15', 'kst_14', 'cmo_25', 'cci_12', 'rsi_8', 'dpo_8', 'wr_17', 'cmo_24', 'rsi_6', 'cmf_10', 'dmi_25', 'rsi_13', 'cmo_7', 'hma_19', 'kdjk_19', 'kdjk_25', 'cmo_23', 'mfi_10', 'trix_20', 'dmi_20', 'rsi_10', 'kdjk_15', 'kdjk_17', 'volume', 'wr_24', 'cci_23', 'kst_22', 'wr_20', 'eom_19', 'dmi_8', 'wr_14', 'cci_14', 'fi_14', 'rsv_24', 'roc_24', 'trix_16', 'cmo_26', 'cmf_12', 'roc_15', 'mfi_12', 'roc_8', 'dpo_16', 'roc_17', 'kst_26', 'cmf_7', 'roc_7', 'kst_12', 'mfi_16', 'cci_21', 'dmi_18', 'rsv_25', 'trix_7', 'rsv_23', 'mfi_14', 'cmf_18', 'cci_8', 'cmf_8', 'wr_9', 'cci_13', 'dmi_26', 'kdjk_16', 'dpo_11', 'eom_18', 'mfi_15', 'roc_14', 'cci_11', 'dmi_17', 'fi_10', 'roc_25', 'dpo_13', 'rsv_14', 'fi_16', 'dmi_14', 'kdjk_8', 'cmo_9', 'dmi_9', 'cmf_25', 'fi_20', 'trix_25']\n",
      "[7, 9, 11, 12, 14, 15, 16, 19, 20, 21, 22, 24, 25, 27, 28, 29, 31, 32, 33, 34, 36, 37, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 55, 59, 60, 61, 62, 64, 69, 72, 73, 74, 76, 81, 82, 84, 86, 90, 93, 97, 98, 100, 101, 103, 107, 111, 113, 115, 116, 117, 118, 119, 120, 122, 124, 126, 127, 130, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 252, 253, 254, 255, 257, 258, 259, 261, 262, 264, 267, 268, 269, 270, 271, 272, 274, 276, 277, 278, 279, 280, 282, 283, 285, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 317, 318, 319, 321, 322, 325, 326, 327, 328, 330, 331, 334, 336, 337, 338, 339, 342, 343, 344, 363, 364, 365, 366, 368, 369, 370, 372, 374, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 404, 406, 408, 409, 410, 412, 413, 414, 415, 416, 417, 419, 421, 424, 425, 433, 434, 435, 436, 437, 440, 441, 442, 443, 444, 445, 446]\n",
      "Shape of x, y train/cv/test (2783, 225) (2783,) (1193, 225) (1193,) (995, 225) (995,)\n",
      "percentage of class 0 = 5.928853754940711, class 1 = 6.072583542939274\n",
      "real class weights are [5.62222222 5.48915187 0.37879407] [0 1 2]\n",
      "value_counts (array([0, 1, 2]), array([ 165,  169, 2449], dtype=int64))\n",
      "Test sample_weights\n",
      "[0 1 2 2 0 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 0 2 2 2 2 2 2]\n",
      "[5.62222222 5.48915187 0.37879407 0.37879407 5.62222222 0.37879407\n",
      " 0.37879407 0.37879407 0.37879407 0.37879407 0.37879407 0.37879407\n",
      " 0.37879407 0.37879407 0.37879407 5.48915187 0.37879407 0.37879407\n",
      " 0.37879407 0.37879407 0.37879407 0.37879407 0.37879407 5.62222222\n",
      " 0.37879407 0.37879407 0.37879407 0.37879407 0.37879407 0.37879407]\n",
      "y_train (2783, 3)\n",
      "final shape of x, y train/test (2783, 15, 15, 3) (2783, 3) (995, 15, 15, 3) (995, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA00AAANLCAYAAACdWnYxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOzdebidZX0v/N8ve0hCRkKYNAwKCqIoKghOiNBjHY5aldqe12I91ta+ailaPb5eVItvW+tAB1vUY7UOr9XiwPCKFkWtKA4gbYAEUVQUwhAgEkLmYWff54/1pOzGve+E3Alr7/D5XNe6svbzrO967mftve69vs+z1k6WUgIAAIDxTev3AAAAACYzpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCadkFmfq/fYxgrM/8yM2/NzLXbLT85Mxdn5khmnr7duq9k5qrM/NJ2yz+dmTdm5vWZ+bHMHNqF8dycmQt3bW926v5fkZlLusv3MvMJe2pbMJVMoblpemZ+NjN/lplXZebh3fKnZOa13eW6zHxJt/yQzPxmZv4oM3+YmX+8C2M5PDOv3x37VdnG+zLzx93cdFFmzt+T24OpZKrMT2PWn56ZJTOP777+b5n5H5m5tPv31HEyX9yVeeZBmp9+s5s/R7ftEw+M0rQLSilP6/cYtnNJRDxlnOXLIuJVEfGZcda9LyLOGGf5pyPi6Ig4NiJmRsRrds8Qd6tfRMSzSimPj4g/j4h/7PN4YFKYQnPT70XEvaWUIyPibyPiPd3y6yPi+FLKcRHx3Ij4cGYORsRIRPxJKeUxEXFSRLw+M4/Z46N/4L4WEY/r5qafRMTb+jwemDSm0PwUmTknIs6MiKvGLP5lRLywlHJsRPxuRHxqu8xLI2LcAjZJXB8RL42Ib/d7IFOV0rQLth2VyMxTMvNbmfm5zPxJZr67Owvyg+5IxBHd7V7YHU29JjO/npkHdsv3z8yvdWeDPpyZt2w7Q5OZv9Pdz7XduoGJxlNKubKUsnyc5TeXUpZExOg4674REWvGWf6vpRMRP4iIRZXHYXZmfrzb1yWZ+bJxbnNxd0Tmh5n5B92ygcz8RHc2a2lmvrFbfmZm3tDd1/mV/f1eKeXe7ssra2OEh5KpMjdFxIsj4pPd9S9ExGmZmaWU9aWUkW75jIgo3f0sL6Us7q6viYgfRcTDK4/Dkd3+XNftwxHbrT88M6/o1i3OzKd1yw/OzG93+3Z9Zj5zovlqgv29bMz4zU0wxhSanyJ6B2TfGxEbx9z+mlLKHd2XP4yIGZk5vdvu7Ih4U0T8xU48Dv2an35USrlxR+OjopTi8gAvEbG2+/eUiFgVEQdHxPSIuD0i3tmt++OI+Lvu+r4Rkd3110TEX3fXz4uIt3XXnxu9FwgLI+Ix0TsCMtSt+2BEvHJnxzXO8k9ExOnjLD8lIr40QWYoIhZHxDMr23vPtn3ctp/dvzdHxMLu+oLu35nRO8qxX0Q8OSK+NiY3v/v3joiYPnbZTuzzmyPio/3+mXBxmQyXqTI3dXPBojFf3zRmzjgxei9I1kbES8a5r8OjdxZ9bmV7V23LRq987dPlru+W7RMRM7rrj4qIf++u/0lEnN1dH4iIORPNVzuxz5dExO/0+2fCxWWyXKbQ/PTEiLigu3559M5+b585PSK+Pubrv42Il4ydZyrb6+v8NNE+uez4Mhi0urp0Ryoy86aIuKxbvjQint1dXxQRn83MgyNiOHpvL4uIeEb0nmRRSvlKZm47e3Ja9J4IV2dmRK9w3L2H92N7H4yIb5dSrqjc5tci4re3fVHuP/sz1pnZfS4hIg6J3gRwY0Q8MjP/ISK+HPc/Zksi4tOZeXFEXLyjAWbms6P3Np9n7Oi28BA0meemHGfZtrNKV0XEYzPzMRHxycy8tJSysduP2RFxQUScVUpZPe4d995W8/BSykXd/W3Ljr3ZUEScl5nHRcTWiHh0t/zqiNj2Wc6LSynXZubPY/z5auKdyzw7em8p/PSObgsPUZNyfsrMadErQK+q3Oax0Tto/Jzu6+Mi4shSyhuz+3xmJdv3+Yld5+157TaNuT465uvRiP8spf8QEeeV3vtgXxu9IwsR479w2Lb8k6WU47rLUaWUc3bvsCeWmX8WEftH71Rz9abRvdCZ4H5OiV6xemop5QkRcU30jp7cGxFPiN7RjtdHxEe7yAsi4gPRm/T+I3ufZZjovh/f5V5cSrlnx3sFDzmTeW66LXoHUaJ7ns+LiJVjb1BK+VFErIuIx3W3G4peYfp0KeXCyn1PNPax3hgRd0VvHjo+ei/IopTy7Yg4OXpHvj+Vma+szFfjbzzzdyPiv0fEK0p3WBf4FZN1fpoTvTnn8sy8OXqfofxi3v/HIBZFxEXRO4N1U5d5akQ8ubv9dyLi0Zl5eWWMO7LH5ifaKE0PjnnR+yGP6H14cJvvRMTLIyIy8znROxUdEfGNiDg9Mw/o1i3IzMMejIFm5msi4tcj4n+UUn7ls1DbuSwi3jAmu+926+dF78Pe6zPz6OhNPtG993haKeWCiHh7RDypO7pzSCnlmxHxvyJifkTMnmCMh0bEhRFxRinlJw90H4H/1K+56Ytjtnd6RPxbKaVk5iO2HSzp7veoiLg5e4dh/ykiflRK+ZvaHXdnoG7LzN/o7md6Zu6z3c3mRcTybo47I3pvddm2zbtLKR/ptvek8earibadmc+NiLdGxItKKet39sEAxvWgz0+llPtKKQtLKYeXUg6P3mcTX1RK+ffs/TXML0fvrYHfHZP5UCnlYd3tnxERPymlnDLB/fdtfqKd0vTgOCciPp+ZV0Tvr69s886IeE5mLo6I50XE8ohYU0q5ISL+NCIuy8wl0fuLTAdPdOeZ+d7MvC0i9snM2zLznG75Cd3y34zeX6H64ZjMFRHx+eh9APu2zPz1btX/jogDI+L73YcN31HZr7+IiH27DyBeF/efUt/mKxEx2O3Dn0dv8onofYD78sy8Nnqft3pb9CaFf87MpdE7I/W3pZRVE2z3HdH7bNQHuzH+e2WMwMTOiT7MTdH7hb9fZv4seme0/59u+TMi4rpubrgoIl5XSvllRDw9ei8eTs37/yT58yv7dUb03hq8JCK+FxEHbbf+gxHxu5l5ZfTe+rKuW35KRFybmddExMsi4v0x/nw1kfOid6T6a90Y/3fltkDdOdGf+Wkib4iIIyPi7WPmoQN2Yb/6Mj9l5ku6/X1qRHw5M7+6C2N/SNv2ATv6IHt/dWVrKWUkM58aER8qvT+1C9A35iZgsjI/0S/+EER/HRoRn+vemrY5In6/z+MBiDA3AZOX+Ym+cKZpCsnMq6L35znHOqOUsnQPb/d/Ru/PgI713VLK6/fG7QIPTB/npg9E7617Y72/lPLxvXG7wANnfnpwtvtQoDQBAABUVN+e97GPfaypUa1YsaIlHmvWrGnKj4yM7PhGFQccsCuf79t9+RkzZuz4RhUDAxP+R9g75b777mvKDw62vftzdHRHf7yvbtasWX3dfubO/GXRiS1atKgpf9JJJ7UNYJJ729ve1jQ/zZw5s2n7Bx20/Wd3H5h169bt+EYVc+fObcrffPPNTfnW+X327HH/OOZO27BhQ1N+7dq1TfnW+XHr1q1N+U2bNu34RhUvf/nLm/KtXvOa1+y189MXvvCFprlp8+bNTdu/4447mvKtrx3mz5/flJ83b15TvvW1x+rV4/4XcDtt1aqJ/obVzml97TptWtvfeGudm5YvX96Uf/WrX92Ub3X00UdPODf563kAAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFAxWFt5zDHHPFjjmJRWr17dlL/yyiub8rNmzWrKH3TQQX3d/uBg9cdr0ttnn32a8q37v3Tp0qb8SSed1JSf7I444oh+D6HJoYce2tftP/e5z+3r9m+44Yam/Lp163bTSPpjzZo1TfnW/f/xj3/clG+dH/dmj3vc4/q6/Sc96Ul93X6/ZWZT3s/21LZ58+Y9dt/ONAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQMVgbeWyZcua7vzee+9tyt9xxx1N+ZGRkab84Ycf3pQ/6KCDmvIDAwNN+Q0bNjTlN2/e3JRfsWJFU3758uVN+WnT2o4JDA8PN+VbH78FCxY05fd2S5cubcq3zg9z585tyh9wwAFN+RtvvLEpv3Llyqb8jBkzmvKzZs1qyt93331N+cc//vFN+dbfT4OD1V+/O7T//vs35Vvnp5tuuqkpvze7++67m/IzZ85syrf+bP3oRz9qys+fP78p3/rapTW/adOmpvyWLVv6ml+3bl1TvnVuyMymfOv+77vvvk35N77xjROuc6YJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqBmsrly1b1nTn9957b1N+3333bcq3uvPOO5vyt956a1N+5cqVTfmHP/zhTfnR0dGm/PDwcFN+2rS2Tn/CCSc05deuXduUX7FiRVO+df/3dqtWrWrKDwwM9HX7y5cvb8oPDlan7x1qnR9afz7XrVvXlB8ZGWnK//CHP2zKb9iwoSn/1Kc+tSl/zz33NOVnzJjRlD/55JOb8nuz73//+0351rll7ty5TfnWubF1btu0aVNf8wsXLmzKb9mypSnf+tpr9uzZTfmjjz66KX/fffc15Vu/f62v3Wq8KgMAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgIospUy48uyzz5545U4YGRlpice6deua8o94xCOa8ocddlhTPjOb8mvWrGnKDw4ONuVnzpzZlH/rW9/alK/9bD4YWr9/rVr3/6abburvDuxhH/nIR5oeoLVr1zZtv/X5MTQ01JS/+uqrm/KrV69uyu+7775N+dbHf+XKlU35VatWNeVb9Xt+6bdvf/vbe+0DcO655zbNTQMDA03bb/3d0frcXrhwYVP+7rvvbspv2LChKd/62rX18X/HO97RlO+31rmt36/91qxZM+EOONMEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVWUqZcOWRRx458Up2KDOb8meddVZTfvr06U35oaGhvuaHh4eb8q37PzAw0JRv3f/W7Z966qltP4CT3Mknn9w0P9XmvoeC1v1/+ctf3pTftGlTU350dLSv+Vatvx9ax986v0yb1nbM9S1vecteOz/NmTPnoT259Nnf//3fN+W3bt3alG+dW6d6fmRkpCnf77n9rLPOmnBucqYJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqBmsrP/GJTzTd+cDAQFN+2rS2Tjc0NNSUz8ym/OjoaFO+lNLX7S9evLgpv3Xr1qb8pk2bmvJ33313U37NmjVN+RUrVjTlN2/e3JQ/9dRTm/KT3Utf+tKmfOv8NDhYnT53aMaMGU35fs+vrfnWx2/Lli1N+db5ffXq1U35kZGRpnzr/Ng6v7Vuf2/2jne8oynf79cO/X7t0ZpftmxZU771tcuJJ57YlG99/Fvnltb937hxY1O+dW5q3X6NM00AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFAxWFt5/vnnN915KaWv+dHR0ab8li1bmvJbt25tyreOvzU/f/78pvyCBQua8hs3bmzK33bbbU352bNnN+Xnzp3b1/zebtmyZU35zZs3N+Vb54fWfOvzo3X/R0ZGmvKt82Pr49f6/HrYwx7WlF++fHlTftGiRU351vm59ffD3qz1d2+/n5sP9fG3vvb83Oc+15RvfW61/m5Yu3ZtU/6ggw5qymdmU37mzJlN+RpnmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoGKwtvKEE05ouvObbrqpKb/PPvs05UspTfnp06f3dfvDw8NN+fvuu6+v258zZ05ft3/UUUc15ZncnvWsZzXllyxZ0pRv/fm87bbbmvLz589vym/YsKEpP21a2zG39evXN+Uzsym/devWpvzmzZub8vvtt19TvvX7d/vttzflmdjs2bOb8q1zw8yZM5vyQ0NDTfm5c+c25UdHR5vyg4PVl7Y7tHLlyqZ869zc+tr3rrvuasq3vnZr1Tq3b9y4cTeN5Fc50wQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVZSplw5T/90z9NvHIn1O77wdDv7WdmU77f45/qWh//Vh/5yEea8sPDw035K664or8PwB72ute9rq/z01TP99tUH/9Ud/755zflW+fX1atX77Xz01ve8pYp/dqplfH3V7/H37r9b37zm33d/pIlSyacm5xpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACAisHaygsuuKDtzgerd7/H8wMDA1N6+8PDw33d/rRpbZ06M/uav+yyy5ry/faZz3ym30OY1M4///ymfL9/Plvz/X5+Dg0NNeVb56ep/vgdfPDBTflWxx13XFP+3HPP3U0j2ft89atfbcr3e26Z6s+t1tdOra/9+v34teavuuqqpnwppSnf6t/+7d/22H070wQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVZSplw5VVXXTXxSmBSO/HEE7PfY9iTzE8wde3N85O5Caau2tzkTBMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQM7sk7v/XWW5vya9asaco/5jGPacrfcsstTfnDDjusKd/q5z//eVP+kY98ZFP+xhtvbMofddRRTfnrr7++Kf+4xz2uKb948eKm/JFHHtmUp27p0qVN+VJKUz4z+5pvHX+rwcG2Xz8jIyNN+enTpzflN23a1JSfPXt2U35oaKgp3++f3xNPPLEpvzebN29eU37z5s1N+eHh4Sm9/Vatz+1+zy2t2+/392/r1q1N+YGBgaZ8jTNNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQMVhbecUVVzTd+erVq5vyg4PV4e3QypUrm/IHHXRQU/6mm25qym/ZsqUpP2vWrKb88uXLm/Kt37/W7e+zzz593f6+++7blF+1alVTfm93++23N+Xnz5/flD/44IOb8q1uuOGGvm6/dX4fGBjYTSPZNStWrGjKz5w5syl/5513NuVnz57dlL/33nub8ps2bWrKv+xlL2vKT2Zz587t6/aHh4eb8pnZlG+dW1u3PzIy0pRvfe3Uavr06U351tderT8/Q0NDTfnW176llKZ8jTNNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQkaWUCVdeeOGFE6/cCXPmzGmJx8aNG5vyb3jDG5ryMJXdcsst2e8x7ElvfvObm+anViMjI035iy66aDeNBKaevXl++tnPftY0N02fPr1p++vXr2/KP+c5z2nKw1RWm5ucaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgIospfR7DAAAAJOWM00AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQ1yszv9XsMY2XmX2bmrZm5drvlJ2fm4swcyczTt1v33sz8YWb+KDP/PjPzAW7zVZl53u4Y/wT3v09mfjkzf9yN8917alswlU2V+ahb9/LMvKF7Tn9mzPJDM/Oybj66ITMP75Znd38/6daduQvjuTkzF7bs0w7u/xWZuaS7fC8zn7CntgVTyVSZmzLzDzNzaWZem5nfycxjuuWv6JZtu4xm5nHduuHM/MdubvpxZr5sF8bzK3Pk7pSZb+rm0yWZ+Y3MPGxPbm9vpTQ1KqU8rd9j2M4lEfGUcZYvi4hXRcRnxi7MzKdFxNMj4vER8biIOCEinrVnh7hLzi2lHB0RT4yIp2fm8/o9IJhspsp8lJmPioi3RcTTSymPjYizxqz+/yLifaWUx3TZu7vlr4qIQyLi6G7d+Xtw3LvqFxHxrFLK4yPizyPiH/s8HpgUpsrcFBGfKaUcW0o5LiLeGxF/ExFRSvl0KeW4bvkZEXFzKeXaLnN2RNxdSnl0RBwTEd/a88N/wK6JiOO7uekL0ds3HiClqdG2owOZeUpmfiszP9cdbXh3d2TiB91RiyO6270wM6/KzGsy8+uZeWC3fP/M/Fp3NujDmXnLtiOimfk73f1c260bmGg8pZQrSynLx1l+cyllSUSMbr8qImZExHBETI+IoYi4q7K/z+3GeF1mfmOc9RPt37PGHKG5JjPnZObBmfntbtn1mfnMCfZpfSnlm931zRGxOCIWTTRGeKiaKvNRRPx+RHyglHJvd7u7u/s+JiIGSylf65avLaWs7zL/d0T8v6WU0bGZCR6H2Zn58W5fl4x35DczL87M/8jema4/6JYNZOYnuvloaWa+sVt+5pijtBOWtVLK97btU0RcGeYpiIipMzeVUlaP+XJW9F4jbe9/RMS/jPn61RHxV11+tJTyy8rjcGBmXtS9hroueweux66fnb0zQYu7x+PF3fJZ2XvHzXXd/PRb3fJ3j5mbzq3s7zfHzKXmpl1VSnFpuETE2u7fUyJiVUQcHL3ycXtEvLNb98cR8Xfd9X0jIrvrr4mIv+6unxcRb+uuPzd6T9SFEfGY6B0RGerWfTAiXrmz4xpn+Sci4vTtlp3bjf2+iPjLyn3uHxG3RsQjuq8XdP++KiLO28H+XRK9o8oREbMjYjAi/iQizu6WDUTEnJ3Yr/kR8fOIeGS/v/cuLpPtMlXmo4i4OHpHOr8bvV/gz+2W/0ZEfCkiLozekdH3RcRAt+6e6B3R/feIuDQiHlXZ3nu27eO2/ez+vTkiFnbXt81fMyPi+ojYLyKeHBFfG5Ob3/17R0RMH7tsJ/b5zRHx0X7/TLi4TIbLVJmbumWvj4ibovd651fmmW7d47rr87vb/U30Duh+PiIOrGzvsxFxVnd9ICLmbff4DEbE3O76woj4WURkRLwsIj4y5n7mRcSCiLhxzOO0s3PTeRHxp/3+mZiKl8Fgd7q6dEcuMvOmiLisW740Ip7dXV8UEZ/NzIOjd3bnF93yZ0TESyIiSilfycxtRytPi94v8quz91GjmXH/21WaZeaR0Ztsth11+FpmnlxK+fY4Nz8pIr5dSvlFN86V49xmov37bkT8TWZ+OiIuLKXclplXR8THMnMoIi4u95/qnmisg9E7uvP3pZSfP7A9hYecyTwfDUbEo6L3AmpRRFyRmY/rlj8zem/DXRa9Fxivioh/it4LrI2llOMz86UR8bHutuP5tYj47W1flPvP/ox1Zma+pLt+SDeeGyPikZn5DxHx5bj/MVsSEZ/OzIujV/iqMvPZEfF70Xscgf9qMs9NUUr5QER8IDP/r4j404j43W3rMvPEiFhfSrm+WzTYjfW7pZQ3Zeaboncg+owJ7v7UiHhlt52t0TtYPVZGxLsy8+TovTPo4RFxYPQem3Mz8z0R8aVSyhXda6KNEfHRzPxy9A44VWXm70TE8TE5P4Yx6Xl73u61acz10TFfj0b8Z0H9h+idlTk2Il4bvbfGRfSeKOPJiPhk6d5LW0o5qpRyzm4c80si4srSexvM2ugdwT2pMpbxTlWPNe7+lVLeHb2jRTMj4srMPLorZidH70jTpzLzlTu473+MiJ+WUv5uJ/YLHuom83x0W0T8/6WULd1BmBujV1pui4hrSik/L6WMRK+gPGlM5oLu+kXR+xzmRKpzVWaeEr1i9dRSyhOid1ZrRleunhARl0fvaPNHu8gLIuID0XtR9h/di5WJ7vvxXe7FpZR7KmOEh6rJPDeNdX70zn6P9dvxX9+ad09ErI/enBTRO9P0pNh1r4jeu3qeXHqfn7orenPTT6I3/yyNiJSJ1OwAACAASURBVL/KzHd0c+RTojcv/kZEfKV2x5n5a9E7W/+iUsqm2m0Zn9L04JsXvZIQMeboRUR8JyJeHhGRmc+J3qnpiIhvRMTpmXlAt25B7t6/erIsIp6VmYPdGZ9nRcSPJrjt97vbPmLbWMa5zbj7l5lHlFKWllLeE7231xzd7cfdpZSPRO9I8oQTTWb+RXffZ010G+AB69d8dHF0R5S7zyM8Onpvu706IvbNzP27250aETeMyZzaXX9WRPykcv+XRcQbtn2Rmftut35eRNxbSlmfmUdHd6CoG8u0UsoFEfH2iHhSZk6LiENK73OV/yt6b8eZPd5GM/PQ6L218IzuRQ6wa/oyN2Xvj9Rs84KI+OmYddMi4jdjzB+hKaWU6L0t8JRu0Wlx/5w1nm9E7/OZ2z5DOXe79fOi97poS3fG+rDutg+L3hmuf47emawnZebs6L2971+j99rouMp+PTEiPhy9wrTb3q30UKM0PfjOiYjPZ+YVETH2w4LvjIjnZObiiHheRCyPiDWllBuid3r4ssxcEhFfi957gceVvT8ffltE7JOZt2XmOd3yE7rlvxkRH87MH3aRL0Tv/blLI+K6iLiulHLJePddSlkREX8QERdm5nXRe+vMzu7fWd2HF6+LiA3RO6N1SkRcm5nXRO/9uu+fYJ8WRe/oyDERsbj7kOdrJnoMgJ12TvRhPoqIr0bEPZl5Q0R8MyLeUkq5p3u7ypsj4huZuTR6R48/0mXeHREv65b/VfTOXE/kL6JXvrbNOc/ebv1XImKw24c/j97nqiJ6b4W5PDOvjd7nP98Wvc8d/HO33Wsi4m9LKasm2O47ovfZqA9289S/V8YITOyc6M/c9Ibs/XGYayPiTfFfC9vJEXHbOB8PeGtEnNNt94zofV57In8cEc/u5pP/iIjHbrf+0xFxfDd3vCIiftwtPzYiftCN6+zozXFzIuJL3Xa/FRFvrGz3fdE72PP5bm76YuW2TGDbh8fos8ycHhFbSykjmfnUiPhQd2oW4EFlPgImI3MT/eQPQUweh0bE57rTv5uj9yd5AfrBfARMRuYm+saZpikqM6+K3l+TGuuMUsrSqXD/k227wK7r43zxP6P3dpexvltKef3euF3ggenj3HR29D4OMdbnSyl/uTdu96FCaQIAAKiovj3vhBNOaGpUmzdvbonH9OnbHxx4YGbNmtWUHxoaasrvv//+O75RRb8L7bp165ry3f+VsMu2bNnSlB8eHm7Kb9rU9hc5R0ZG+rr9b33rW23fgEnuAx/4QNMT5IADDmja/h133NGUv+++7f97jgfmmGOOacovX768Kd/6/FywYLw/vrnzWueX1vmh9fdD6+/HFStWNOUvv/zypnyriy66aK+dny6++OKmuemXv/zljm9UccsttzTlW3/3zJw5synfOje3/u5tnVvuvXe8/xZu57W+9tqwYUNTfnR0tCk/Y8aMHd+o4jvf+U5T/thjj23Kf/SjH53wB8BfzwMAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgIrB6srB6uod2rp1a1N+ZGSkKX/fffc15Vv98pe/7Ov2mdqe//zn93sIk9rXv/71pvzRRx+9m0aya4aGhpryS5Ysaco/8pGPbMr3W+vvp9HR0ab8jBkzmvJz585tyh9xxBFN+ZNOOqkpf9dddzXl92a/+MUvmvKzZs1qyh9yyCFN+amudW5tdfDBB/d1+1PdUUcd1ZRftGjRbhrJr3KmCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKgZrK0844YSmO9+wYUNf8/Pnz2/KH3bYYU35UkpTftq0tk47OjralB8ZGWnKf+c732nKr1ixoim/adOmpvzs2bOb8jNmzGjK33DDDU35vd0f/dEfNeVnzpzZlB8aGmrK//znP2/Kt84v06dPb8qvX7++r9tvfX5+97vfbcpv3LixKT8wMNCUb/35ax1/6/Zf9KIXNeUnsyc/+clN+eHh4ab8ypUrm/JXXnllU751bsrMpnzrz2bra6dVq1Y15ffdd9+mfOvc3O/Xrlu2bGnK//jHP27KP//5z59wnTNNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQMVhbeccddzTd+fr165vyM2fObMrfe++9Tfmbb765KT88PNyU37hxY1N+3rx5TfnR0dGm/D777NOUP/zww5vyt99+e1N+ZGSkKX/PPfc05VufP3u7yy+/vCnf+vxauHBhU/7OO+9syrfOj1u2bGnKtz6/W7U+P/bbb7+m/IIFC5ryV155ZVP+mc98ZlN+//33b8pv3ry5Kb83+/KXv9yUX7t2bVO+9bVH689G69xw/fXXN+VbfzZbXzu1zk1r1qxpyq9ataopf+yxxzbljz/++Kb8ihUr+pqvcaYJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqspQy4cqnPOUpE6/cCbNmzWqJx6ZNm5rys2fPbsovXLiwKb9ly5am/ObNm5vymdnXfOv3b2BgoClf+9l+MLSOvzV/4YUXtn0DJ7n3v//9Td/gVatWNW3/iCOOaMrPmDGjKb9s2bKm/Jw5c5ryw8PDfc23Pr/f8IY3NOVps3Llyr12fvqzP/uzph/O1t+drXPbQQcd1JRfsGBBU771tdPatWub8uvWretr/ktf+lJTvnVubH3t16rfr92WLVs24QPgTBMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQM1lYuWrSo6c6nTWvrZAMDA33Nt5oxY0ZTfsuWLU35/fffvym/zz77NOVb93/mzJl93f7Q0FBTfvr06X3d/t7une98Z7+H8JD2rne9qyk/OjralB8crP762qHzzjuvr9tvfX5nZlO+9fdz6/b3Zh//+Mf7PYSHtLe+9a1N+YULF+6mkeyaxz72sU351ud2a76U0tft70mTd2QAAACTgNIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUDNZWvuhFL2q788Hq3e/QwMBAU354eLgpP21aW6cspTTl++3SSy9tyq9bt64pn5lN+dbHf8OGDU351atXN+XXr1/flH/ta1/blJ/sPvnJT/Z7CE1afz5HR0eb8iMjI035rVu3NuU3b97clP/iF7/YlG+d3/fbb7+m/KxZs5ryrb8ft2zZ0pRv/f698IUvbMpPZm9/+9ub8q2/+/qtdW5rnVta58bW58ZVV13VlD/yyCOb8gsWLGjKt/78bdq0qSm/atWqvm6/xpkmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoGKyt/MQnPtF056Ojo035ftu8eXNTvnX/Syl93f7JJ5/clF+/fn1T/rTTTmvKT58+vSm/cePGpvzMmTOb8q3j39tdeumlTfnW51drvtWWLVua8lu3bm3Kt84vrfmFCxc25efPn9+Ubx3/3Xff3ZRvHf/Q0FBTfr/99mvK782uv/76pny/f/e3br91bhoZGWnK93tumjVrVlN+w4YNTfk777yzr/k5c+Y05QcGBpryM2bMaMrXONMEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVg7WVn/rUpx6scYxr3rx5TfmbbrqpKT9jxoym/D333NOUnz59elP+5ptvbsq37v+sWbOa8pdddllTPjOb8q36vf1f//Vf7+v297TjjjuuKd/v+aH152N4eLgpPzo62pQfGhpqyq9evbqv2x8YGGjKL1y4sCl/+OGHN+X7rZTS7yFMWo9+9KOb8rfccktTvnVuGhysvjTcoda5aevWrU351rlh1apVTfnWx2/u3LlN+ZUrVzblH/awhzXl92bONAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQMVgbeWHPvShB2sce0Qppd9DoEFm9nsITVavXt2UHx0d3U0j2TuNjIw05Q899NDdNJJd81Cfn2bNmtXvITTZunVrv4fQ5JJLLmnKt87Pr3vd65ryk9ntt9/elB8YGGjKb9mypSm/efPmpvz69eub8lPdpk2bmvIrVqzYTSOZmi644IK+bv9d73rXhOucaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgIrB2sp77rmn6c5HR0eb8qWUvm5/69atTfnW8bduv3X/W7Xuf78fv8HB6tNjhwYGBpryZ555ZlN+b3fJJZf0dfutP5+t+v387rfMnNLbv/LKK3fTSPrjq1/9ar+HMGmdf/75Tfl+/+5s1e/Xfq36vf1+6/fc2uoLX/jCHrtvZ5oAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBisLby1a9+9YM1DoAH5Jxzzun3EAB+xb/8y7/0ewjAHuBMEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVAzuyTt/+9vf3pS/4447mvL77bdfU37FihVN+Xnz5jXlW913331N+RkzZjTlN2zY0JSfOXNmU37jxo1N+db937x5c1N+eHi4Kb948eKm/N7u3e9+d1P+0EMPbcovW7asKf+oRz2qKf/Tn/60Kd/qhBNOaMqvWbOmKb9o0aKm/G233daUP+SQQ5ryd955Z1O+3/t/4oknNuX3ZmeeeWZT/owzzmjKX3vttU350047rSnf+rM9a9aspny/5/bWueHWW2+1/Qa1ucmZJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqMhSyoQrjz322IlXPgiGhob6ufm+GxkZacoPDg7uppHsmq1btzblBwYGHtLbb3XddddlXwewh3384x9vmp9mzpzZtP2NGzc25ae6Aw88sCm/cuXK3TSSXdM6/rvuuqspv2jRoqb8mjVrmvIHHHBAU7719/MTn/jEvXZ+uvTSS/v62unWW2/t5+b77pBDDmnK9/vx6/f4p/r2Wz3vec+bcG5ypgkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACoGaysHBgaa7nzatLZOVkppyq9cubIpD0xeJ5xwQlN+3bp1TflZs2Y15V/wghc05WEqu+WWW/o9hEnr1ltvbcofeuihTfnXvva1TXmYympzkzNNAAAAFUoTAABAhdIEAABQoTQBAABUKE0AAAAVShMAAECF0gQAAFChNAEAAFQoTQAAABVKEwAAQIXSBAAAUKE0AQAAVChNAAAAFUoTAABAhdIEAABQkaWUfo8BAABg0nKmCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUJgAAgAqlCQAAoEJpAgAAqFCaAAAAKpQmAACACqUJAACgQmkCAACoUJoAAAAqlCYAAIAKpQkAAKBCaQIAAKhQmgAAACqUpkaZ+b1+j2GszBzOzH/MzJ9k5o8z82Xd8pMzc3FmjmTm6WNu/+zMvHbMZWNm/sYD3OYpmfml3b0v223j05l5Y2Zen5kfy8yhPbk9mIom4Xz0l5l5a2au3W75H2bm0m7O+U5mHtMtH87Mj3frrsvMU8Zkfiszl2TmDzPzvbs4nrU7vtWuy8w3ZeYN3Ti/kZmH7cntwVQ1meaqzJyz3eugX2bm33XrpmfmZzPzZ5l5VWYePib3+Mz8fjcnLc3MGQ9wu6/KzPN27978l/vfJzO/3L0W/GFmvntPbeuhQmlqVEp5Wr/HsJ2zI+LuUsqjI+KYiPhWt3xZRLwqIj4z9sallG+WUo4rpRwXEadGxPqIuOzBG+5O+3REHB0Rx0bEzIh4TX+HA5PPJJyPLomIp4yz/DOllGO7eee9EfE33fLfj4gopRwbEf8tIv46M6dl5n4R8b6IOK2U8tiIODAzT9vzw3/AromI40spj4+IL0Rv34DtTKa5qpSyZtvroG5OuiUiLuxW/15E3FtKOTIi/jYi3hMRkZmDEfHPEfGH3Zx0SkRsedAHv2PnllKOjognRsTTM/N5/R7QVKY0Ndp25LI72/KtzPxcd5bn3Zn5isz8QXcE4ojudi/sjlZck5lfz8wDu+X7Z+bXurNBH87MWzJzYbfud7r7ubZbN1AZ0qsj4q8iIkopo6WUX3bXby6lLImI0Ur29Ii4tJSyvrK/J2Tm97qjwD/IzDnbrX9Kt/6a7t+juuWPHbMPSzLzUZk5qzsKcl13Bum3JtpuKeVfSycifhARiyr7AQ9Jk20+KqVcWUpZPs7y1WO+nBURpbt+TER8o7vN3RGxKiKOj4hHRsRPSikrutt9PSJeVnkcDszMi7q55brMfNp262d3Z4IWd4/Hi7vl485J3eO37QzSuZX9/eaY+fPKME/BuCbbXDVmXI+KiAMi4opu0Ysj4pPd9S9ExGmZmRHxnIhYUkq5LiKilHJPKWVr5X6f243xusz8xjjrJ9q/Z+X9Z8Cuyd5ZsYMz89vdsusz85njbbOUsr6U8s3u+uaIWBzmpDalFJeGS0Ss7f49JXq/4A+OiOkRcXtEvLNb98cR8Xfd9X0jIrvrr4mIv+6unxcRb+uuPzd6LyIWRsRjone0dqhb98GIeOUEY5kfEbdG76jt4oj4fEQcuN1tPhERp0+Q/7eI+O+VfR2OiJ9HxAnd13MjYrDb9y+NXdZd/7WIuKC7/g8R8Yox9zMzei96PjLm/uftxOM91O3bM/v9vXdxmWyXyTQfjTeu7Za9PiJu6uasR3XL/qCbtwYj4hHdPrysG+dtEXF4t+6CiLiksr3PRsRZ3fWBbXPLmMdnMCLmdtcXRsTPIiLHm5MiYkFE3DjmcZq/k9+L8yLiT/v9M+HiMhkvk3iuekf0zs5s+/r6iFg05uubuvs/KyI+FRFf7V6T/K/Kfe7fzXOP6L5e0P37qog4bwf7d0lEPL27Prubu/4kIs7ulg1ExJyd2K/50Xv99sh+f++n8mUw2J2uLt1R1cy8Ke5/m9vSiHh2d31RRHw2Mw+OXnn4Rbf8GRHxkoiIUspXMvPebvlpEfHkiLi6d3AjZkbE3RNsf7C7/++WUt6UmW+KiHMj4owdDbwbz7HRmwAmclRELC+lXN2Nc3WXHXubeRHxye5oTYleyYmI+H5EnJ2ZiyLiwlLKTzNz6f9p726D7CzP+4Bft3ZXEkKskHkxSg120FjCvNhgG2FMLUqa2mN3Qv2B0iTEoaV1O2470yRjmxkn4xK3bhunZOKklDZ17OnUkTv1AG3i4OJxahMXyXWIjEc2yMiivEhgNtjoBfSyWu3dD+eoKLL2ksyFdJD4/T7BPud/7uc5u+c+z//c5zyKiH/bWvuNGJSur8WR/fuI+NOjvC28ko16Pkr13m+LiNtaaz8fEb8WETdGxKdjcLJzfww+IrM2ImZ678+21j4QgzI0O/z5+cnd/1RE/OJwnP0Rsf2Q7S0i/lVrbfXw/v5KRLw6Bo/NX5qT2uBjOHsi4lOttT+OiCN+f7O19gsxWCG7+ogPBPBymqt+Nv7yOVM7zG16DM63/mpEXB6DrzX8SWvtz3vvP7KKFBFvi8F5y/8d7ucPD3ObuY7vvoj4rdbaH8Tg3GlLa+3PIuLAd7v/e+/9geyAhnPY5yLid3rvj2S3JefjeS+tvQf99+xB/z8b8f8L6u/G4J2FSyLiH0XEgS8OHu6JeeDn/7m/8Hnblb33W+a47Q9i8OS9a/j/n4+INx/lvl8fEXf13rPP5LZ44WM0c/kXEfGV3vvFEfEzMTy+3vuaiLg2InZHxD2ttZ/qvT8cg0ltQ0T869baR7M7bq398xi8Y/MrR3E88Eo36vnoaP3XiHhvRETvfab3/svD+/5bMXh3dNNw2x/13q/ovV8Zg5WfTYUxb4jBXPKWPvgOw9MRsfBwc1LvfSYG38u6Y7if/zO749baT8fgu6XX9t73ZrcFIuJlMle11t4Ug0/K/PlBP94SEecOt4/H4I3hHw5/fm/v/Zk++Eju3TH3+dbRnDsd9vh67/8mBitPp0TE11trF/Te/zQiVsdgVe6/tNZ+8Qj3/XsRsan3/ttHuB1HoDQdf0ti8IceMXhn9YD/HYPiEq21d8ZgqTZi8Pn+61prZw+3varNcUWm3nuPwVLuXxv+6K9HxINHuV8/F4N3IjIbI+InWmuXD/fltOEkcrCDj+/vHvhha+38iHik9/47EfGHEfHG1tpPRMSu3vtnY7AiNmfBa639g4h4V0T8XO89+14WcPSO2XyUGa5EH/A3Y1iA2uBqT6cO//tvxGCV6cHh/x8Yc2lE/OOI+FQyxJ9ExAeGtx9rrU0esn1JDC6Ys6+1dk1EvHZ42x+Zk1pri2Pw8b67Y/CRnEuT47osIv5jDArTi1qBAw7reMxVhzsP+sODxrsuIv7X8FzrnhicxywangddHXOfb62LiKtbaz95YF+O9vhaa8t77xt6778RgxX4C4bHMdV7/08R8fuRnzv9y+F9/9Jct+Ho+Xje8XdLRHy+tbY1Bl8U/snhz389Ij7XBl88vjcinoqInb33Z1prvxYRX2qtzYvB1Vn+SQw+unI4N8fgnYffjoi/iIi/FzG4gEMMVqCWRsTPtNZ+vQ+u+BJtcAnNc+OFK+0dVu99erh/v9taOyUGq0Y/fcjNPhGDj+f9Sgy+I3XA34mIX2it7YuI70fEx2KwrP2brbXZ4XF9IBn+PwyPed1wqf3O3vvHsv0FjuiWOIbzURtcGvznI2JRa21LRHxq+G7vPx2uyOyLiGfjhZOEs2OwEj0bgxOIgz8m88nhO8ERER8brgrN5Z9FxO+11v5+ROyPwdyy7qDtfxARf9Rauz8iHojBG0IRg48oHzonnRYR/6MNLifcIuKXk3F/MwbfO/j8cJ56vPd+bXJ74OjcEsf23CliUL7ec8jPfj8G51Tfi8EK089GRAw/MvxbEfFnMVhFurv3/seHu9Pe+1+01v5hRNw53JepGFwd9GiO75eGb+zsj0Ep++JwHz40PJ96LoYfRT7U8OsQvxqD+W39cE76d7337A0nEge+dMaItdYWRMT+3vtMa+3KiLh9+LERgOPKfAScCMxVHE9Wml4+zouI/zZ8F2I6hv9eCcAImI+AE4G5iuPGStMJqrX2f2Jwec6Dva/3vuEluv+74oXl4QNu7r1nV9c7YccFXrxjPR8l4/5qRPztQ378+d77x0/GcYGa43DuNKq5cCTjvtIoTQAAAIn043m33357qVGdc845lXjs2bOnlN+yZUsp/9RTP/IP2f9YqoV0+/ZD/2mRH895551Xyu/bl119/NibN692cccnnniilN+1a1cpv2bNmlK+anx8fK5LsZ4UPvOZz5SeYDt27CiNX81X58fFixeX8lNTtYu7/fCHh/unRo7e5OShF7T78VRfH2ZmZkr5sbGxUr76+1u0aFEpf9NNN5XyVSfz/HTjjTeW5qYzzzyzNH71uXHaaaeV8vPnzy/l9+/fX8pXn1vV/R8fr33zpTq37ty5s5SvHv/sbO0Cx5/73JEu5Jyr/v08+eSTc85NLjkOAACQUJoAAAASShMAAEBCaQIAAEgoTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQUJoAAAASShMAAEBCaQIAAEgoTQAAAAmlCQAAIKE0AQAAJFrvfc6NMzMzc288AbTWRjr+t771rVJ+27ZtpfwTTzxRylMzOztbyi9fvryUX7169WifAMfYiT4/nehGPb++0o368b///vtL+VWrVp20f0DLli0zNxXMm1d7P39ycrKUv+aaa0r5hQsXlvJVCxYsGOn4ozY9PV3K33rrrXPOTVaaAAAAEkoTAABAQmkCAABIKE0AAAAJpQkAACChNAEAACSUJgAAgITSBAAAkFCaAAAAEkoTAABAQmkCAABIKE0AAAAJpQkAACChNAEAWW5jdAAADMZJREFUACSUJgAAgMR4tvGb3/xm6c6///3vl/ILFiwo5aempkr5LVu2lPLz5tU66fz580v5U045pZRfvHhxKb9z585Svvr7O+2000r5pUuXlvKPPfZYKb927dpSfvXq1aX8y93dd99dylf/Pnfs2FHKb926tZTftWvXSPPj4+nLxxFNTk6W8s8++2wpv2LFilL+6aefLuVnZmZK+errY/X1afv27aX8qlWrSvmXs/e///2l/JlnnlnKV1/7Fy5cWMpPTEyU8tW/7ercXN3/PXv2lPIbN24s5auvbdW5ofr7672X8seSlSYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgMZ5tXLNmTenOZ2dnS/lXvepVpfz4eHp4R7Rr165Sfs+ePaX8888/X8pXH//q/i9durSU3717dyl//fXXl/IPPfRQKX/JJZeU8ldccUUpf7L76le/WspPT0+X8osXLy7l3/3ud5fyjz76aClffX5VH79nn322lJ+cnCzlt23bVsq31kr5G264oZRfv359Kb9jx45S/pxzzinlT2bVc4fNmzeX8mNjY6X8unXrSvlzzz23lN+3b18p33sv5RcsWFDKV889JyYmSvnq3Pbwww+X8tVzlzPPPLOUrz7+GStNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQGI823j55ZeX7vyJJ54o5aempkr5/fv3l/Lbtm0r5c8444yR5qu2bt1ayt9xxx0v0Z68OHffffdIx6/qvZfy1d/fy93pp59eyp9zzjmlfGutlK/+fjZv3lzK79u3r5Q/9dRTS/mqZcuWlfLz589/ifbkxVm7du1Ixx/18Z/M9uzZU8pPTk6W8tu3by/l3/nOd5byCxcuLOWrc1P1teEHP/hBKT89PV3Kj42NlfKvec1rSvmLL764lK+qvrYeS1aaAAAAEkoTAABAQmkCAABIKE0AAAAJpQkAACChNAEAACSUJgAAgITSBAAAkFCaAAAAEkoTAABAQmkCAABIKE0AAAAJpQkAACChNAEAACSUJgAAgMR4tvFDH/rQ8dqPY6L3Xsq31kY6/s0331zK79ixo5Q/99xzS/kbbrihlJ+enh5pfvfu3aX8vn37Svnq/p/szjjjjFK++vupqo7/6le/+iXakxdndnZ2pOOPen6eN6/2nuPY2FgpP+rXN+a2dOnSkY5fnRurf1vV186q6tx01llnlfLV53b1uVkd39w0NytNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQGI82/imN72pdOd79+4t5aenp0v5Xbt2jXT8av6Tn/xkKV89/i9+8Yul/MzMTCm/e/fuUn7//v2l/NNPP13KP/7446X8tm3bSvmT3djY2EjH772PdPzWWilffX5Uj786/vLly0v56vxcPf7Z2dlS/rnnnivln3nmmVK+Oj+fzF772teOehdKqn/bo54bqvl9+/aV8rfeemspf9ZZZ5Xyk5OTpfz4eFoNjnm+qvr7v+mmm+bcZqUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASIxnG1esWFG6871795byMzMzpfzs7GwpPz09Xcrv379/pPnq43fbbbeV8qeffnopv3PnzpHmlyxZUsovW7aslK8+fie7LVu2lPLV54f5abT5733ve6X82WefXcpPTU2V8g8//HApf9FFF5Xy8+fPL+UXLVpUyp/M7rzzzlJ+bGyslJ+YmCjlW2ulfO99pPnq3Fod/73vfW8pf+qpp5byzz//fClf/ftbunRpKb9r165S/ljOTVaaAAAAEkoTAABAQmkCAABIKE0AAAAJpQkAACChNAEAACSUJgAAgITSBAAAkFCaAAAAEkoTAABAQmkCAABIKE0AAAAJpQkAACChNAEAACSUJgAAgMR4tvHCCy8s3fn69etL+VNOOaWUn56eLuUXLVpUyo+NjZXy4+Ppr+eYq46/bNmyUv7LX/5yKd9aK+Wfe+65Un7r1q2lfNWHP/zhkY5/rJ133nml/NTUVCm/cOHCUn7Pnj2l/Pz580v5iYmJkeYffPDBkY6/e/fuUn7JkiWl/OWXX17Kj1r19fVkdv3115fyGzduLOWr506991J+wYIFpXz1tbs6Nzz11FMjHb86t8ybN9r1kOrcUD33PJZzk5UmAACAhNIEAACQUJoAAAASShMAAEBCaQIAAEgoTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQUJoAAAASShMAAEBCaQIAAEgoTQAAAAmlCQAAINF673NuvPbaa+feeBSy+z4RVPe/tfYS7cmL80p//Ed9/Bs2bCjlx8bGSvlHH310tH+Ax9hHPvKRE3p+GvX4vLLdfvvtIx1/+/btJ+389NGPfvSEfnK/0uemV/rxj9qaNWtK+eq59+bNm+e8AytNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQKL13ufcuGLFirk3HoV582qdrLVWylfHH/X+T0xMlPIn+vFX848//ngpP2p33XVXKb9q1araA/gyt2TJktL8RM2o54eqUY//8Y9/fKTjV1122WWl/BVXXHHSzk/Lly8vzU3ZednxUB1/dnZ2pONTc+ONN5byoz73e8973lPKZ3OTlSYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgMZ5t/OxnP3u89gPgx/KlL31p1LsA8CPWrFkz6l0AjgErTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQUJoAAAASShMAAEBCaQIAAEgoTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQUJoAAAASShMAAEBi/Fje+Zvf/OZS/itf+Uopf80115Ty9957byl/9dVXl/JVX/va10r5d7zjHaX82rVrS/m3v/3tpXz1+FevXl3K33PPPaX8u971rlKe3Fve8pZSfv369aV8dX4c9fhVX//610v5t73tbaX8unXrSvkrr7yylL/vvvtK+auuuqqUH/XxM7frrruulD///PNL+U2bNpXyK1euLOU3btxYylddfPHFpfy3v/3tUv7SSy8t5R944IFSftWqVaX8N77xjVK+OrdV59annnpqzm1WmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAICE0gQAAJBQmgAAABJKEwAAQEJpAgAASChNAAAACaUJAAAgoTQBAAAklCYAAIBE673PufGOO+6Ye+NR2LlzZyUeb3zjG0v5bdu2lfJTU1Ol/N69e0v57373u6X8ypUrS/mqUe//xo0bS/kLLriglH/ooYdK+dnZ2VL+E5/4RCvdwcvcfffdV5qfxsbGSuO/9a1vLeVbO7F/PQ888EApf+mll75Ee/LibNiwoZS/5JJLSvnvfOc7pfxFF11Uylfnp4mJiVJ+5cqVJ/YTIPG+972vNDdVH9t77rmnlJ+cnCzlTz/99FL+sssuK+Wrz81NmzaV8lVveMMbSvnqc/vCCy8s5Tdv3lzKv/71ry/lq2666aY55yYrTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQUJoAAAASShMAAEBCaQIAAEgoTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQUJoAAAASShMAAECi9d7n3PjBD35w7o1H4aKLLqrE4/777y/lv/CFL5TycCJ77LHH2qj34Vh65JFHSvPT6173utL4Tz75ZCl/1VVXlfJwIjuZ56dPf/rTpblp06ZNpfFXrFhRyt9yyy2lPJzIsrnJShMAAEBCaQIAAEgoTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQUJoAAAASShMAAEBCaQIAAEgoTQAAAAmlCQAAIKE0AQAAJJQmAACAhNIEAACQaL33Ue8DAADAy5aVJgAAgITSBAAAkFCaAAAAEkoTAABAQmkCAABIKE0AAACJ/wfciVnFKAukjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.01, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.01, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 1.2621 - accuracy: 0.8906 - f1_metric: 0.48 - ETA: 0s - loss: 1.3018 - accuracy: 0.5885 - f1_metric: 0.16 - ETA: 0s - loss: 1.4178 - accuracy: 0.3333 - f1_metric: 0.08 - ETA: 0s - loss: 1.3646 - accuracy: 0.2500 - f1_metric: 0.05 - ETA: 0s - loss: 1.3718 - accuracy: 0.2682 - f1_metric: 0.04 - ETA: 0s - loss: 1.3596 - accuracy: 0.3203 - f1_metric: 0.03 - ETA: 0s - loss: 1.3479 - accuracy: 0.3548 - f1_metric: 0.03 - ETA: 0s - loss: 1.3476 - accuracy: 0.3314 - f1_metric: 0.03 - ETA: 0s - loss: 1.3255 - accuracy: 0.3043 - f1_metric: 0.03 - ETA: 0s - loss: 1.3179 - accuracy: 0.2840 - f1_metric: 0.03 - ETA: 0s - loss: 1.3281 - accuracy: 0.2694 - f1_metric: 0.03 - ETA: 0s - loss: 1.3310 - accuracy: 0.2573 - f1_metric: 0.03 - ETA: 0s - loss: 1.3252 - accuracy: 0.2469 - f1_metric: 0.02 - ETA: 0s - loss: 1.2921 - accuracy: 0.2315 - f1_metric: 0.02 - ETA: 0s - loss: 1.2698 - accuracy: 0.2231 - f1_metric: 0.03 - ETA: 0s - loss: 1.2585 - accuracy: 0.2332 - f1_metric: 0.03 - ETA: 0s - loss: 1.2404 - accuracy: 0.2541 - f1_metric: 0.0392\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.17624, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 124ms/step - loss: 1.2343 - accuracy: 0.2695 - f1_metric: 0.0459 - val_loss: 1.0218 - val_accuracy: 0.6262 - val_f1_metric: 0.1762 - lr: 0.0010\n",
      "Epoch 2/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.8927 - accuracy: 0.7031 - f1_metric: 0.26 - ETA: 0s - loss: 1.0881 - accuracy: 0.5898 - f1_metric: 0.16 - ETA: 0s - loss: 1.0469 - accuracy: 0.5045 - f1_metric: 0.09 - ETA: 0s - loss: 1.0125 - accuracy: 0.4594 - f1_metric: 0.08 - ETA: 0s - loss: 1.0843 - accuracy: 0.4255 - f1_metric: 0.08 - ETA: 0s - loss: 1.1114 - accuracy: 0.3916 - f1_metric: 0.08 - ETA: 0s - loss: 1.1190 - accuracy: 0.3655 - f1_metric: 0.08 - ETA: 0s - loss: 1.0872 - accuracy: 0.3359 - f1_metric: 0.08 - ETA: 0s - loss: 1.0833 - accuracy: 0.3098 - f1_metric: 0.08 - ETA: 0s - loss: 1.0917 - accuracy: 0.2993 - f1_metric: 0.08 - ETA: 0s - loss: 1.0983 - accuracy: 0.3033 - f1_metric: 0.08 - ETA: 0s - loss: 1.0795 - accuracy: 0.3009 - f1_metric: 0.08 - ETA: 0s - loss: 1.0888 - accuracy: 0.2988 - f1_metric: 0.08 - ETA: 0s - loss: 1.0903 - accuracy: 0.3027 - f1_metric: 0.08 - ETA: 0s - loss: 1.1060 - accuracy: 0.3007 - f1_metric: 0.08 - ETA: 0s - loss: 1.1020 - accuracy: 0.2933 - f1_metric: 0.08 - ETA: 0s - loss: 1.0987 - accuracy: 0.2898 - f1_metric: 0.08 - ETA: 0s - loss: 1.0989 - accuracy: 0.2843 - f1_metric: 0.08 - ETA: 0s - loss: 1.0947 - accuracy: 0.2756 - f1_metric: 0.0817\n",
      "Epoch 00002: val_f1_metric did not improve from 0.17624\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 1.0947 - accuracy: 0.2756 - f1_metric: 0.0817 - val_loss: 1.2759 - val_accuracy: 0.1484 - val_f1_metric: 0.0802 - lr: 0.0010\n",
      "Epoch 3/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0475 - accuracy: 0.1719 - f1_metric: 0.12 - ETA: 0s - loss: 1.1273 - accuracy: 0.1979 - f1_metric: 0.10 - ETA: 0s - loss: 1.1041 - accuracy: 0.1937 - f1_metric: 0.10 - ETA: 0s - loss: 1.0656 - accuracy: 0.2188 - f1_metric: 0.10 - ETA: 0s - loss: 1.0297 - accuracy: 0.2639 - f1_metric: 0.12 - ETA: 0s - loss: 1.0509 - accuracy: 0.2869 - f1_metric: 0.14 - ETA: 0s - loss: 1.0472 - accuracy: 0.3005 - f1_metric: 0.14 - ETA: 0s - loss: 1.0225 - accuracy: 0.3021 - f1_metric: 0.14 - ETA: 0s - loss: 1.0012 - accuracy: 0.2987 - f1_metric: 0.13 - ETA: 0s - loss: 0.9978 - accuracy: 0.2952 - f1_metric: 0.13 - ETA: 0s - loss: 1.0104 - accuracy: 0.2961 - f1_metric: 0.13 - ETA: 0s - loss: 0.9881 - accuracy: 0.2928 - f1_metric: 0.12 - ETA: 0s - loss: 1.0081 - accuracy: 0.3019 - f1_metric: 0.13 - ETA: 0s - loss: 1.0226 - accuracy: 0.3090 - f1_metric: 0.13 - ETA: 0s - loss: 1.0118 - accuracy: 0.3141 - f1_metric: 0.13 - ETA: 0s - loss: 0.9979 - accuracy: 0.3150 - f1_metric: 0.13 - ETA: 0s - loss: 1.0194 - accuracy: 0.3205 - f1_metric: 0.14 - ETA: 0s - loss: 1.0232 - accuracy: 0.3219 - f1_metric: 0.14 - ETA: 0s - loss: 1.0163 - accuracy: 0.3176 - f1_metric: 0.13 - ETA: 0s - loss: 1.0126 - accuracy: 0.3149 - f1_metric: 0.13 - ETA: 0s - loss: 1.0160 - accuracy: 0.3114 - f1_metric: 0.13 - ETA: 0s - loss: 1.0137 - accuracy: 0.3121 - f1_metric: 0.1352\n",
      "Epoch 00003: val_f1_metric did not improve from 0.17624\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 1.0101 - accuracy: 0.3126 - f1_metric: 0.1361 - val_loss: 1.1581 - val_accuracy: 0.3194 - val_f1_metric: 0.1494 - lr: 0.0010\n",
      "Epoch 4/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0739 - accuracy: 0.3438 - f1_metric: 0.12 - ETA: 0s - loss: 0.9996 - accuracy: 0.4336 - f1_metric: 0.22 - ETA: 0s - loss: 0.9512 - accuracy: 0.4531 - f1_metric: 0.24 - ETA: 0s - loss: 0.9720 - accuracy: 0.4512 - f1_metric: 0.24 - ETA: 0s - loss: 0.9239 - accuracy: 0.4631 - f1_metric: 0.26 - ETA: 0s - loss: 0.9398 - accuracy: 0.4856 - f1_metric: 0.30 - ETA: 0s - loss: 0.9213 - accuracy: 0.4746 - f1_metric: 0.29 - ETA: 0s - loss: 0.9467 - accuracy: 0.4670 - f1_metric: 0.28 - ETA: 0s - loss: 0.9313 - accuracy: 0.4570 - f1_metric: 0.27 - ETA: 0s - loss: 0.9259 - accuracy: 0.4545 - f1_metric: 0.27 - ETA: 0s - loss: 0.9641 - accuracy: 0.4406 - f1_metric: 0.27 - ETA: 0s - loss: 0.9594 - accuracy: 0.4202 - f1_metric: 0.25 - ETA: 0s - loss: 0.9475 - accuracy: 0.3957 - f1_metric: 0.23 - ETA: 0s - loss: 0.9616 - accuracy: 0.3984 - f1_metric: 0.24 - ETA: 0s - loss: 0.9529 - accuracy: 0.4003 - f1_metric: 0.24 - ETA: 0s - loss: 0.9528 - accuracy: 0.4062 - f1_metric: 0.24 - ETA: 0s - loss: 0.9450 - accuracy: 0.4077 - f1_metric: 0.2481\n",
      "Epoch 00004: val_f1_metric improved from 0.17624 to 0.24493, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 115ms/step - loss: 0.9423 - accuracy: 0.4068 - f1_metric: 0.2481 - val_loss: 1.0704 - val_accuracy: 0.4283 - val_f1_metric: 0.2449 - lr: 0.0010\n",
      "Epoch 5/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.7083 - accuracy: 0.3438 - f1_metric: 0.20 - ETA: 0s - loss: 1.0128 - accuracy: 0.4961 - f1_metric: 0.34 - ETA: 0s - loss: 0.9647 - accuracy: 0.4505 - f1_metric: 0.29 - ETA: 0s - loss: 0.9467 - accuracy: 0.4531 - f1_metric: 0.29 - ETA: 0s - loss: 0.9410 - accuracy: 0.4115 - f1_metric: 0.26 - ETA: 0s - loss: 0.9333 - accuracy: 0.4018 - f1_metric: 0.24 - ETA: 0s - loss: 0.9213 - accuracy: 0.4053 - f1_metric: 0.25 - ETA: 0s - loss: 0.9143 - accuracy: 0.4132 - f1_metric: 0.27 - ETA: 0s - loss: 0.8994 - accuracy: 0.4234 - f1_metric: 0.28 - ETA: 0s - loss: 0.8976 - accuracy: 0.4341 - f1_metric: 0.30 - ETA: 0s - loss: 0.8927 - accuracy: 0.4412 - f1_metric: 0.30 - ETA: 0s - loss: 0.8959 - accuracy: 0.4479 - f1_metric: 0.31 - ETA: 0s - loss: 0.9079 - accuracy: 0.4429 - f1_metric: 0.31 - ETA: 0s - loss: 0.8945 - accuracy: 0.4435 - f1_metric: 0.31 - ETA: 0s - loss: 0.8910 - accuracy: 0.4453 - f1_metric: 0.31 - ETA: 0s - loss: 0.8916 - accuracy: 0.4446 - f1_metric: 0.31 - ETA: 0s - loss: 0.9011 - accuracy: 0.4388 - f1_metric: 0.31 - ETA: 0s - loss: 0.8932 - accuracy: 0.4405 - f1_metric: 0.31 - ETA: 0s - loss: 0.8886 - accuracy: 0.4400 - f1_metric: 0.31 - ETA: 0s - loss: 0.8877 - accuracy: 0.4431 - f1_metric: 0.31 - ETA: 0s - loss: 0.8841 - accuracy: 0.4421 - f1_metric: 0.31 - ETA: 0s - loss: 0.8842 - accuracy: 0.4446 - f1_metric: 0.3199\n",
      "Epoch 00005: val_f1_metric improved from 0.24493 to 0.36035, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 144ms/step - loss: 0.8942 - accuracy: 0.4463 - f1_metric: 0.3210 - val_loss: 1.0092 - val_accuracy: 0.4736 - val_f1_metric: 0.3604 - lr: 0.0010\n",
      "Epoch 6/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.7442 - accuracy: 0.5000 - f1_metric: 0.38 - ETA: 0s - loss: 1.0184 - accuracy: 0.4844 - f1_metric: 0.34 - ETA: 0s - loss: 0.9069 - accuracy: 0.4500 - f1_metric: 0.32 - ETA: 0s - loss: 0.9054 - accuracy: 0.4375 - f1_metric: 0.30 - ETA: 0s - loss: 0.8955 - accuracy: 0.4271 - f1_metric: 0.30 - ETA: 0s - loss: 0.8797 - accuracy: 0.4077 - f1_metric: 0.28 - ETA: 0s - loss: 0.8779 - accuracy: 0.4026 - f1_metric: 0.27 - ETA: 0s - loss: 0.8766 - accuracy: 0.4021 - f1_metric: 0.27 - ETA: 0s - loss: 0.8799 - accuracy: 0.4072 - f1_metric: 0.28 - ETA: 0s - loss: 0.8854 - accuracy: 0.4137 - f1_metric: 0.29 - ETA: 0s - loss: 0.8618 - accuracy: 0.4211 - f1_metric: 0.30 - ETA: 0s - loss: 0.8731 - accuracy: 0.4261 - f1_metric: 0.30 - ETA: 0s - loss: 0.8876 - accuracy: 0.4425 - f1_metric: 0.32 - ETA: 0s - loss: 0.8652 - accuracy: 0.4479 - f1_metric: 0.33 - ETA: 0s - loss: 0.8605 - accuracy: 0.4475 - f1_metric: 0.33 - ETA: 0s - loss: 0.8543 - accuracy: 0.4490 - f1_metric: 0.33 - ETA: 0s - loss: 0.8563 - accuracy: 0.4531 - f1_metric: 0.34 - ETA: 0s - loss: 0.8480 - accuracy: 0.4555 - f1_metric: 0.34 - ETA: 0s - loss: 0.8408 - accuracy: 0.4559 - f1_metric: 0.34 - ETA: 0s - loss: 0.8368 - accuracy: 0.4692 - f1_metric: 0.36 - ETA: 0s - loss: 0.8425 - accuracy: 0.4708 - f1_metric: 0.36 - ETA: 0s - loss: 0.8480 - accuracy: 0.4752 - f1_metric: 0.37 - ETA: 0s - loss: 0.8494 - accuracy: 0.4786 - f1_metric: 0.3796\n",
      "Epoch 00006: val_f1_metric did not improve from 0.36035\n",
      "44/44 [==============================] - 2s 42ms/step - loss: 0.8499 - accuracy: 0.4790 - f1_metric: 0.3819 - val_loss: 1.1430 - val_accuracy: 0.3923 - val_f1_metric: 0.3146 - lr: 0.0010\n",
      "Epoch 7/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0700 - accuracy: 0.3906 - f1_metric: 0.30 - ETA: 1s - loss: 0.9233 - accuracy: 0.3385 - f1_metric: 0.28 - ETA: 1s - loss: 0.8969 - accuracy: 0.3219 - f1_metric: 0.25 - ETA: 1s - loss: 0.8579 - accuracy: 0.3438 - f1_metric: 0.27 - ETA: 1s - loss: 0.8038 - accuracy: 0.3455 - f1_metric: 0.28 - ETA: 0s - loss: 0.7832 - accuracy: 0.3892 - f1_metric: 0.32 - ETA: 0s - loss: 0.7983 - accuracy: 0.4303 - f1_metric: 0.35 - ETA: 0s - loss: 0.8026 - accuracy: 0.4604 - f1_metric: 0.38 - ETA: 0s - loss: 0.7859 - accuracy: 0.4770 - f1_metric: 0.40 - ETA: 0s - loss: 0.7935 - accuracy: 0.4910 - f1_metric: 0.41 - ETA: 0s - loss: 0.7865 - accuracy: 0.4903 - f1_metric: 0.41 - ETA: 0s - loss: 0.8047 - accuracy: 0.4796 - f1_metric: 0.40 - ETA: 0s - loss: 0.8127 - accuracy: 0.4700 - f1_metric: 0.39 - ETA: 0s - loss: 0.8267 - accuracy: 0.4693 - f1_metric: 0.38 - ETA: 0s - loss: 0.8329 - accuracy: 0.4677 - f1_metric: 0.38 - ETA: 0s - loss: 0.8372 - accuracy: 0.4746 - f1_metric: 0.39 - ETA: 0s - loss: 0.8328 - accuracy: 0.4747 - f1_metric: 0.39 - ETA: 0s - loss: 0.8415 - accuracy: 0.4721 - f1_metric: 0.39 - ETA: 0s - loss: 0.8318 - accuracy: 0.4680 - f1_metric: 0.38 - ETA: 0s - loss: 0.8391 - accuracy: 0.4658 - f1_metric: 0.38 - ETA: 0s - loss: 0.8349 - accuracy: 0.4625 - f1_metric: 0.3798\n",
      "Epoch 00007: val_f1_metric improved from 0.36035 to 0.38185, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 131ms/step - loss: 0.8349 - accuracy: 0.4625 - f1_metric: 0.3798 - val_loss: 1.0786 - val_accuracy: 0.4476 - val_f1_metric: 0.3818 - lr: 0.0010\n",
      "Epoch 8/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.4531 - f1_metric: 0.37 - ETA: 0s - loss: 0.7909 - accuracy: 0.4844 - f1_metric: 0.44 - ETA: 0s - loss: 0.7305 - accuracy: 0.5335 - f1_metric: 0.48 - ETA: 0s - loss: 0.7768 - accuracy: 0.5453 - f1_metric: 0.49 - ETA: 0s - loss: 0.8092 - accuracy: 0.5612 - f1_metric: 0.49 - ETA: 0s - loss: 0.8086 - accuracy: 0.5490 - f1_metric: 0.47 - ETA: 0s - loss: 0.8168 - accuracy: 0.5267 - f1_metric: 0.45 - ETA: 0s - loss: 0.8345 - accuracy: 0.4969 - f1_metric: 0.42 - ETA: 0s - loss: 0.8252 - accuracy: 0.4708 - f1_metric: 0.40 - ETA: 0s - loss: 0.8300 - accuracy: 0.4760 - f1_metric: 0.40 - ETA: 0s - loss: 0.8207 - accuracy: 0.4827 - f1_metric: 0.41 - ETA: 0s - loss: 0.8093 - accuracy: 0.4891 - f1_metric: 0.41 - ETA: 0s - loss: 0.8159 - accuracy: 0.4965 - f1_metric: 0.42 - ETA: 0s - loss: 0.8200 - accuracy: 0.5057 - f1_metric: 0.44 - ETA: 0s - loss: 0.8196 - accuracy: 0.5076 - f1_metric: 0.44 - ETA: 0s - loss: 0.8185 - accuracy: 0.5090 - f1_metric: 0.44 - ETA: 0s - loss: 0.8104 - accuracy: 0.4985 - f1_metric: 0.43 - ETA: 0s - loss: 0.8136 - accuracy: 0.4920 - f1_metric: 0.42 - ETA: 0s - loss: 0.8131 - accuracy: 0.4887 - f1_metric: 0.4213\n",
      "Epoch 00008: val_f1_metric did not improve from 0.38185\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.8131 - accuracy: 0.4887 - f1_metric: 0.4213 - val_loss: 1.1464 - val_accuracy: 0.4124 - val_f1_metric: 0.3562 - lr: 0.0010\n",
      "Epoch 9/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.6199 - accuracy: 0.4688 - f1_metric: 0.40 - ETA: 0s - loss: 0.7802 - accuracy: 0.4635 - f1_metric: 0.41 - ETA: 0s - loss: 0.7491 - accuracy: 0.4656 - f1_metric: 0.41 - ETA: 0s - loss: 0.7860 - accuracy: 0.5000 - f1_metric: 0.45 - ETA: 0s - loss: 0.7862 - accuracy: 0.5266 - f1_metric: 0.48 - ETA: 0s - loss: 0.8036 - accuracy: 0.5378 - f1_metric: 0.48 - ETA: 0s - loss: 0.7943 - accuracy: 0.5424 - f1_metric: 0.49 - ETA: 0s - loss: 0.7840 - accuracy: 0.5371 - f1_metric: 0.49 - ETA: 0s - loss: 0.7813 - accuracy: 0.5174 - f1_metric: 0.47 - ETA: 0s - loss: 0.7656 - accuracy: 0.5097 - f1_metric: 0.46 - ETA: 0s - loss: 0.7605 - accuracy: 0.5034 - f1_metric: 0.45 - ETA: 0s - loss: 0.7587 - accuracy: 0.5096 - f1_metric: 0.46 - ETA: 0s - loss: 0.7555 - accuracy: 0.5128 - f1_metric: 0.46 - ETA: 0s - loss: 0.7633 - accuracy: 0.5193 - f1_metric: 0.47 - ETA: 0s - loss: 0.7677 - accuracy: 0.5254 - f1_metric: 0.47 - ETA: 0s - loss: 0.7639 - accuracy: 0.5294 - f1_metric: 0.47 - ETA: 0s - loss: 0.7608 - accuracy: 0.5312 - f1_metric: 0.48 - ETA: 0s - loss: 0.7680 - accuracy: 0.5366 - f1_metric: 0.48 - ETA: 0s - loss: 0.7666 - accuracy: 0.5332 - f1_metric: 0.48 - ETA: 0s - loss: 0.7692 - accuracy: 0.5265 - f1_metric: 0.4748\n",
      "Epoch 00009: val_f1_metric did not improve from 0.38185\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.7705 - accuracy: 0.5243 - f1_metric: 0.4697 - val_loss: 1.1774 - val_accuracy: 0.4091 - val_f1_metric: 0.3333 - lr: 0.0010\n",
      "Epoch 10/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.9717 - accuracy: 0.4219 - f1_metric: 0.40 - ETA: 0s - loss: 0.7937 - accuracy: 0.4062 - f1_metric: 0.36 - ETA: 0s - loss: 0.7553 - accuracy: 0.4018 - f1_metric: 0.36 - ETA: 0s - loss: 0.7561 - accuracy: 0.4328 - f1_metric: 0.39 - ETA: 0s - loss: 0.7276 - accuracy: 0.4627 - f1_metric: 0.41 - ETA: 0s - loss: 0.7318 - accuracy: 0.4802 - f1_metric: 0.43 - ETA: 0s - loss: 0.7160 - accuracy: 0.4954 - f1_metric: 0.45 - ETA: 0s - loss: 0.7304 - accuracy: 0.5049 - f1_metric: 0.46 - ETA: 0s - loss: 0.7174 - accuracy: 0.5119 - f1_metric: 0.46 - ETA: 0s - loss: 0.7153 - accuracy: 0.5054 - f1_metric: 0.46 - ETA: 0s - loss: 0.7211 - accuracy: 0.5063 - f1_metric: 0.46 - ETA: 0s - loss: 0.7247 - accuracy: 0.5017 - f1_metric: 0.45 - ETA: 0s - loss: 0.7318 - accuracy: 0.4979 - f1_metric: 0.45 - ETA: 0s - loss: 0.7291 - accuracy: 0.5000 - f1_metric: 0.45 - ETA: 0s - loss: 0.7349 - accuracy: 0.4963 - f1_metric: 0.45 - ETA: 0s - loss: 0.7402 - accuracy: 0.5009 - f1_metric: 0.45 - ETA: 0s - loss: 0.7387 - accuracy: 0.5064 - f1_metric: 0.45 - ETA: 0s - loss: 0.7336 - accuracy: 0.5126 - f1_metric: 0.4644\n",
      "Epoch 00010: val_f1_metric improved from 0.38185 to 0.60980, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 5s 119ms/step - loss: 0.7295 - accuracy: 0.5167 - f1_metric: 0.4701 - val_loss: 0.7999 - val_accuracy: 0.6471 - val_f1_metric: 0.6098 - lr: 0.0010\n",
      "Epoch 11/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.7671 - accuracy: 0.7656 - f1_metric: 0.67 - ETA: 0s - loss: 0.6630 - accuracy: 0.6523 - f1_metric: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.6719 - f1_metric: 0.63 - ETA: 0s - loss: 0.6738 - accuracy: 0.6528 - f1_metric: 0.61 - ETA: 0s - loss: 0.6979 - accuracy: 0.6380 - f1_metric: 0.59 - ETA: 0s - loss: 0.6901 - accuracy: 0.6083 - f1_metric: 0.56 - ETA: 0s - loss: 0.7099 - accuracy: 0.5781 - f1_metric: 0.54 - ETA: 0s - loss: 0.7112 - accuracy: 0.5658 - f1_metric: 0.52 - ETA: 0s - loss: 0.7220 - accuracy: 0.5595 - f1_metric: 0.52 - ETA: 0s - loss: 0.7415 - accuracy: 0.5443 - f1_metric: 0.50 - ETA: 0s - loss: 0.7367 - accuracy: 0.5409 - f1_metric: 0.50 - ETA: 0s - loss: 0.7452 - accuracy: 0.5446 - f1_metric: 0.50 - ETA: 0s - loss: 0.7416 - accuracy: 0.5480 - f1_metric: 0.50 - ETA: 0s - loss: 0.7398 - accuracy: 0.5519 - f1_metric: 0.50 - ETA: 0s - loss: 0.7432 - accuracy: 0.5530 - f1_metric: 0.51 - ETA: 0s - loss: 0.7493 - accuracy: 0.5518 - f1_metric: 0.51 - ETA: 0s - loss: 0.7414 - accuracy: 0.5532 - f1_metric: 0.50 - ETA: 0s - loss: 0.7388 - accuracy: 0.5521 - f1_metric: 0.50 - ETA: 0s - loss: 0.7387 - accuracy: 0.5492 - f1_metric: 0.50 - ETA: 0s - loss: 0.7263 - accuracy: 0.5469 - f1_metric: 0.5043\n",
      "Epoch 00011: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.7252 - accuracy: 0.5462 - f1_metric: 0.5034 - val_loss: 0.9378 - val_accuracy: 0.5457 - val_f1_metric: 0.5049 - lr: 0.0010\n",
      "Epoch 12/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.7538 - accuracy: 0.5625 - f1_metric: 0.50 - ETA: 0s - loss: 0.7204 - accuracy: 0.5703 - f1_metric: 0.52 - ETA: 0s - loss: 0.7326 - accuracy: 0.5443 - f1_metric: 0.50 - ETA: 0s - loss: 0.7138 - accuracy: 0.5391 - f1_metric: 0.48 - ETA: 0s - loss: 0.7198 - accuracy: 0.5359 - f1_metric: 0.48 - ETA: 0s - loss: 0.6969 - accuracy: 0.5560 - f1_metric: 0.50 - ETA: 0s - loss: 0.6601 - accuracy: 0.5802 - f1_metric: 0.53 - ETA: 0s - loss: 0.6602 - accuracy: 0.5955 - f1_metric: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.5982 - f1_metric: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.5833 - f1_metric: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.5602 - f1_metric: 0.52 - ETA: 0s - loss: 0.6869 - accuracy: 0.5523 - f1_metric: 0.51 - ETA: 0s - loss: 0.7042 - accuracy: 0.5415 - f1_metric: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.5393 - f1_metric: 0.50 - ETA: 0s - loss: 0.6906 - accuracy: 0.5428 - f1_metric: 0.51 - ETA: 0s - loss: 0.6881 - accuracy: 0.5438 - f1_metric: 0.51 - ETA: 0s - loss: 0.6871 - accuracy: 0.5429 - f1_metric: 0.5103\n",
      "Epoch 00012: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.6871 - accuracy: 0.5429 - f1_metric: 0.5103 - val_loss: 0.8787 - val_accuracy: 0.5851 - val_f1_metric: 0.5424 - lr: 0.0010\n",
      "Epoch 13/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.6768 - accuracy: 0.5000 - f1_metric: 0.50 - ETA: 0s - loss: 0.6882 - accuracy: 0.5312 - f1_metric: 0.50 - ETA: 0s - loss: 0.6620 - accuracy: 0.5246 - f1_metric: 0.47 - ETA: 0s - loss: 0.7313 - accuracy: 0.5063 - f1_metric: 0.46 - ETA: 0s - loss: 0.6966 - accuracy: 0.5108 - f1_metric: 0.46 - ETA: 0s - loss: 0.7041 - accuracy: 0.5078 - f1_metric: 0.46 - ETA: 0s - loss: 0.7136 - accuracy: 0.4967 - f1_metric: 0.45 - ETA: 0s - loss: 0.7109 - accuracy: 0.4978 - f1_metric: 0.45 - ETA: 0s - loss: 0.7120 - accuracy: 0.5104 - f1_metric: 0.47 - ETA: 0s - loss: 0.6996 - accuracy: 0.5255 - f1_metric: 0.48 - ETA: 0s - loss: 0.6930 - accuracy: 0.5344 - f1_metric: 0.49 - ETA: 0s - loss: 0.6917 - accuracy: 0.5431 - f1_metric: 0.50 - ETA: 0s - loss: 0.6867 - accuracy: 0.5495 - f1_metric: 0.51 - ETA: 0s - loss: 0.6886 - accuracy: 0.5549 - f1_metric: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.5547 - f1_metric: 0.5148\n",
      "Epoch 00013: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.6803 - accuracy: 0.5555 - f1_metric: 0.5172 - val_loss: 0.9085 - val_accuracy: 0.5750 - val_f1_metric: 0.5454 - lr: 0.0010\n",
      "Epoch 14/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.8455 - accuracy: 0.6562 - f1_metric: 0.64 - ETA: 0s - loss: 0.6304 - accuracy: 0.6211 - f1_metric: 0.59 - ETA: 0s - loss: 0.6164 - accuracy: 0.6429 - f1_metric: 0.61 - ETA: 0s - loss: 0.6097 - accuracy: 0.6219 - f1_metric: 0.59 - ETA: 0s - loss: 0.6065 - accuracy: 0.6262 - f1_metric: 0.60 - ETA: 0s - loss: 0.6238 - accuracy: 0.6191 - f1_metric: 0.59 - ETA: 0s - loss: 0.6508 - accuracy: 0.6003 - f1_metric: 0.57 - ETA: 0s - loss: 0.6570 - accuracy: 0.5980 - f1_metric: 0.57 - ETA: 0s - loss: 0.6490 - accuracy: 0.5994 - f1_metric: 0.57 - ETA: 0s - loss: 0.6634 - accuracy: 0.5971 - f1_metric: 0.56 - ETA: 0s - loss: 0.6669 - accuracy: 0.5902 - f1_metric: 0.56 - ETA: 0s - loss: 0.6686 - accuracy: 0.5846 - f1_metric: 0.55 - ETA: 0s - loss: 0.6731 - accuracy: 0.5845 - f1_metric: 0.55 - ETA: 0s - loss: 0.6628 - accuracy: 0.5852 - f1_metric: 0.55 - ETA: 0s - loss: 0.6752 - accuracy: 0.5843 - f1_metric: 0.5510\n",
      "Epoch 00014: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.6727 - accuracy: 0.5832 - f1_metric: 0.5487 - val_loss: 0.9671 - val_accuracy: 0.5264 - val_f1_metric: 0.4937 - lr: 0.0010\n",
      "Epoch 15/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5171 - accuracy: 0.5156 - f1_metric: 0.50 - ETA: 0s - loss: 0.6247 - accuracy: 0.5156 - f1_metric: 0.46 - ETA: 0s - loss: 0.6024 - accuracy: 0.5208 - f1_metric: 0.47 - ETA: 0s - loss: 0.6156 - accuracy: 0.5347 - f1_metric: 0.50 - ETA: 0s - loss: 0.6600 - accuracy: 0.5521 - f1_metric: 0.51 - ETA: 0s - loss: 0.6524 - accuracy: 0.5667 - f1_metric: 0.53 - ETA: 0s - loss: 0.6576 - accuracy: 0.5755 - f1_metric: 0.54 - ETA: 0s - loss: 0.6392 - accuracy: 0.5818 - f1_metric: 0.54 - ETA: 0s - loss: 0.6434 - accuracy: 0.5833 - f1_metric: 0.54 - ETA: 0s - loss: 0.6442 - accuracy: 0.5851 - f1_metric: 0.54 - ETA: 0s - loss: 0.6315 - accuracy: 0.5797 - f1_metric: 0.54 - ETA: 0s - loss: 0.6403 - accuracy: 0.5777 - f1_metric: 0.54 - ETA: 0s - loss: 0.6365 - accuracy: 0.5846 - f1_metric: 0.54 - ETA: 0s - loss: 0.6442 - accuracy: 0.5859 - f1_metric: 0.54 - ETA: 0s - loss: 0.6441 - accuracy: 0.5871 - f1_metric: 0.55 - ETA: 0s - loss: 0.6413 - accuracy: 0.5847 - f1_metric: 0.5486\n",
      "Epoch 00015: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.6441 - accuracy: 0.5850 - f1_metric: 0.5493 - val_loss: 0.9454 - val_accuracy: 0.5348 - val_f1_metric: 0.5079 - lr: 0.0010\n",
      "Epoch 16/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5451 - accuracy: 0.5000 - f1_metric: 0.50 - ETA: 0s - loss: 0.6237 - accuracy: 0.5625 - f1_metric: 0.51 - ETA: 0s - loss: 0.6086 - accuracy: 0.5469 - f1_metric: 0.49 - ETA: 0s - loss: 0.6345 - accuracy: 0.5625 - f1_metric: 0.51 - ETA: 0s - loss: 0.6498 - accuracy: 0.5925 - f1_metric: 0.55 - ETA: 0s - loss: 0.6281 - accuracy: 0.5958 - f1_metric: 0.55 - ETA: 0s - loss: 0.6117 - accuracy: 0.5981 - f1_metric: 0.56 - ETA: 0s - loss: 0.6112 - accuracy: 0.6049 - f1_metric: 0.57 - ETA: 0s - loss: 0.6010 - accuracy: 0.6068 - f1_metric: 0.57 - ETA: 0s - loss: 0.6052 - accuracy: 0.6186 - f1_metric: 0.58 - ETA: 0s - loss: 0.6103 - accuracy: 0.6229 - f1_metric: 0.59 - ETA: 0s - loss: 0.6122 - accuracy: 0.6174 - f1_metric: 0.58 - ETA: 0s - loss: 0.6179 - accuracy: 0.6141 - f1_metric: 0.58 - ETA: 0s - loss: 0.6210 - accuracy: 0.6118 - f1_metric: 0.58 - ETA: 0s - loss: 0.6166 - accuracy: 0.6053 - f1_metric: 0.5756\n",
      "Epoch 00016: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.6162 - accuracy: 0.6029 - f1_metric: 0.5728 - val_loss: 0.9521 - val_accuracy: 0.5390 - val_f1_metric: 0.5105 - lr: 0.0010\n",
      "Epoch 17/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5789 - accuracy: 0.5469 - f1_metric: 0.53 - ETA: 0s - loss: 0.5897 - accuracy: 0.5664 - f1_metric: 0.53 - ETA: 0s - loss: 0.5937 - accuracy: 0.5893 - f1_metric: 0.56 - ETA: 0s - loss: 0.5824 - accuracy: 0.6266 - f1_metric: 0.60 - ETA: 0s - loss: 0.5857 - accuracy: 0.6334 - f1_metric: 0.61 - ETA: 0s - loss: 0.5643 - accuracy: 0.6348 - f1_metric: 0.61 - ETA: 0s - loss: 0.6046 - accuracy: 0.6349 - f1_metric: 0.61 - ETA: 0s - loss: 0.5981 - accuracy: 0.6328 - f1_metric: 0.61 - ETA: 0s - loss: 0.6048 - accuracy: 0.6288 - f1_metric: 0.60 - ETA: 0s - loss: 0.6013 - accuracy: 0.6228 - f1_metric: 0.59 - ETA: 0s - loss: 0.5937 - accuracy: 0.6164 - f1_metric: 0.58 - ETA: 0s - loss: 0.5879 - accuracy: 0.6135 - f1_metric: 0.58 - ETA: 0s - loss: 0.5895 - accuracy: 0.6204 - f1_metric: 0.59 - ETA: 0s - loss: 0.6012 - accuracy: 0.6270 - f1_metric: 0.59 - ETA: 0s - loss: 0.6071 - accuracy: 0.6283 - f1_metric: 0.6027\n",
      "Epoch 00017: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.6045 - accuracy: 0.6274 - f1_metric: 0.6009 - val_loss: 0.9824 - val_accuracy: 0.5197 - val_f1_metric: 0.4958 - lr: 0.0010\n",
      "Epoch 18/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4995 - accuracy: 0.5156 - f1_metric: 0.48 - ETA: 0s - loss: 0.5964 - accuracy: 0.5078 - f1_metric: 0.48 - ETA: 0s - loss: 0.6084 - accuracy: 0.5089 - f1_metric: 0.48 - ETA: 0s - loss: 0.6290 - accuracy: 0.5219 - f1_metric: 0.49 - ETA: 0s - loss: 0.6062 - accuracy: 0.5493 - f1_metric: 0.51 - ETA: 0s - loss: 0.5932 - accuracy: 0.5879 - f1_metric: 0.55 - ETA: 0s - loss: 0.5973 - accuracy: 0.6077 - f1_metric: 0.58 - ETA: 0s - loss: 0.6067 - accuracy: 0.6051 - f1_metric: 0.57 - ETA: 0s - loss: 0.6053 - accuracy: 0.5931 - f1_metric: 0.56 - ETA: 0s - loss: 0.6029 - accuracy: 0.5871 - f1_metric: 0.55 - ETA: 0s - loss: 0.6020 - accuracy: 0.5801 - f1_metric: 0.54 - ETA: 0s - loss: 0.6125 - accuracy: 0.5772 - f1_metric: 0.54 - ETA: 0s - loss: 0.6076 - accuracy: 0.5870 - f1_metric: 0.55 - ETA: 0s - loss: 0.6095 - accuracy: 0.5953 - f1_metric: 0.56 - ETA: 0s - loss: 0.6104 - accuracy: 0.5982 - f1_metric: 0.5687\n",
      "Epoch 00018: val_f1_metric did not improve from 0.60980\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.6097 - accuracy: 0.5994 - f1_metric: 0.5694 - val_loss: 0.8516 - val_accuracy: 0.6127 - val_f1_metric: 0.5886 - lr: 0.0010\n",
      "Epoch 19/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.8746 - accuracy: 0.5625 - f1_metric: 0.52 - ETA: 0s - loss: 0.6278 - accuracy: 0.5781 - f1_metric: 0.53 - ETA: 0s - loss: 0.5812 - accuracy: 0.5677 - f1_metric: 0.51 - ETA: 0s - loss: 0.5465 - accuracy: 0.5781 - f1_metric: 0.53 - ETA: 0s - loss: 0.5492 - accuracy: 0.6237 - f1_metric: 0.59 - ETA: 0s - loss: 0.5465 - accuracy: 0.6406 - f1_metric: 0.61 - ETA: 0s - loss: 0.5706 - accuracy: 0.6489 - f1_metric: 0.62 - ETA: 0s - loss: 0.5651 - accuracy: 0.6469 - f1_metric: 0.62 - ETA: 0s - loss: 0.5783 - accuracy: 0.6399 - f1_metric: 0.61 - ETA: 0s - loss: 0.5846 - accuracy: 0.6350 - f1_metric: 0.61 - ETA: 0s - loss: 0.5850 - accuracy: 0.6302 - f1_metric: 0.60 - ETA: 0s - loss: 0.5796 - accuracy: 0.6240 - f1_metric: 0.60 - ETA: 0s - loss: 0.5838 - accuracy: 0.6235 - f1_metric: 0.60 - ETA: 0s - loss: 0.5792 - accuracy: 0.6187 - f1_metric: 0.59 - ETA: 0s - loss: 0.5792 - accuracy: 0.6204 - f1_metric: 0.59 - ETA: 0s - loss: 0.5756 - accuracy: 0.6270 - f1_metric: 0.60 - ETA: 0s - loss: 0.5730 - accuracy: 0.6232 - f1_metric: 0.6011\n",
      "Epoch 00019: val_f1_metric improved from 0.60980 to 0.61760, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 115ms/step - loss: 0.5746 - accuracy: 0.6231 - f1_metric: 0.5999 - val_loss: 0.8235 - val_accuracy: 0.6354 - val_f1_metric: 0.6176 - lr: 0.0010\n",
      "Epoch 20/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5042 - accuracy: 0.6406 - f1_metric: 0.57 - ETA: 0s - loss: 0.5297 - accuracy: 0.6836 - f1_metric: 0.66 - ETA: 0s - loss: 0.5530 - accuracy: 0.6652 - f1_metric: 0.64 - ETA: 0s - loss: 0.5430 - accuracy: 0.6406 - f1_metric: 0.61 - ETA: 0s - loss: 0.5332 - accuracy: 0.6394 - f1_metric: 0.61 - ETA: 0s - loss: 0.5214 - accuracy: 0.6573 - f1_metric: 0.63 - ETA: 0s - loss: 0.5362 - accuracy: 0.6693 - f1_metric: 0.64 - ETA: 0s - loss: 0.5262 - accuracy: 0.6727 - f1_metric: 0.64 - ETA: 0s - loss: 0.5277 - accuracy: 0.6705 - f1_metric: 0.64 - ETA: 0s - loss: 0.5305 - accuracy: 0.6576 - f1_metric: 0.63 - ETA: 0s - loss: 0.5329 - accuracy: 0.6544 - f1_metric: 0.63 - ETA: 0s - loss: 0.5275 - accuracy: 0.6590 - f1_metric: 0.63 - ETA: 0s - loss: 0.5266 - accuracy: 0.6583 - f1_metric: 0.63 - ETA: 0s - loss: 0.5258 - accuracy: 0.6611 - f1_metric: 0.64 - ETA: 0s - loss: 0.5260 - accuracy: 0.6634 - f1_metric: 0.64 - ETA: 0s - loss: 0.5237 - accuracy: 0.6613 - f1_metric: 0.64 - ETA: 0s - loss: 0.5235 - accuracy: 0.6621 - f1_metric: 0.64 - ETA: 0s - loss: 0.5331 - accuracy: 0.6653 - f1_metric: 0.64 - ETA: 0s - loss: 0.5359 - accuracy: 0.6668 - f1_metric: 0.64 - ETA: 0s - loss: 0.5412 - accuracy: 0.6670 - f1_metric: 0.6497\n",
      "Epoch 00020: val_f1_metric did not improve from 0.61760\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.5491 - accuracy: 0.6648 - f1_metric: 0.6479 - val_loss: 0.8805 - val_accuracy: 0.5893 - val_f1_metric: 0.5602 - lr: 0.0010\n",
      "Epoch 21/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4020 - accuracy: 0.6094 - f1_metric: 0.58 - ETA: 0s - loss: 0.4557 - accuracy: 0.5859 - f1_metric: 0.55 - ETA: 0s - loss: 0.4815 - accuracy: 0.5911 - f1_metric: 0.56 - ETA: 0s - loss: 0.4836 - accuracy: 0.6250 - f1_metric: 0.59 - ETA: 0s - loss: 0.5176 - accuracy: 0.6438 - f1_metric: 0.61 - ETA: 0s - loss: 0.5239 - accuracy: 0.6576 - f1_metric: 0.63 - ETA: 0s - loss: 0.5333 - accuracy: 0.6696 - f1_metric: 0.64 - ETA: 0s - loss: 0.5277 - accuracy: 0.6768 - f1_metric: 0.65 - ETA: 0s - loss: 0.5364 - accuracy: 0.6823 - f1_metric: 0.65 - ETA: 0s - loss: 0.5375 - accuracy: 0.6695 - f1_metric: 0.64 - ETA: 0s - loss: 0.5369 - accuracy: 0.6634 - f1_metric: 0.64 - ETA: 0s - loss: 0.5363 - accuracy: 0.6519 - f1_metric: 0.62 - ETA: 0s - loss: 0.5255 - accuracy: 0.6507 - f1_metric: 0.62 - ETA: 0s - loss: 0.5377 - accuracy: 0.6522 - f1_metric: 0.63 - ETA: 0s - loss: 0.5381 - accuracy: 0.6572 - f1_metric: 0.63 - ETA: 0s - loss: 0.5392 - accuracy: 0.6589 - f1_metric: 0.64 - ETA: 0s - loss: 0.5373 - accuracy: 0.6579 - f1_metric: 0.63 - ETA: 0s - loss: 0.5421 - accuracy: 0.6574 - f1_metric: 0.63 - ETA: 0s - loss: 0.5411 - accuracy: 0.6548 - f1_metric: 0.6346\n",
      "Epoch 00021: val_f1_metric did not improve from 0.61760\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.5403 - accuracy: 0.6547 - f1_metric: 0.6330 - val_loss: 0.9329 - val_accuracy: 0.5759 - val_f1_metric: 0.5507 - lr: 0.0010\n",
      "Epoch 22/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.4846 - accuracy: 0.5781 - f1_metric: 0.59 - ETA: 1s - loss: 0.6602 - accuracy: 0.5547 - f1_metric: 0.54 - ETA: 3s - loss: 0.6304 - accuracy: 0.5938 - f1_metric: 0.58 - ETA: 3s - loss: 0.6095 - accuracy: 0.5898 - f1_metric: 0.58 - ETA: 3s - loss: 0.6498 - accuracy: 0.5969 - f1_metric: 0.59 - ETA: 3s - loss: 0.6274 - accuracy: 0.6094 - f1_metric: 0.60 - ETA: 3s - loss: 0.6232 - accuracy: 0.6071 - f1_metric: 0.59 - ETA: 3s - loss: 0.6083 - accuracy: 0.6113 - f1_metric: 0.59 - ETA: 3s - loss: 0.5878 - accuracy: 0.6250 - f1_metric: 0.61 - ETA: 3s - loss: 0.5716 - accuracy: 0.6219 - f1_metric: 0.60 - ETA: 2s - loss: 0.5646 - accuracy: 0.6136 - f1_metric: 0.59 - ETA: 2s - loss: 0.5609 - accuracy: 0.6159 - f1_metric: 0.60 - ETA: 2s - loss: 0.5565 - accuracy: 0.6226 - f1_metric: 0.60 - ETA: 2s - loss: 0.5571 - accuracy: 0.6306 - f1_metric: 0.61 - ETA: 2s - loss: 0.5440 - accuracy: 0.6354 - f1_metric: 0.62 - ETA: 2s - loss: 0.5456 - accuracy: 0.6396 - f1_metric: 0.62 - ETA: 2s - loss: 0.5507 - accuracy: 0.6517 - f1_metric: 0.63 - ETA: 2s - loss: 0.5563 - accuracy: 0.6606 - f1_metric: 0.64 - ETA: 2s - loss: 0.5561 - accuracy: 0.6653 - f1_metric: 0.65 - ETA: 2s - loss: 0.5455 - accuracy: 0.6680 - f1_metric: 0.65 - ETA: 2s - loss: 0.5369 - accuracy: 0.6763 - f1_metric: 0.66 - ETA: 1s - loss: 0.5426 - accuracy: 0.6747 - f1_metric: 0.66 - ETA: 1s - loss: 0.5402 - accuracy: 0.6780 - f1_metric: 0.66 - ETA: 1s - loss: 0.5485 - accuracy: 0.6777 - f1_metric: 0.66 - ETA: 1s - loss: 0.5517 - accuracy: 0.6750 - f1_metric: 0.66 - ETA: 1s - loss: 0.5491 - accuracy: 0.6695 - f1_metric: 0.65 - ETA: 1s - loss: 0.5455 - accuracy: 0.6690 - f1_metric: 0.65 - ETA: 1s - loss: 0.5396 - accuracy: 0.6685 - f1_metric: 0.65 - ETA: 1s - loss: 0.5438 - accuracy: 0.6665 - f1_metric: 0.65 - ETA: 1s - loss: 0.5393 - accuracy: 0.6646 - f1_metric: 0.64 - ETA: 1s - loss: 0.5364 - accuracy: 0.6643 - f1_metric: 0.64 - ETA: 1s - loss: 0.5429 - accuracy: 0.6616 - f1_metric: 0.64 - ETA: 0s - loss: 0.5417 - accuracy: 0.6605 - f1_metric: 0.64 - ETA: 0s - loss: 0.5413 - accuracy: 0.6595 - f1_metric: 0.64 - ETA: 0s - loss: 0.5406 - accuracy: 0.6580 - f1_metric: 0.64 - ETA: 0s - loss: 0.5398 - accuracy: 0.6576 - f1_metric: 0.64 - ETA: 0s - loss: 0.5408 - accuracy: 0.6567 - f1_metric: 0.64 - ETA: 0s - loss: 0.5381 - accuracy: 0.6567 - f1_metric: 0.64 - ETA: 0s - loss: 0.5411 - accuracy: 0.6582 - f1_metric: 0.64 - ETA: 0s - loss: 0.5375 - accuracy: 0.6624 - f1_metric: 0.6481\n",
      "Epoch 00022: val_f1_metric improved from 0.61760 to 0.69328, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 8s 182ms/step - loss: 0.5349 - accuracy: 0.6644 - f1_metric: 0.6524 - val_loss: 0.6936 - val_accuracy: 0.7049 - val_f1_metric: 0.6933 - lr: 0.0010\n",
      "Epoch 23/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.6719 - f1_metric: 0.65 - ETA: 0s - loss: 0.4788 - accuracy: 0.6992 - f1_metric: 0.68 - ETA: 0s - loss: 0.5003 - accuracy: 0.6875 - f1_metric: 0.67 - ETA: 0s - loss: 0.4936 - accuracy: 0.7012 - f1_metric: 0.68 - ETA: 0s - loss: 0.4920 - accuracy: 0.7016 - f1_metric: 0.69 - ETA: 0s - loss: 0.4874 - accuracy: 0.7055 - f1_metric: 0.69 - ETA: 0s - loss: 0.4851 - accuracy: 0.7104 - f1_metric: 0.70 - ETA: 0s - loss: 0.4832 - accuracy: 0.7050 - f1_metric: 0.69 - ETA: 0s - loss: 0.4956 - accuracy: 0.7055 - f1_metric: 0.69 - ETA: 0s - loss: 0.4859 - accuracy: 0.6989 - f1_metric: 0.68 - ETA: 0s - loss: 0.4845 - accuracy: 0.6988 - f1_metric: 0.69 - ETA: 0s - loss: 0.4885 - accuracy: 0.6979 - f1_metric: 0.68 - ETA: 0s - loss: 0.4821 - accuracy: 0.7015 - f1_metric: 0.69 - ETA: 0s - loss: 0.4838 - accuracy: 0.7031 - f1_metric: 0.69 - ETA: 0s - loss: 0.4804 - accuracy: 0.7079 - f1_metric: 0.70 - ETA: 0s - loss: 0.4870 - accuracy: 0.7082 - f1_metric: 0.70 - ETA: 0s - loss: 0.4914 - accuracy: 0.7079 - f1_metric: 0.70 - ETA: 0s - loss: 0.4913 - accuracy: 0.7081 - f1_metric: 0.70 - ETA: 0s - loss: 0.4897 - accuracy: 0.7070 - f1_metric: 0.69 - ETA: 0s - loss: 0.4894 - accuracy: 0.7024 - f1_metric: 0.69 - ETA: 0s - loss: 0.4970 - accuracy: 0.7000 - f1_metric: 0.6931\n",
      "Epoch 00023: val_f1_metric did not improve from 0.69328\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.4970 - accuracy: 0.7000 - f1_metric: 0.6931 - val_loss: 0.8189 - val_accuracy: 0.6379 - val_f1_metric: 0.6245 - lr: 0.0010\n",
      "Epoch 24/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.6406 - f1_metric: 0.62 - ETA: 0s - loss: 0.4638 - accuracy: 0.6562 - f1_metric: 0.63 - ETA: 0s - loss: 0.5375 - accuracy: 0.6406 - f1_metric: 0.61 - ETA: 0s - loss: 0.5717 - accuracy: 0.6406 - f1_metric: 0.61 - ETA: 0s - loss: 0.5491 - accuracy: 0.6562 - f1_metric: 0.63 - ETA: 0s - loss: 0.5549 - accuracy: 0.6536 - f1_metric: 0.63 - ETA: 0s - loss: 0.5493 - accuracy: 0.6585 - f1_metric: 0.63 - ETA: 0s - loss: 0.5472 - accuracy: 0.6543 - f1_metric: 0.63 - ETA: 0s - loss: 0.5502 - accuracy: 0.6562 - f1_metric: 0.63 - ETA: 0s - loss: 0.5422 - accuracy: 0.6594 - f1_metric: 0.63 - ETA: 0s - loss: 0.5310 - accuracy: 0.6739 - f1_metric: 0.65 - ETA: 0s - loss: 0.5311 - accuracy: 0.6787 - f1_metric: 0.65 - ETA: 0s - loss: 0.5298 - accuracy: 0.6829 - f1_metric: 0.66 - ETA: 0s - loss: 0.5323 - accuracy: 0.6823 - f1_metric: 0.66 - ETA: 0s - loss: 0.5327 - accuracy: 0.6747 - f1_metric: 0.65 - ETA: 0s - loss: 0.5268 - accuracy: 0.6762 - f1_metric: 0.65 - ETA: 0s - loss: 0.5246 - accuracy: 0.6783 - f1_metric: 0.66 - ETA: 0s - loss: 0.5228 - accuracy: 0.6803 - f1_metric: 0.66 - ETA: 0s - loss: 0.5194 - accuracy: 0.6809 - f1_metric: 0.6642\n",
      "Epoch 00024: val_f1_metric improved from 0.69328 to 0.73288, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.5194 - accuracy: 0.6809 - f1_metric: 0.6642 - val_loss: 0.6381 - val_accuracy: 0.7460 - val_f1_metric: 0.7329 - lr: 0.0010\n",
      "Epoch 25/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3750 - accuracy: 0.7031 - f1_metric: 0.69 - ETA: 0s - loss: 0.4295 - accuracy: 0.7422 - f1_metric: 0.74 - ETA: 0s - loss: 0.5049 - accuracy: 0.7396 - f1_metric: 0.73 - ETA: 0s - loss: 0.4865 - accuracy: 0.7363 - f1_metric: 0.73 - ETA: 0s - loss: 0.4829 - accuracy: 0.7250 - f1_metric: 0.72 - ETA: 0s - loss: 0.4934 - accuracy: 0.7109 - f1_metric: 0.70 - ETA: 0s - loss: 0.5075 - accuracy: 0.6953 - f1_metric: 0.68 - ETA: 0s - loss: 0.5061 - accuracy: 0.6895 - f1_metric: 0.68 - ETA: 0s - loss: 0.5002 - accuracy: 0.6892 - f1_metric: 0.67 - ETA: 0s - loss: 0.4945 - accuracy: 0.6867 - f1_metric: 0.67 - ETA: 0s - loss: 0.5002 - accuracy: 0.6896 - f1_metric: 0.68 - ETA: 0s - loss: 0.5018 - accuracy: 0.6923 - f1_metric: 0.68 - ETA: 0s - loss: 0.4973 - accuracy: 0.6977 - f1_metric: 0.69 - ETA: 0s - loss: 0.4906 - accuracy: 0.7009 - f1_metric: 0.69 - ETA: 0s - loss: 0.4927 - accuracy: 0.7053 - f1_metric: 0.69 - ETA: 0s - loss: 0.4917 - accuracy: 0.7017 - f1_metric: 0.69 - ETA: 0s - loss: 0.4920 - accuracy: 0.6984 - f1_metric: 0.68 - ETA: 0s - loss: 0.4915 - accuracy: 0.6951 - f1_metric: 0.68 - ETA: 0s - loss: 0.4913 - accuracy: 0.6955 - f1_metric: 0.68 - ETA: 0s - loss: 0.4900 - accuracy: 0.6951 - f1_metric: 0.68 - ETA: 0s - loss: 0.4936 - accuracy: 0.6951 - f1_metric: 0.68 - ETA: 0s - loss: 0.4913 - accuracy: 0.6973 - f1_metric: 0.6861\n",
      "Epoch 00025: val_f1_metric did not improve from 0.73288\n",
      "44/44 [==============================] - 2s 37ms/step - loss: 0.4910 - accuracy: 0.6974 - f1_metric: 0.6869 - val_loss: 0.6683 - val_accuracy: 0.7234 - val_f1_metric: 0.7163 - lr: 0.0010\n",
      "Epoch 26/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4397 - accuracy: 0.6875 - f1_metric: 0.68 - ETA: 0s - loss: 0.4419 - accuracy: 0.7148 - f1_metric: 0.70 - ETA: 0s - loss: 0.4541 - accuracy: 0.7321 - f1_metric: 0.72 - ETA: 0s - loss: 0.4387 - accuracy: 0.7465 - f1_metric: 0.73 - ETA: 0s - loss: 0.4451 - accuracy: 0.7415 - f1_metric: 0.73 - ETA: 0s - loss: 0.4866 - accuracy: 0.7392 - f1_metric: 0.73 - ETA: 0s - loss: 0.4898 - accuracy: 0.7314 - f1_metric: 0.72 - ETA: 0s - loss: 0.4842 - accuracy: 0.7144 - f1_metric: 0.70 - ETA: 0s - loss: 0.4796 - accuracy: 0.7068 - f1_metric: 0.69 - ETA: 0s - loss: 0.4820 - accuracy: 0.6927 - f1_metric: 0.68 - ETA: 0s - loss: 0.4866 - accuracy: 0.6927 - f1_metric: 0.68 - ETA: 0s - loss: 0.4803 - accuracy: 0.6995 - f1_metric: 0.68 - ETA: 0s - loss: 0.4841 - accuracy: 0.7064 - f1_metric: 0.69 - ETA: 0s - loss: 0.4768 - accuracy: 0.7140 - f1_metric: 0.70 - ETA: 0s - loss: 0.4768 - accuracy: 0.7179 - f1_metric: 0.70 - ETA: 0s - loss: 0.4761 - accuracy: 0.7143 - f1_metric: 0.7038\n",
      "Epoch 00026: val_f1_metric did not improve from 0.73288\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.4738 - accuracy: 0.7125 - f1_metric: 0.7019 - val_loss: 0.7544 - val_accuracy: 0.6873 - val_f1_metric: 0.6792 - lr: 0.0010\n",
      "Epoch 27/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.4632 - accuracy: 0.7383 - f1_metric: 0.72 - ETA: 0s - loss: 0.4361 - accuracy: 0.7545 - f1_metric: 0.74 - ETA: 0s - loss: 0.4317 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.4270 - accuracy: 0.7692 - f1_metric: 0.76 - ETA: 0s - loss: 0.4399 - accuracy: 0.7734 - f1_metric: 0.76 - ETA: 0s - loss: 0.4394 - accuracy: 0.7673 - f1_metric: 0.76 - ETA: 0s - loss: 0.4478 - accuracy: 0.7597 - f1_metric: 0.75 - ETA: 0s - loss: 0.4549 - accuracy: 0.7351 - f1_metric: 0.72 - ETA: 0s - loss: 0.4538 - accuracy: 0.7230 - f1_metric: 0.71 - ETA: 0s - loss: 0.4579 - accuracy: 0.7177 - f1_metric: 0.70 - ETA: 0s - loss: 0.4715 - accuracy: 0.7251 - f1_metric: 0.71 - ETA: 0s - loss: 0.4677 - accuracy: 0.7241 - f1_metric: 0.71 - ETA: 0s - loss: 0.4746 - accuracy: 0.7229 - f1_metric: 0.71 - ETA: 0s - loss: 0.4791 - accuracy: 0.7165 - f1_metric: 0.70 - ETA: 0s - loss: 0.4762 - accuracy: 0.7172 - f1_metric: 0.7072\n",
      "Epoch 00027: val_f1_metric did not improve from 0.73288\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.4762 - accuracy: 0.7172 - f1_metric: 0.7072 - val_loss: 0.7280 - val_accuracy: 0.6806 - val_f1_metric: 0.6685 - lr: 0.0010\n",
      "Epoch 28/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.7031 - f1_metric: 0.70 - ETA: 0s - loss: 0.4510 - accuracy: 0.6953 - f1_metric: 0.69 - ETA: 0s - loss: 0.4295 - accuracy: 0.7076 - f1_metric: 0.70 - ETA: 0s - loss: 0.4398 - accuracy: 0.7094 - f1_metric: 0.69 - ETA: 0s - loss: 0.4493 - accuracy: 0.7200 - f1_metric: 0.70 - ETA: 0s - loss: 0.4433 - accuracy: 0.7266 - f1_metric: 0.71 - ETA: 0s - loss: 0.4338 - accuracy: 0.7451 - f1_metric: 0.73 - ETA: 0s - loss: 0.4298 - accuracy: 0.7515 - f1_metric: 0.74 - ETA: 0s - loss: 0.4212 - accuracy: 0.7637 - f1_metric: 0.75 - ETA: 0s - loss: 0.4408 - accuracy: 0.7578 - f1_metric: 0.74 - ETA: 0s - loss: 0.4435 - accuracy: 0.7473 - f1_metric: 0.73 - ETA: 0s - loss: 0.4467 - accuracy: 0.7388 - f1_metric: 0.72 - ETA: 0s - loss: 0.4500 - accuracy: 0.7321 - f1_metric: 0.72 - ETA: 0s - loss: 0.4511 - accuracy: 0.7323 - f1_metric: 0.72 - ETA: 0s - loss: 0.4526 - accuracy: 0.7302 - f1_metric: 0.72 - ETA: 0s - loss: 0.4550 - accuracy: 0.7326 - f1_metric: 0.7250\n",
      "Epoch 00028: val_f1_metric improved from 0.73288 to 0.74414, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 122ms/step - loss: 0.4550 - accuracy: 0.7323 - f1_metric: 0.7246 - val_loss: 0.6288 - val_accuracy: 0.7527 - val_f1_metric: 0.7441 - lr: 0.0010\n",
      "Epoch 29/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3181 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.4199 - accuracy: 0.7070 - f1_metric: 0.71 - ETA: 0s - loss: 0.4682 - accuracy: 0.7031 - f1_metric: 0.70 - ETA: 0s - loss: 0.4681 - accuracy: 0.7078 - f1_metric: 0.70 - ETA: 0s - loss: 0.4713 - accuracy: 0.7148 - f1_metric: 0.71 - ETA: 0s - loss: 0.4786 - accuracy: 0.7054 - f1_metric: 0.70 - ETA: 0s - loss: 0.4722 - accuracy: 0.7002 - f1_metric: 0.69 - ETA: 0s - loss: 0.4733 - accuracy: 0.6957 - f1_metric: 0.68 - ETA: 0s - loss: 0.4740 - accuracy: 0.7053 - f1_metric: 0.69 - ETA: 0s - loss: 0.4662 - accuracy: 0.7161 - f1_metric: 0.70 - ETA: 0s - loss: 0.4558 - accuracy: 0.7236 - f1_metric: 0.71 - ETA: 0s - loss: 0.4569 - accuracy: 0.7232 - f1_metric: 0.71 - ETA: 0s - loss: 0.4580 - accuracy: 0.7252 - f1_metric: 0.71 - ETA: 0s - loss: 0.4521 - accuracy: 0.7266 - f1_metric: 0.72 - ETA: 0s - loss: 0.4453 - accuracy: 0.7268 - f1_metric: 0.72 - ETA: 0s - loss: 0.4410 - accuracy: 0.7262 - f1_metric: 0.72 - ETA: 0s - loss: 0.4411 - accuracy: 0.7289 - f1_metric: 0.72 - ETA: 0s - loss: 0.4446 - accuracy: 0.7325 - f1_metric: 0.7273\n",
      "Epoch 00029: val_f1_metric improved from 0.74414 to 0.74853, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 130ms/step - loss: 0.4540 - accuracy: 0.7348 - f1_metric: 0.7302 - val_loss: 0.6232 - val_accuracy: 0.7561 - val_f1_metric: 0.7485 - lr: 0.0010\n",
      "Epoch 30/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8281 - f1_metric: 0.81 - ETA: 0s - loss: 0.4254 - accuracy: 0.6914 - f1_metric: 0.68 - ETA: 0s - loss: 0.4537 - accuracy: 0.6693 - f1_metric: 0.65 - ETA: 0s - loss: 0.4506 - accuracy: 0.6680 - f1_metric: 0.65 - ETA: 0s - loss: 0.4565 - accuracy: 0.6828 - f1_metric: 0.67 - ETA: 0s - loss: 0.4742 - accuracy: 0.6927 - f1_metric: 0.68 - ETA: 0s - loss: 0.4789 - accuracy: 0.6947 - f1_metric: 0.68 - ETA: 0s - loss: 0.4617 - accuracy: 0.6927 - f1_metric: 0.68 - ETA: 0s - loss: 0.4656 - accuracy: 0.6904 - f1_metric: 0.68 - ETA: 0s - loss: 0.4544 - accuracy: 0.6949 - f1_metric: 0.68 - ETA: 0s - loss: 0.4449 - accuracy: 0.7054 - f1_metric: 0.69 - ETA: 0s - loss: 0.4491 - accuracy: 0.7102 - f1_metric: 0.70 - ETA: 0s - loss: 0.4600 - accuracy: 0.7161 - f1_metric: 0.70 - ETA: 0s - loss: 0.4550 - accuracy: 0.7151 - f1_metric: 0.70 - ETA: 0s - loss: 0.4510 - accuracy: 0.7182 - f1_metric: 0.71 - ETA: 0s - loss: 0.4475 - accuracy: 0.7224 - f1_metric: 0.71 - ETA: 0s - loss: 0.4499 - accuracy: 0.7238 - f1_metric: 0.71 - ETA: 0s - loss: 0.4472 - accuracy: 0.7279 - f1_metric: 0.72 - ETA: 0s - loss: 0.4546 - accuracy: 0.7230 - f1_metric: 0.71 - ETA: 0s - loss: 0.4562 - accuracy: 0.7258 - f1_metric: 0.71 - ETA: 0s - loss: 0.4493 - accuracy: 0.7282 - f1_metric: 0.7203\n",
      "Epoch 00030: val_f1_metric did not improve from 0.74853\n",
      "44/44 [==============================] - 2s 37ms/step - loss: 0.4482 - accuracy: 0.7276 - f1_metric: 0.7193 - val_loss: 0.6471 - val_accuracy: 0.7418 - val_f1_metric: 0.7366 - lr: 0.0010\n",
      "Epoch 31/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.4666 - accuracy: 0.6562 - f1_metric: 0.62 - ETA: 0s - loss: 0.4056 - accuracy: 0.6771 - f1_metric: 0.66 - ETA: 0s - loss: 0.3849 - accuracy: 0.7312 - f1_metric: 0.72 - ETA: 0s - loss: 0.4098 - accuracy: 0.7715 - f1_metric: 0.76 - ETA: 0s - loss: 0.4187 - accuracy: 0.7699 - f1_metric: 0.76 - ETA: 0s - loss: 0.4276 - accuracy: 0.7701 - f1_metric: 0.76 - ETA: 0s - loss: 0.4254 - accuracy: 0.7592 - f1_metric: 0.75 - ETA: 0s - loss: 0.4298 - accuracy: 0.7578 - f1_metric: 0.75 - ETA: 0s - loss: 0.4318 - accuracy: 0.7473 - f1_metric: 0.74 - ETA: 0s - loss: 0.4366 - accuracy: 0.7456 - f1_metric: 0.73 - ETA: 0s - loss: 0.4340 - accuracy: 0.7416 - f1_metric: 0.73 - ETA: 0s - loss: 0.4342 - accuracy: 0.7417 - f1_metric: 0.73 - ETA: 0s - loss: 0.4281 - accuracy: 0.7438 - f1_metric: 0.73 - ETA: 0s - loss: 0.4326 - accuracy: 0.7429 - f1_metric: 0.73 - ETA: 0s - loss: 0.4333 - accuracy: 0.7455 - f1_metric: 0.74 - ETA: 0s - loss: 0.4297 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.4313 - accuracy: 0.7488 - f1_metric: 0.7438\n",
      "Epoch 00031: val_f1_metric did not improve from 0.74853\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.4313 - accuracy: 0.7488 - f1_metric: 0.7438 - val_loss: 0.6459 - val_accuracy: 0.7460 - val_f1_metric: 0.7399 - lr: 0.0010\n",
      "Epoch 32/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3818 - accuracy: 0.7344 - f1_metric: 0.74 - ETA: 0s - loss: 0.4494 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4540 - accuracy: 0.7254 - f1_metric: 0.72 - ETA: 0s - loss: 0.4438 - accuracy: 0.7437 - f1_metric: 0.73 - ETA: 0s - loss: 0.4356 - accuracy: 0.7474 - f1_metric: 0.74 - ETA: 0s - loss: 0.4360 - accuracy: 0.7469 - f1_metric: 0.74 - ETA: 0s - loss: 0.4396 - accuracy: 0.7283 - f1_metric: 0.72 - ETA: 0s - loss: 0.4306 - accuracy: 0.7277 - f1_metric: 0.72 - ETA: 0s - loss: 0.4325 - accuracy: 0.7435 - f1_metric: 0.73 - ETA: 0s - loss: 0.4262 - accuracy: 0.7517 - f1_metric: 0.74 - ETA: 0s - loss: 0.4199 - accuracy: 0.7609 - f1_metric: 0.75 - ETA: 0s - loss: 0.4317 - accuracy: 0.7604 - f1_metric: 0.75 - ETA: 0s - loss: 0.4367 - accuracy: 0.7526 - f1_metric: 0.74 - ETA: 0s - loss: 0.4349 - accuracy: 0.7459 - f1_metric: 0.73 - ETA: 0s - loss: 0.4369 - accuracy: 0.7412 - f1_metric: 0.73 - ETA: 0s - loss: 0.4373 - accuracy: 0.7402 - f1_metric: 0.7302\n",
      "Epoch 00032: val_f1_metric improved from 0.74853 to 0.77867, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 126ms/step - loss: 0.4373 - accuracy: 0.7402 - f1_metric: 0.7302 - val_loss: 0.5612 - val_accuracy: 0.7871 - val_f1_metric: 0.7787 - lr: 0.0010\n",
      "Epoch 33/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4563 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3963 - accuracy: 0.8393 - f1_metric: 0.83 - ETA: 0s - loss: 0.4753 - accuracy: 0.8125 - f1_metric: 0.80 - ETA: 0s - loss: 0.4566 - accuracy: 0.8026 - f1_metric: 0.80 - ETA: 0s - loss: 0.4548 - accuracy: 0.7752 - f1_metric: 0.77 - ETA: 0s - loss: 0.4488 - accuracy: 0.7615 - f1_metric: 0.75 - ETA: 0s - loss: 0.4533 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.4464 - accuracy: 0.7426 - f1_metric: 0.73 - ETA: 0s - loss: 0.4500 - accuracy: 0.7408 - f1_metric: 0.73 - ETA: 0s - loss: 0.4496 - accuracy: 0.7454 - f1_metric: 0.74 - ETA: 0s - loss: 0.4469 - accuracy: 0.7470 - f1_metric: 0.74 - ETA: 0s - loss: 0.4432 - accuracy: 0.7532 - f1_metric: 0.74 - ETA: 0s - loss: 0.4439 - accuracy: 0.7526 - f1_metric: 0.74 - ETA: 0s - loss: 0.4422 - accuracy: 0.7529 - f1_metric: 0.74 - ETA: 0s - loss: 0.4347 - accuracy: 0.7541 - f1_metric: 0.75 - ETA: 0s - loss: 0.4404 - accuracy: 0.7522 - f1_metric: 0.74 - ETA: 0s - loss: 0.4357 - accuracy: 0.7541 - f1_metric: 0.75 - ETA: 0s - loss: 0.4324 - accuracy: 0.7578 - f1_metric: 0.75 - ETA: 0s - loss: 0.4335 - accuracy: 0.7588 - f1_metric: 0.75 - ETA: 0s - loss: 0.4317 - accuracy: 0.7598 - f1_metric: 0.7571\n",
      "Epoch 00033: val_f1_metric did not improve from 0.77867\n",
      "44/44 [==============================] - 2s 38ms/step - loss: 0.4364 - accuracy: 0.7607 - f1_metric: 0.7586 - val_loss: 0.6492 - val_accuracy: 0.7460 - val_f1_metric: 0.7406 - lr: 0.0010\n",
      "Epoch 34/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3335 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3975 - accuracy: 0.7396 - f1_metric: 0.74 - ETA: 0s - loss: 0.3901 - accuracy: 0.7125 - f1_metric: 0.71 - ETA: 0s - loss: 0.3949 - accuracy: 0.7009 - f1_metric: 0.69 - ETA: 0s - loss: 0.3952 - accuracy: 0.7049 - f1_metric: 0.70 - ETA: 0s - loss: 0.4027 - accuracy: 0.7145 - f1_metric: 0.71 - ETA: 0s - loss: 0.4237 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.4267 - accuracy: 0.7363 - f1_metric: 0.72 - ETA: 0s - loss: 0.4214 - accuracy: 0.7378 - f1_metric: 0.73 - ETA: 0s - loss: 0.4307 - accuracy: 0.7414 - f1_metric: 0.73 - ETA: 0s - loss: 0.4349 - accuracy: 0.7386 - f1_metric: 0.73 - ETA: 0s - loss: 0.4415 - accuracy: 0.7400 - f1_metric: 0.73 - ETA: 0s - loss: 0.4412 - accuracy: 0.7297 - f1_metric: 0.72 - ETA: 0s - loss: 0.4354 - accuracy: 0.7234 - f1_metric: 0.71 - ETA: 0s - loss: 0.4399 - accuracy: 0.7256 - f1_metric: 0.72 - ETA: 0s - loss: 0.4406 - accuracy: 0.7275 - f1_metric: 0.72 - ETA: 0s - loss: 0.4382 - accuracy: 0.7335 - f1_metric: 0.72 - ETA: 0s - loss: 0.4346 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.4390 - accuracy: 0.7355 - f1_metric: 0.73 - ETA: 0s - loss: 0.4369 - accuracy: 0.7329 - f1_metric: 0.7279\n",
      "Epoch 00034: val_f1_metric did not improve from 0.77867\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.4357 - accuracy: 0.7327 - f1_metric: 0.7275 - val_loss: 0.6773 - val_accuracy: 0.7309 - val_f1_metric: 0.7254 - lr: 0.0010\n",
      "Epoch 35/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3792 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.3988 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.3783 - accuracy: 0.7478 - f1_metric: 0.74 - ETA: 0s - loss: 0.3944 - accuracy: 0.7674 - f1_metric: 0.76 - ETA: 0s - loss: 0.4097 - accuracy: 0.7727 - f1_metric: 0.77 - ETA: 0s - loss: 0.4141 - accuracy: 0.7712 - f1_metric: 0.77 - ETA: 0s - loss: 0.4034 - accuracy: 0.7822 - f1_metric: 0.77 - ETA: 0s - loss: 0.3993 - accuracy: 0.7805 - f1_metric: 0.77 - ETA: 0s - loss: 0.4123 - accuracy: 0.7717 - f1_metric: 0.76 - ETA: 0s - loss: 0.4090 - accuracy: 0.7719 - f1_metric: 0.76 - ETA: 0s - loss: 0.4088 - accuracy: 0.7662 - f1_metric: 0.76 - ETA: 0s - loss: 0.4059 - accuracy: 0.7661 - f1_metric: 0.76 - ETA: 0s - loss: 0.4130 - accuracy: 0.7574 - f1_metric: 0.75 - ETA: 0s - loss: 0.4123 - accuracy: 0.7580 - f1_metric: 0.75 - ETA: 0s - loss: 0.4192 - accuracy: 0.7582 - f1_metric: 0.75 - ETA: 0s - loss: 0.4231 - accuracy: 0.7525 - f1_metric: 0.7476\n",
      "Epoch 00035: val_f1_metric did not improve from 0.77867\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.4222 - accuracy: 0.7517 - f1_metric: 0.7460 - val_loss: 0.6942 - val_accuracy: 0.7209 - val_f1_metric: 0.7138 - lr: 0.0010\n",
      "Epoch 36/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3746 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.4020 - accuracy: 0.7070 - f1_metric: 0.70 - ETA: 0s - loss: 0.3899 - accuracy: 0.7388 - f1_metric: 0.73 - ETA: 0s - loss: 0.4049 - accuracy: 0.7453 - f1_metric: 0.74 - ETA: 0s - loss: 0.4062 - accuracy: 0.7428 - f1_metric: 0.74 - ETA: 0s - loss: 0.4024 - accuracy: 0.7461 - f1_metric: 0.74 - ETA: 0s - loss: 0.3976 - accuracy: 0.7508 - f1_metric: 0.74 - ETA: 0s - loss: 0.4028 - accuracy: 0.7557 - f1_metric: 0.75 - ETA: 0s - loss: 0.3975 - accuracy: 0.7650 - f1_metric: 0.76 - ETA: 0s - loss: 0.3983 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3925 - accuracy: 0.7681 - f1_metric: 0.76 - ETA: 0s - loss: 0.3935 - accuracy: 0.7684 - f1_metric: 0.76 - ETA: 0s - loss: 0.3963 - accuracy: 0.7677 - f1_metric: 0.76 - ETA: 0s - loss: 0.3976 - accuracy: 0.7680 - f1_metric: 0.76 - ETA: 0s - loss: 0.4006 - accuracy: 0.7649 - f1_metric: 0.7623\n",
      "Epoch 00036: val_f1_metric did not improve from 0.77867\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.4009 - accuracy: 0.7646 - f1_metric: 0.7619 - val_loss: 0.6897 - val_accuracy: 0.7200 - val_f1_metric: 0.7140 - lr: 0.0010\n",
      "Epoch 37/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.6222 - accuracy: 0.7031 - f1_metric: 0.70 - ETA: 0s - loss: 0.4872 - accuracy: 0.7070 - f1_metric: 0.69 - ETA: 0s - loss: 0.4431 - accuracy: 0.7143 - f1_metric: 0.70 - ETA: 0s - loss: 0.4136 - accuracy: 0.7453 - f1_metric: 0.73 - ETA: 0s - loss: 0.4135 - accuracy: 0.7632 - f1_metric: 0.75 - ETA: 0s - loss: 0.4154 - accuracy: 0.7627 - f1_metric: 0.75 - ETA: 0s - loss: 0.4137 - accuracy: 0.7697 - f1_metric: 0.76 - ETA: 0s - loss: 0.4077 - accuracy: 0.7741 - f1_metric: 0.77 - ETA: 0s - loss: 0.4103 - accuracy: 0.7713 - f1_metric: 0.76 - ETA: 0s - loss: 0.4040 - accuracy: 0.7785 - f1_metric: 0.77 - ETA: 0s - loss: 0.4021 - accuracy: 0.7732 - f1_metric: 0.76 - ETA: 0s - loss: 0.4045 - accuracy: 0.7780 - f1_metric: 0.77 - ETA: 0s - loss: 0.4034 - accuracy: 0.7817 - f1_metric: 0.77 - ETA: 0s - loss: 0.4010 - accuracy: 0.7833 - f1_metric: 0.77 - ETA: 0s - loss: 0.4062 - accuracy: 0.7812 - f1_metric: 0.7769\n",
      "Epoch 00037: val_f1_metric did not improve from 0.77867\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.4053 - accuracy: 0.7787 - f1_metric: 0.7732 - val_loss: 0.7543 - val_accuracy: 0.6882 - val_f1_metric: 0.6834 - lr: 0.0010\n",
      "Epoch 38/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5109 - accuracy: 0.6406 - f1_metric: 0.63 - ETA: 0s - loss: 0.4267 - accuracy: 0.6992 - f1_metric: 0.69 - ETA: 0s - loss: 0.4055 - accuracy: 0.7165 - f1_metric: 0.70 - ETA: 0s - loss: 0.4093 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.3943 - accuracy: 0.7752 - f1_metric: 0.76 - ETA: 0s - loss: 0.4075 - accuracy: 0.7832 - f1_metric: 0.77 - ETA: 0s - loss: 0.4038 - accuracy: 0.7738 - f1_metric: 0.76 - ETA: 0s - loss: 0.4004 - accuracy: 0.7678 - f1_metric: 0.76 - ETA: 0s - loss: 0.4021 - accuracy: 0.7600 - f1_metric: 0.75 - ETA: 0s - loss: 0.4109 - accuracy: 0.7573 - f1_metric: 0.75 - ETA: 0s - loss: 0.4058 - accuracy: 0.7626 - f1_metric: 0.75 - ETA: 0s - loss: 0.4117 - accuracy: 0.7642 - f1_metric: 0.75 - ETA: 0s - loss: 0.4129 - accuracy: 0.7555 - f1_metric: 0.75 - ETA: 0s - loss: 0.4109 - accuracy: 0.7560 - f1_metric: 0.75 - ETA: 0s - loss: 0.4058 - accuracy: 0.7582 - f1_metric: 0.7540\n",
      "Epoch 00038: val_f1_metric improved from 0.77867 to 0.78868, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 110ms/step - loss: 0.4027 - accuracy: 0.7621 - f1_metric: 0.7590 - val_loss: 0.5470 - val_accuracy: 0.7904 - val_f1_metric: 0.7887 - lr: 0.0010\n",
      "Epoch 39/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3614 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.3849 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.4142 - accuracy: 0.8058 - f1_metric: 0.80 - ETA: 0s - loss: 0.4166 - accuracy: 0.8062 - f1_metric: 0.80 - ETA: 0s - loss: 0.4131 - accuracy: 0.7885 - f1_metric: 0.78 - ETA: 0s - loss: 0.4216 - accuracy: 0.7823 - f1_metric: 0.78 - ETA: 0s - loss: 0.4227 - accuracy: 0.7702 - f1_metric: 0.76 - ETA: 0s - loss: 0.4135 - accuracy: 0.7633 - f1_metric: 0.76 - ETA: 0s - loss: 0.4141 - accuracy: 0.7685 - f1_metric: 0.76 - ETA: 0s - loss: 0.4236 - accuracy: 0.7650 - f1_metric: 0.76 - ETA: 0s - loss: 0.4241 - accuracy: 0.7627 - f1_metric: 0.76 - ETA: 0s - loss: 0.4282 - accuracy: 0.7573 - f1_metric: 0.75 - ETA: 0s - loss: 0.4244 - accuracy: 0.7607 - f1_metric: 0.75 - ETA: 0s - loss: 0.4240 - accuracy: 0.7615 - f1_metric: 0.75 - ETA: 0s - loss: 0.4173 - accuracy: 0.7606 - f1_metric: 0.75 - ETA: 0s - loss: 0.4155 - accuracy: 0.7627 - f1_metric: 0.75 - ETA: 0s - loss: 0.4109 - accuracy: 0.7664 - f1_metric: 0.76 - ETA: 0s - loss: 0.4081 - accuracy: 0.7697 - f1_metric: 0.7668\n",
      "Epoch 00039: val_f1_metric improved from 0.78868 to 0.82388, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 124ms/step - loss: 0.4053 - accuracy: 0.7708 - f1_metric: 0.7685 - val_loss: 0.4862 - val_accuracy: 0.8282 - val_f1_metric: 0.8239 - lr: 0.0010\n",
      "Epoch 40/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4910 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.3602 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.3496 - accuracy: 0.8705 - f1_metric: 0.86 - ETA: 0s - loss: 0.3725 - accuracy: 0.8611 - f1_metric: 0.85 - ETA: 0s - loss: 0.3590 - accuracy: 0.8608 - f1_metric: 0.85 - ETA: 0s - loss: 0.3622 - accuracy: 0.8570 - f1_metric: 0.85 - ETA: 0s - loss: 0.3623 - accuracy: 0.8408 - f1_metric: 0.83 - ETA: 0s - loss: 0.3788 - accuracy: 0.8273 - f1_metric: 0.82 - ETA: 0s - loss: 0.3769 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.3790 - accuracy: 0.8104 - f1_metric: 0.80 - ETA: 0s - loss: 0.3817 - accuracy: 0.7937 - f1_metric: 0.79 - ETA: 0s - loss: 0.3790 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3806 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3769 - accuracy: 0.8004 - f1_metric: 0.79 - ETA: 0s - loss: 0.3798 - accuracy: 0.8013 - f1_metric: 0.79 - ETA: 0s - loss: 0.3858 - accuracy: 0.8040 - f1_metric: 0.80 - ETA: 0s - loss: 0.3839 - accuracy: 0.8019 - f1_metric: 0.80 - ETA: 0s - loss: 0.3893 - accuracy: 0.7993 - f1_metric: 0.79 - ETA: 0s - loss: 0.3890 - accuracy: 0.7942 - f1_metric: 0.79 - ETA: 0s - loss: 0.3883 - accuracy: 0.7917 - f1_metric: 0.78 - ETA: 0s - loss: 0.3895 - accuracy: 0.7920 - f1_metric: 0.7897\n",
      "Epoch 00040: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.3895 - accuracy: 0.7920 - f1_metric: 0.7897 - val_loss: 0.6236 - val_accuracy: 0.7603 - val_f1_metric: 0.7571 - lr: 0.0010\n",
      "Epoch 41/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3588 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3577 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3912 - accuracy: 0.7871 - f1_metric: 0.78 - ETA: 0s - loss: 0.3825 - accuracy: 0.7797 - f1_metric: 0.78 - ETA: 0s - loss: 0.3686 - accuracy: 0.7837 - f1_metric: 0.78 - ETA: 0s - loss: 0.3922 - accuracy: 0.7906 - f1_metric: 0.79 - ETA: 0s - loss: 0.3883 - accuracy: 0.7895 - f1_metric: 0.79 - ETA: 0s - loss: 0.3930 - accuracy: 0.7758 - f1_metric: 0.77 - ETA: 0s - loss: 0.3940 - accuracy: 0.7602 - f1_metric: 0.75 - ETA: 0s - loss: 0.3899 - accuracy: 0.7584 - f1_metric: 0.75 - ETA: 0s - loss: 0.3877 - accuracy: 0.7629 - f1_metric: 0.76 - ETA: 0s - loss: 0.3870 - accuracy: 0.7666 - f1_metric: 0.76 - ETA: 0s - loss: 0.3846 - accuracy: 0.7768 - f1_metric: 0.77 - ETA: 0s - loss: 0.3861 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3942 - accuracy: 0.7785 - f1_metric: 0.77 - ETA: 0s - loss: 0.3961 - accuracy: 0.7747 - f1_metric: 0.7731\n",
      "Epoch 00041: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3955 - accuracy: 0.7743 - f1_metric: 0.7724 - val_loss: 0.8301 - val_accuracy: 0.6597 - val_f1_metric: 0.6554 - lr: 0.0010\n",
      "Epoch 42/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4073 - accuracy: 0.6875 - f1_metric: 0.68 - ETA: 0s - loss: 0.4130 - accuracy: 0.7070 - f1_metric: 0.70 - ETA: 0s - loss: 0.3920 - accuracy: 0.7411 - f1_metric: 0.74 - ETA: 0s - loss: 0.3878 - accuracy: 0.7688 - f1_metric: 0.76 - ETA: 0s - loss: 0.3801 - accuracy: 0.7728 - f1_metric: 0.77 - ETA: 0s - loss: 0.3902 - accuracy: 0.7803 - f1_metric: 0.77 - ETA: 0s - loss: 0.3876 - accuracy: 0.7856 - f1_metric: 0.78 - ETA: 0s - loss: 0.3869 - accuracy: 0.7844 - f1_metric: 0.78 - ETA: 0s - loss: 0.3903 - accuracy: 0.7805 - f1_metric: 0.77 - ETA: 0s - loss: 0.3939 - accuracy: 0.7756 - f1_metric: 0.77 - ETA: 0s - loss: 0.3908 - accuracy: 0.7773 - f1_metric: 0.77 - ETA: 0s - loss: 0.3879 - accuracy: 0.7757 - f1_metric: 0.77 - ETA: 0s - loss: 0.3847 - accuracy: 0.7760 - f1_metric: 0.77 - ETA: 0s - loss: 0.3860 - accuracy: 0.7768 - f1_metric: 0.77 - ETA: 0s - loss: 0.3868 - accuracy: 0.7800 - f1_metric: 0.77 - ETA: 0s - loss: 0.3872 - accuracy: 0.7808 - f1_metric: 0.77 - ETA: 0s - loss: 0.3868 - accuracy: 0.7853 - f1_metric: 0.7844\n",
      "Epoch 00042: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3878 - accuracy: 0.7844 - f1_metric: 0.7827 - val_loss: 0.6210 - val_accuracy: 0.7552 - val_f1_metric: 0.7497 - lr: 0.0010\n",
      "Epoch 43/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.4065 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.3945 - accuracy: 0.7545 - f1_metric: 0.75 - ETA: 0s - loss: 0.4010 - accuracy: 0.7641 - f1_metric: 0.76 - ETA: 0s - loss: 0.3941 - accuracy: 0.7716 - f1_metric: 0.76 - ETA: 0s - loss: 0.3960 - accuracy: 0.7803 - f1_metric: 0.77 - ETA: 0s - loss: 0.3900 - accuracy: 0.7821 - f1_metric: 0.78 - ETA: 0s - loss: 0.3909 - accuracy: 0.7834 - f1_metric: 0.78 - ETA: 0s - loss: 0.3914 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3842 - accuracy: 0.7835 - f1_metric: 0.78 - ETA: 0s - loss: 0.3899 - accuracy: 0.7818 - f1_metric: 0.78 - ETA: 0s - loss: 0.3887 - accuracy: 0.7822 - f1_metric: 0.78 - ETA: 0s - loss: 0.3873 - accuracy: 0.7846 - f1_metric: 0.78 - ETA: 0s - loss: 0.3809 - accuracy: 0.7910 - f1_metric: 0.78 - ETA: 0s - loss: 0.3754 - accuracy: 0.7943 - f1_metric: 0.7930\n",
      "Epoch 00043: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3745 - accuracy: 0.7952 - f1_metric: 0.7947 - val_loss: 0.4848 - val_accuracy: 0.8240 - val_f1_metric: 0.8226 - lr: 0.0010\n",
      "Epoch 44/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3325 - accuracy: 0.8594 - f1_metric: 0.86 - ETA: 0s - loss: 0.3985 - accuracy: 0.8415 - f1_metric: 0.84 - ETA: 0s - loss: 0.3925 - accuracy: 0.8078 - f1_metric: 0.80 - ETA: 0s - loss: 0.3924 - accuracy: 0.7873 - f1_metric: 0.78 - ETA: 0s - loss: 0.3846 - accuracy: 0.7773 - f1_metric: 0.77 - ETA: 0s - loss: 0.3870 - accuracy: 0.7771 - f1_metric: 0.77 - ETA: 0s - loss: 0.3893 - accuracy: 0.7784 - f1_metric: 0.77 - ETA: 0s - loss: 0.3757 - accuracy: 0.7844 - f1_metric: 0.78 - ETA: 0s - loss: 0.3762 - accuracy: 0.7885 - f1_metric: 0.78 - ETA: 0s - loss: 0.3781 - accuracy: 0.7928 - f1_metric: 0.79 - ETA: 0s - loss: 0.3864 - accuracy: 0.7918 - f1_metric: 0.78 - ETA: 0s - loss: 0.3924 - accuracy: 0.7927 - f1_metric: 0.79 - ETA: 0s - loss: 0.3907 - accuracy: 0.7844 - f1_metric: 0.78 - ETA: 0s - loss: 0.3911 - accuracy: 0.7834 - f1_metric: 0.7796\n",
      "Epoch 00044: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3904 - accuracy: 0.7833 - f1_metric: 0.7795 - val_loss: 0.7376 - val_accuracy: 0.7016 - val_f1_metric: 0.6946 - lr: 0.0010\n",
      "Epoch 45/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3948 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.3779 - accuracy: 0.7383 - f1_metric: 0.72 - ETA: 0s - loss: 0.4176 - accuracy: 0.7478 - f1_metric: 0.74 - ETA: 0s - loss: 0.3997 - accuracy: 0.7469 - f1_metric: 0.74 - ETA: 0s - loss: 0.3886 - accuracy: 0.7644 - f1_metric: 0.76 - ETA: 0s - loss: 0.3729 - accuracy: 0.7891 - f1_metric: 0.78 - ETA: 0s - loss: 0.3631 - accuracy: 0.8010 - f1_metric: 0.79 - ETA: 0s - loss: 0.3550 - accuracy: 0.8118 - f1_metric: 0.80 - ETA: 0s - loss: 0.3606 - accuracy: 0.8150 - f1_metric: 0.81 - ETA: 0s - loss: 0.3616 - accuracy: 0.8108 - f1_metric: 0.80 - ETA: 0s - loss: 0.3671 - accuracy: 0.8019 - f1_metric: 0.79 - ETA: 0s - loss: 0.3726 - accuracy: 0.7992 - f1_metric: 0.79 - ETA: 0s - loss: 0.3748 - accuracy: 0.7935 - f1_metric: 0.79 - ETA: 0s - loss: 0.3727 - accuracy: 0.7921 - f1_metric: 0.79 - ETA: 0s - loss: 0.3730 - accuracy: 0.7920 - f1_metric: 0.7899\n",
      "Epoch 00045: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3737 - accuracy: 0.7923 - f1_metric: 0.7898 - val_loss: 0.5300 - val_accuracy: 0.7938 - val_f1_metric: 0.7945 - lr: 0.0010\n",
      "Epoch 46/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3528 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3174 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.3494 - accuracy: 0.8460 - f1_metric: 0.84 - ETA: 0s - loss: 0.3552 - accuracy: 0.8344 - f1_metric: 0.83 - ETA: 0s - loss: 0.3507 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.3494 - accuracy: 0.8315 - f1_metric: 0.83 - ETA: 0s - loss: 0.3553 - accuracy: 0.8235 - f1_metric: 0.82 - ETA: 0s - loss: 0.3550 - accuracy: 0.8199 - f1_metric: 0.81 - ETA: 0s - loss: 0.3520 - accuracy: 0.8182 - f1_metric: 0.81 - ETA: 0s - loss: 0.3532 - accuracy: 0.8150 - f1_metric: 0.81 - ETA: 0s - loss: 0.3575 - accuracy: 0.8175 - f1_metric: 0.81 - ETA: 0s - loss: 0.3582 - accuracy: 0.8160 - f1_metric: 0.81 - ETA: 0s - loss: 0.3649 - accuracy: 0.8120 - f1_metric: 0.81 - ETA: 0s - loss: 0.3672 - accuracy: 0.8087 - f1_metric: 0.80 - ETA: 0s - loss: 0.3714 - accuracy: 0.7996 - f1_metric: 0.79 - ETA: 0s - loss: 0.3726 - accuracy: 0.7943 - f1_metric: 0.7921\n",
      "Epoch 00046: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3729 - accuracy: 0.7927 - f1_metric: 0.7907 - val_loss: 0.5430 - val_accuracy: 0.7896 - val_f1_metric: 0.7875 - lr: 0.0010\n",
      "Epoch 47/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3472 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3495 - accuracy: 0.8304 - f1_metric: 0.83 - ETA: 0s - loss: 0.3659 - accuracy: 0.8391 - f1_metric: 0.83 - ETA: 0s - loss: 0.3614 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3557 - accuracy: 0.8271 - f1_metric: 0.82 - ETA: 0s - loss: 0.3615 - accuracy: 0.8168 - f1_metric: 0.81 - ETA: 0s - loss: 0.3678 - accuracy: 0.8065 - f1_metric: 0.80 - ETA: 0s - loss: 0.3726 - accuracy: 0.7982 - f1_metric: 0.79 - ETA: 0s - loss: 0.3714 - accuracy: 0.7946 - f1_metric: 0.79 - ETA: 0s - loss: 0.3694 - accuracy: 0.7953 - f1_metric: 0.79 - ETA: 0s - loss: 0.3739 - accuracy: 0.7949 - f1_metric: 0.79 - ETA: 0s - loss: 0.3732 - accuracy: 0.7973 - f1_metric: 0.79 - ETA: 0s - loss: 0.3727 - accuracy: 0.7977 - f1_metric: 0.79 - ETA: 0s - loss: 0.3760 - accuracy: 0.7954 - f1_metric: 0.79 - ETA: 0s - loss: 0.3715 - accuracy: 0.7952 - f1_metric: 0.7931\n",
      "Epoch 00047: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3715 - accuracy: 0.7952 - f1_metric: 0.7931 - val_loss: 0.5515 - val_accuracy: 0.7904 - val_f1_metric: 0.7908 - lr: 0.0010\n",
      "Epoch 48/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4393 - accuracy: 0.7812 - f1_metric: 0.77 - ETA: 0s - loss: 0.4237 - accuracy: 0.7852 - f1_metric: 0.78 - ETA: 0s - loss: 0.4039 - accuracy: 0.7835 - f1_metric: 0.78 - ETA: 0s - loss: 0.3704 - accuracy: 0.7953 - f1_metric: 0.79 - ETA: 0s - loss: 0.3970 - accuracy: 0.7921 - f1_metric: 0.79 - ETA: 0s - loss: 0.3779 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3744 - accuracy: 0.7847 - f1_metric: 0.78 - ETA: 0s - loss: 0.3707 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.3695 - accuracy: 0.7995 - f1_metric: 0.80 - ETA: 0s - loss: 0.3710 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3717 - accuracy: 0.8005 - f1_metric: 0.80 - ETA: 0s - loss: 0.3657 - accuracy: 0.8049 - f1_metric: 0.80 - ETA: 0s - loss: 0.3684 - accuracy: 0.8012 - f1_metric: 0.80 - ETA: 0s - loss: 0.3726 - accuracy: 0.8029 - f1_metric: 0.80 - ETA: 0s - loss: 0.3725 - accuracy: 0.7991 - f1_metric: 0.7997\n",
      "Epoch 00048: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3716 - accuracy: 0.7970 - f1_metric: 0.7983 - val_loss: 0.5908 - val_accuracy: 0.7779 - val_f1_metric: 0.7696 - lr: 0.0010\n",
      "Epoch 49/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.3674 - accuracy: 0.7891 - f1_metric: 0.78 - ETA: 0s - loss: 0.3545 - accuracy: 0.7924 - f1_metric: 0.79 - ETA: 0s - loss: 0.3623 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3491 - accuracy: 0.8221 - f1_metric: 0.81 - ETA: 0s - loss: 0.3443 - accuracy: 0.8184 - f1_metric: 0.81 - ETA: 0s - loss: 0.3478 - accuracy: 0.8133 - f1_metric: 0.80 - ETA: 0s - loss: 0.3413 - accuracy: 0.8082 - f1_metric: 0.80 - ETA: 0s - loss: 0.3347 - accuracy: 0.8194 - f1_metric: 0.81 - ETA: 0s - loss: 0.3402 - accuracy: 0.8237 - f1_metric: 0.82 - ETA: 0s - loss: 0.3443 - accuracy: 0.8216 - f1_metric: 0.81 - ETA: 0s - loss: 0.3510 - accuracy: 0.8222 - f1_metric: 0.81 - ETA: 0s - loss: 0.3572 - accuracy: 0.8087 - f1_metric: 0.80 - ETA: 0s - loss: 0.3619 - accuracy: 0.8008 - f1_metric: 0.79 - ETA: 0s - loss: 0.3614 - accuracy: 0.7983 - f1_metric: 0.7958\n",
      "Epoch 00049: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3613 - accuracy: 0.7973 - f1_metric: 0.7939 - val_loss: 0.5748 - val_accuracy: 0.7812 - val_f1_metric: 0.7775 - lr: 0.0010\n",
      "Epoch 50/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.3847 - accuracy: 0.7930 - f1_metric: 0.79 - ETA: 0s - loss: 0.3752 - accuracy: 0.8214 - f1_metric: 0.82 - ETA: 0s - loss: 0.3866 - accuracy: 0.8108 - f1_metric: 0.80 - ETA: 0s - loss: 0.3645 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.3631 - accuracy: 0.8115 - f1_metric: 0.81 - ETA: 0s - loss: 0.3620 - accuracy: 0.8090 - f1_metric: 0.80 - ETA: 0s - loss: 0.3763 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3662 - accuracy: 0.8132 - f1_metric: 0.81 - ETA: 0s - loss: 0.3618 - accuracy: 0.8077 - f1_metric: 0.80 - ETA: 0s - loss: 0.3600 - accuracy: 0.8039 - f1_metric: 0.80 - ETA: 0s - loss: 0.3595 - accuracy: 0.8071 - f1_metric: 0.80 - ETA: 0s - loss: 0.3613 - accuracy: 0.8120 - f1_metric: 0.81 - ETA: 0s - loss: 0.3633 - accuracy: 0.8129 - f1_metric: 0.81 - ETA: 0s - loss: 0.3599 - accuracy: 0.8129 - f1_metric: 0.81 - ETA: 0s - loss: 0.3575 - accuracy: 0.8117 - f1_metric: 0.81 - ETA: 0s - loss: 0.3553 - accuracy: 0.8107 - f1_metric: 0.8097\n",
      "Epoch 00050: val_f1_metric did not improve from 0.82388\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3548 - accuracy: 0.8096 - f1_metric: 0.8074 - val_loss: 0.5197 - val_accuracy: 0.8072 - val_f1_metric: 0.8059 - lr: 0.0010\n",
      "Epoch 51/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3424 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2889 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.3313 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.3462 - accuracy: 0.8641 - f1_metric: 0.86 - ETA: 0s - loss: 0.3457 - accuracy: 0.8522 - f1_metric: 0.85 - ETA: 0s - loss: 0.3437 - accuracy: 0.8389 - f1_metric: 0.84 - ETA: 0s - loss: 0.3470 - accuracy: 0.8240 - f1_metric: 0.82 - ETA: 0s - loss: 0.3429 - accuracy: 0.8274 - f1_metric: 0.82 - ETA: 0s - loss: 0.3501 - accuracy: 0.8307 - f1_metric: 0.83 - ETA: 0s - loss: 0.3483 - accuracy: 0.8275 - f1_metric: 0.82 - ETA: 0s - loss: 0.3500 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.3527 - accuracy: 0.8205 - f1_metric: 0.82 - ETA: 0s - loss: 0.3517 - accuracy: 0.8095 - f1_metric: 0.81 - ETA: 0s - loss: 0.3537 - accuracy: 0.8077 - f1_metric: 0.80 - ETA: 0s - loss: 0.3491 - accuracy: 0.8080 - f1_metric: 0.8085\n",
      "Epoch 00051: val_f1_metric improved from 0.82388 to 0.83432, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 113ms/step - loss: 0.3480 - accuracy: 0.8099 - f1_metric: 0.8111 - val_loss: 0.4592 - val_accuracy: 0.8349 - val_f1_metric: 0.8343 - lr: 0.0010\n",
      "Epoch 52/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.9531 - f1_metric: 0.95 - ETA: 0s - loss: 0.4083 - accuracy: 0.8906 - f1_metric: 0.88 - ETA: 0s - loss: 0.3812 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.3571 - accuracy: 0.8194 - f1_metric: 0.81 - ETA: 0s - loss: 0.3523 - accuracy: 0.8168 - f1_metric: 0.81 - ETA: 0s - loss: 0.3542 - accuracy: 0.8069 - f1_metric: 0.80 - ETA: 0s - loss: 0.3560 - accuracy: 0.8018 - f1_metric: 0.80 - ETA: 0s - loss: 0.3545 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3546 - accuracy: 0.8023 - f1_metric: 0.80 - ETA: 0s - loss: 0.3487 - accuracy: 0.8037 - f1_metric: 0.80 - ETA: 0s - loss: 0.3405 - accuracy: 0.8062 - f1_metric: 0.80 - ETA: 0s - loss: 0.3438 - accuracy: 0.8113 - f1_metric: 0.81 - ETA: 0s - loss: 0.3440 - accuracy: 0.8131 - f1_metric: 0.81 - ETA: 0s - loss: 0.3474 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3449 - accuracy: 0.8172 - f1_metric: 0.81 - ETA: 0s - loss: 0.3458 - accuracy: 0.8179 - f1_metric: 0.81 - ETA: 0s - loss: 0.3435 - accuracy: 0.8172 - f1_metric: 0.81 - ETA: 0s - loss: 0.3428 - accuracy: 0.8142 - f1_metric: 0.81 - ETA: 0s - loss: 0.3398 - accuracy: 0.8162 - f1_metric: 0.81 - ETA: 0s - loss: 0.3414 - accuracy: 0.8148 - f1_metric: 0.81 - ETA: 0s - loss: 0.3436 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3452 - accuracy: 0.8107 - f1_metric: 0.8105\n",
      "Epoch 00052: val_f1_metric did not improve from 0.83432\n",
      "44/44 [==============================] - 2s 38ms/step - loss: 0.3465 - accuracy: 0.8106 - f1_metric: 0.8104 - val_loss: 0.5593 - val_accuracy: 0.7829 - val_f1_metric: 0.7825 - lr: 0.0010\n",
      "Epoch 53/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3880 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3280 - accuracy: 0.8229 - f1_metric: 0.81 - ETA: 0s - loss: 0.3124 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.3426 - accuracy: 0.8170 - f1_metric: 0.81 - ETA: 0s - loss: 0.3320 - accuracy: 0.8160 - f1_metric: 0.81 - ETA: 0s - loss: 0.3412 - accuracy: 0.8210 - f1_metric: 0.82 - ETA: 0s - loss: 0.3456 - accuracy: 0.8181 - f1_metric: 0.81 - ETA: 0s - loss: 0.3479 - accuracy: 0.8116 - f1_metric: 0.81 - ETA: 0s - loss: 0.3538 - accuracy: 0.8078 - f1_metric: 0.80 - ETA: 0s - loss: 0.3573 - accuracy: 0.8033 - f1_metric: 0.80 - ETA: 0s - loss: 0.3564 - accuracy: 0.8050 - f1_metric: 0.80 - ETA: 0s - loss: 0.3605 - accuracy: 0.8030 - f1_metric: 0.80 - ETA: 0s - loss: 0.3630 - accuracy: 0.8029 - f1_metric: 0.80 - ETA: 0s - loss: 0.3588 - accuracy: 0.8016 - f1_metric: 0.80 - ETA: 0s - loss: 0.3579 - accuracy: 0.8013 - f1_metric: 0.80 - ETA: 0s - loss: 0.3525 - accuracy: 0.8072 - f1_metric: 0.80 - ETA: 0s - loss: 0.3486 - accuracy: 0.8091 - f1_metric: 0.80 - ETA: 0s - loss: 0.3452 - accuracy: 0.8132 - f1_metric: 0.8125\n",
      "Epoch 00053: val_f1_metric improved from 0.83432 to 0.85097, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 127ms/step - loss: 0.3451 - accuracy: 0.8135 - f1_metric: 0.8131 - val_loss: 0.4187 - val_accuracy: 0.8508 - val_f1_metric: 0.8510 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.3312 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.3303 - accuracy: 0.8781 - f1_metric: 0.87 - ETA: 0s - loss: 0.3337 - accuracy: 0.8594 - f1_metric: 0.86 - ETA: 0s - loss: 0.3718 - accuracy: 0.8247 - f1_metric: 0.82 - ETA: 0s - loss: 0.3528 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3503 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3496 - accuracy: 0.8057 - f1_metric: 0.80 - ETA: 0s - loss: 0.3476 - accuracy: 0.8030 - f1_metric: 0.80 - ETA: 0s - loss: 0.3468 - accuracy: 0.7984 - f1_metric: 0.79 - ETA: 0s - loss: 0.3497 - accuracy: 0.7976 - f1_metric: 0.79 - ETA: 0s - loss: 0.3516 - accuracy: 0.8010 - f1_metric: 0.80 - ETA: 0s - loss: 0.3431 - accuracy: 0.8065 - f1_metric: 0.80 - ETA: 0s - loss: 0.3403 - accuracy: 0.8067 - f1_metric: 0.80 - ETA: 0s - loss: 0.3388 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3367 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3345 - accuracy: 0.8153 - f1_metric: 0.81 - ETA: 0s - loss: 0.3341 - accuracy: 0.8143 - f1_metric: 0.81 - ETA: 0s - loss: 0.3350 - accuracy: 0.8155 - f1_metric: 0.81 - ETA: 0s - loss: 0.3336 - accuracy: 0.8149 - f1_metric: 0.81 - ETA: 0s - loss: 0.3302 - accuracy: 0.8159 - f1_metric: 0.81 - ETA: 0s - loss: 0.3300 - accuracy: 0.8201 - f1_metric: 0.8202\n",
      "Epoch 00054: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.3304 - accuracy: 0.8200 - f1_metric: 0.8199 - val_loss: 0.4936 - val_accuracy: 0.8206 - val_f1_metric: 0.8219 - lr: 0.0010\n",
      "Epoch 55/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.7969 - f1_metric: 0.78 - ETA: 0s - loss: 0.3125 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.3332 - accuracy: 0.7995 - f1_metric: 0.79 - ETA: 0s - loss: 0.3271 - accuracy: 0.8021 - f1_metric: 0.79 - ETA: 0s - loss: 0.3196 - accuracy: 0.8139 - f1_metric: 0.81 - ETA: 0s - loss: 0.3249 - accuracy: 0.8185 - f1_metric: 0.81 - ETA: 0s - loss: 0.3294 - accuracy: 0.8104 - f1_metric: 0.80 - ETA: 0s - loss: 0.3342 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3330 - accuracy: 0.8076 - f1_metric: 0.80 - ETA: 0s - loss: 0.3398 - accuracy: 0.8065 - f1_metric: 0.80 - ETA: 0s - loss: 0.3398 - accuracy: 0.8037 - f1_metric: 0.80 - ETA: 0s - loss: 0.3516 - accuracy: 0.8000 - f1_metric: 0.79 - ETA: 0s - loss: 0.3474 - accuracy: 0.8008 - f1_metric: 0.79 - ETA: 0s - loss: 0.3420 - accuracy: 0.8054 - f1_metric: 0.80 - ETA: 0s - loss: 0.3368 - accuracy: 0.8097 - f1_metric: 0.80 - ETA: 0s - loss: 0.3387 - accuracy: 0.8095 - f1_metric: 0.80 - ETA: 0s - loss: 0.3412 - accuracy: 0.8133 - f1_metric: 0.81 - ETA: 0s - loss: 0.3381 - accuracy: 0.8180 - f1_metric: 0.8168\n",
      "Epoch 00055: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3375 - accuracy: 0.8171 - f1_metric: 0.8151 - val_loss: 0.5477 - val_accuracy: 0.7988 - val_f1_metric: 0.7981 - lr: 0.0010\n",
      "Epoch 56/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3594 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3611 - accuracy: 0.7539 - f1_metric: 0.75 - ETA: 0s - loss: 0.3558 - accuracy: 0.7723 - f1_metric: 0.77 - ETA: 0s - loss: 0.3474 - accuracy: 0.7828 - f1_metric: 0.78 - ETA: 0s - loss: 0.3387 - accuracy: 0.7993 - f1_metric: 0.79 - ETA: 0s - loss: 0.3406 - accuracy: 0.7990 - f1_metric: 0.79 - ETA: 0s - loss: 0.3354 - accuracy: 0.8142 - f1_metric: 0.81 - ETA: 0s - loss: 0.3405 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3366 - accuracy: 0.8275 - f1_metric: 0.82 - ETA: 0s - loss: 0.3391 - accuracy: 0.8223 - f1_metric: 0.82 - ETA: 0s - loss: 0.3356 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.3350 - accuracy: 0.8144 - f1_metric: 0.81 - ETA: 0s - loss: 0.3338 - accuracy: 0.8121 - f1_metric: 0.81 - ETA: 0s - loss: 0.3368 - accuracy: 0.8137 - f1_metric: 0.81 - ETA: 0s - loss: 0.3389 - accuracy: 0.8158 - f1_metric: 0.8158\n",
      "Epoch 00056: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3411 - accuracy: 0.8135 - f1_metric: 0.8134 - val_loss: 0.5968 - val_accuracy: 0.7812 - val_f1_metric: 0.7791 - lr: 0.0010\n",
      "Epoch 57/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3030 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.3324 - accuracy: 0.7461 - f1_metric: 0.73 - ETA: 0s - loss: 0.3487 - accuracy: 0.7589 - f1_metric: 0.75 - ETA: 0s - loss: 0.3505 - accuracy: 0.7750 - f1_metric: 0.76 - ETA: 0s - loss: 0.3579 - accuracy: 0.7849 - f1_metric: 0.77 - ETA: 0s - loss: 0.3537 - accuracy: 0.7920 - f1_metric: 0.78 - ETA: 0s - loss: 0.3511 - accuracy: 0.7960 - f1_metric: 0.79 - ETA: 0s - loss: 0.3443 - accuracy: 0.8036 - f1_metric: 0.79 - ETA: 0s - loss: 0.3489 - accuracy: 0.8021 - f1_metric: 0.79 - ETA: 0s - loss: 0.3484 - accuracy: 0.7998 - f1_metric: 0.79 - ETA: 0s - loss: 0.3438 - accuracy: 0.8033 - f1_metric: 0.80 - ETA: 0s - loss: 0.3436 - accuracy: 0.8054 - f1_metric: 0.80 - ETA: 0s - loss: 0.3466 - accuracy: 0.8056 - f1_metric: 0.80 - ETA: 0s - loss: 0.3416 - accuracy: 0.8070 - f1_metric: 0.80 - ETA: 0s - loss: 0.3395 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.3369 - accuracy: 0.8121 - f1_metric: 0.8098\n",
      "Epoch 00057: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3388 - accuracy: 0.8139 - f1_metric: 0.8130 - val_loss: 0.4382 - val_accuracy: 0.8433 - val_f1_metric: 0.8412 - lr: 0.0010\n",
      "Epoch 58/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.3136 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.3066 - accuracy: 0.8504 - f1_metric: 0.84 - ETA: 0s - loss: 0.3066 - accuracy: 0.8507 - f1_metric: 0.84 - ETA: 0s - loss: 0.3172 - accuracy: 0.8451 - f1_metric: 0.84 - ETA: 0s - loss: 0.3204 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3160 - accuracy: 0.8420 - f1_metric: 0.84 - ETA: 0s - loss: 0.3169 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3161 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.3175 - accuracy: 0.8368 - f1_metric: 0.83 - ETA: 0s - loss: 0.3231 - accuracy: 0.8339 - f1_metric: 0.83 - ETA: 0s - loss: 0.3186 - accuracy: 0.8350 - f1_metric: 0.83 - ETA: 0s - loss: 0.3194 - accuracy: 0.8350 - f1_metric: 0.83 - ETA: 0s - loss: 0.3210 - accuracy: 0.8323 - f1_metric: 0.83 - ETA: 0s - loss: 0.3233 - accuracy: 0.8324 - f1_metric: 0.83 - ETA: 0s - loss: 0.3231 - accuracy: 0.8318 - f1_metric: 0.8310\n",
      "Epoch 00058: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3235 - accuracy: 0.8300 - f1_metric: 0.8275 - val_loss: 0.5543 - val_accuracy: 0.7904 - val_f1_metric: 0.7905 - lr: 0.0010\n",
      "Epoch 59/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3292 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.3352 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3678 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3481 - accuracy: 0.8156 - f1_metric: 0.81 - ETA: 0s - loss: 0.3461 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3387 - accuracy: 0.8104 - f1_metric: 0.80 - ETA: 0s - loss: 0.3375 - accuracy: 0.8160 - f1_metric: 0.81 - ETA: 0s - loss: 0.3333 - accuracy: 0.8251 - f1_metric: 0.82 - ETA: 0s - loss: 0.3342 - accuracy: 0.8255 - f1_metric: 0.82 - ETA: 0s - loss: 0.3291 - accuracy: 0.8252 - f1_metric: 0.82 - ETA: 0s - loss: 0.3271 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3262 - accuracy: 0.8248 - f1_metric: 0.82 - ETA: 0s - loss: 0.3236 - accuracy: 0.8299 - f1_metric: 0.82 - ETA: 0s - loss: 0.3222 - accuracy: 0.8285 - f1_metric: 0.82 - ETA: 0s - loss: 0.3252 - accuracy: 0.8278 - f1_metric: 0.8267\n",
      "Epoch 00059: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3224 - accuracy: 0.8286 - f1_metric: 0.8274 - val_loss: 0.5461 - val_accuracy: 0.7997 - val_f1_metric: 0.7980 - lr: 0.0010\n",
      "Epoch 60/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2935 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.3706 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 1s - loss: 0.3652 - accuracy: 0.8313 - f1_metric: 0.83 - ETA: 1s - loss: 0.3551 - accuracy: 0.8304 - f1_metric: 0.82 - ETA: 0s - loss: 0.3571 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3512 - accuracy: 0.8310 - f1_metric: 0.82 - ETA: 0s - loss: 0.3352 - accuracy: 0.8292 - f1_metric: 0.82 - ETA: 0s - loss: 0.3367 - accuracy: 0.8164 - f1_metric: 0.81 - ETA: 0s - loss: 0.3338 - accuracy: 0.8160 - f1_metric: 0.81 - ETA: 0s - loss: 0.3352 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3350 - accuracy: 0.8261 - f1_metric: 0.82 - ETA: 0s - loss: 0.3309 - accuracy: 0.8305 - f1_metric: 0.82 - ETA: 0s - loss: 0.3363 - accuracy: 0.8324 - f1_metric: 0.83 - ETA: 0s - loss: 0.3337 - accuracy: 0.8286 - f1_metric: 0.82 - ETA: 0s - loss: 0.3329 - accuracy: 0.8259 - f1_metric: 0.82 - ETA: 0s - loss: 0.3341 - accuracy: 0.8236 - f1_metric: 0.82 - ETA: 0s - loss: 0.3311 - accuracy: 0.8254 - f1_metric: 0.82 - ETA: 0s - loss: 0.3333 - accuracy: 0.8233 - f1_metric: 0.8229\n",
      "Epoch 00060: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.3329 - accuracy: 0.8239 - f1_metric: 0.8242 - val_loss: 0.4709 - val_accuracy: 0.8290 - val_f1_metric: 0.8287 - lr: 0.0010\n",
      "Epoch 61/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2877 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.3253 - accuracy: 0.8490 - f1_metric: 0.84 - ETA: 0s - loss: 0.3019 - accuracy: 0.8620 - f1_metric: 0.85 - ETA: 0s - loss: 0.3074 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.3138 - accuracy: 0.8537 - f1_metric: 0.85 - ETA: 0s - loss: 0.3100 - accuracy: 0.8415 - f1_metric: 0.83 - ETA: 0s - loss: 0.3145 - accuracy: 0.8364 - f1_metric: 0.83 - ETA: 0s - loss: 0.3254 - accuracy: 0.8322 - f1_metric: 0.83 - ETA: 0s - loss: 0.3260 - accuracy: 0.8310 - f1_metric: 0.82 - ETA: 0s - loss: 0.3246 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3234 - accuracy: 0.8258 - f1_metric: 0.82 - ETA: 0s - loss: 0.3209 - accuracy: 0.8260 - f1_metric: 0.82 - ETA: 0s - loss: 0.3203 - accuracy: 0.8256 - f1_metric: 0.82 - ETA: 0s - loss: 0.3190 - accuracy: 0.8277 - f1_metric: 0.82 - ETA: 0s - loss: 0.3232 - accuracy: 0.8329 - f1_metric: 0.83 - ETA: 0s - loss: 0.3202 - accuracy: 0.8373 - f1_metric: 0.83 - ETA: 0s - loss: 0.3212 - accuracy: 0.8315 - f1_metric: 0.8306\n",
      "Epoch 00061: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3206 - accuracy: 0.8304 - f1_metric: 0.8303 - val_loss: 0.5543 - val_accuracy: 0.8005 - val_f1_metric: 0.7999 - lr: 0.0010\n",
      "Epoch 62/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.2746 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2785 - accuracy: 0.8571 - f1_metric: 0.85 - ETA: 0s - loss: 0.2894 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.3004 - accuracy: 0.8522 - f1_metric: 0.85 - ETA: 0s - loss: 0.2992 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.3003 - accuracy: 0.8462 - f1_metric: 0.84 - ETA: 0s - loss: 0.3027 - accuracy: 0.8466 - f1_metric: 0.84 - ETA: 0s - loss: 0.3103 - accuracy: 0.8369 - f1_metric: 0.83 - ETA: 0s - loss: 0.3085 - accuracy: 0.8382 - f1_metric: 0.83 - ETA: 0s - loss: 0.3114 - accuracy: 0.8357 - f1_metric: 0.83 - ETA: 0s - loss: 0.3067 - accuracy: 0.8373 - f1_metric: 0.83 - ETA: 0s - loss: 0.3058 - accuracy: 0.8377 - f1_metric: 0.83 - ETA: 0s - loss: 0.3114 - accuracy: 0.8401 - f1_metric: 0.84 - ETA: 0s - loss: 0.3161 - accuracy: 0.8363 - f1_metric: 0.8362\n",
      "Epoch 00062: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3189 - accuracy: 0.8347 - f1_metric: 0.8335 - val_loss: 0.6366 - val_accuracy: 0.7536 - val_f1_metric: 0.7519 - lr: 0.0010\n",
      "Epoch 63/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.6562 - f1_metric: 0.65 - ETA: 0s - loss: 0.3817 - accuracy: 0.7031 - f1_metric: 0.70 - ETA: 0s - loss: 0.3672 - accuracy: 0.7254 - f1_metric: 0.72 - ETA: 0s - loss: 0.3457 - accuracy: 0.7422 - f1_metric: 0.74 - ETA: 0s - loss: 0.3305 - accuracy: 0.7704 - f1_metric: 0.77 - ETA: 0s - loss: 0.3332 - accuracy: 0.7871 - f1_metric: 0.78 - ETA: 0s - loss: 0.3278 - accuracy: 0.7911 - f1_metric: 0.79 - ETA: 0s - loss: 0.3203 - accuracy: 0.8026 - f1_metric: 0.80 - ETA: 0s - loss: 0.3198 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3194 - accuracy: 0.8170 - f1_metric: 0.81 - ETA: 0s - loss: 0.3162 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3212 - accuracy: 0.8163 - f1_metric: 0.81 - ETA: 0s - loss: 0.3202 - accuracy: 0.8194 - f1_metric: 0.82 - ETA: 0s - loss: 0.3213 - accuracy: 0.8173 - f1_metric: 0.81 - ETA: 0s - loss: 0.3171 - accuracy: 0.8181 - f1_metric: 0.81 - ETA: 0s - loss: 0.3188 - accuracy: 0.8178 - f1_metric: 0.8174\n",
      "Epoch 00063: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3188 - accuracy: 0.8178 - f1_metric: 0.8174 - val_loss: 0.4928 - val_accuracy: 0.8198 - val_f1_metric: 0.8182 - lr: 0.0010\n",
      "Epoch 64/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8438 - f1_metric: 0.83 - ETA: 0s - loss: 0.2669 - accuracy: 0.8711 - f1_metric: 0.86 - ETA: 0s - loss: 0.3248 - accuracy: 0.8638 - f1_metric: 0.85 - ETA: 0s - loss: 0.3267 - accuracy: 0.8484 - f1_metric: 0.84 - ETA: 0s - loss: 0.3216 - accuracy: 0.8401 - f1_metric: 0.83 - ETA: 0s - loss: 0.3328 - accuracy: 0.8354 - f1_metric: 0.83 - ETA: 0s - loss: 0.3249 - accuracy: 0.8325 - f1_metric: 0.82 - ETA: 0s - loss: 0.3227 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3227 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.3285 - accuracy: 0.8287 - f1_metric: 0.82 - ETA: 0s - loss: 0.3323 - accuracy: 0.8298 - f1_metric: 0.82 - ETA: 0s - loss: 0.3273 - accuracy: 0.8339 - f1_metric: 0.83 - ETA: 0s - loss: 0.3240 - accuracy: 0.8362 - f1_metric: 0.83 - ETA: 0s - loss: 0.3210 - accuracy: 0.8371 - f1_metric: 0.83 - ETA: 0s - loss: 0.3182 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.3205 - accuracy: 0.8344 - f1_metric: 0.83 - ETA: 0s - loss: 0.3186 - accuracy: 0.8321 - f1_metric: 0.8306\n",
      "Epoch 00064: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3183 - accuracy: 0.8315 - f1_metric: 0.8293 - val_loss: 0.4858 - val_accuracy: 0.8265 - val_f1_metric: 0.8253 - lr: 0.0010\n",
      "Epoch 65/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4255 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3708 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3741 - accuracy: 0.8411 - f1_metric: 0.84 - ETA: 0s - loss: 0.3472 - accuracy: 0.8340 - f1_metric: 0.83 - ETA: 0s - loss: 0.3352 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3407 - accuracy: 0.8305 - f1_metric: 0.82 - ETA: 0s - loss: 0.3335 - accuracy: 0.8291 - f1_metric: 0.82 - ETA: 0s - loss: 0.3367 - accuracy: 0.8255 - f1_metric: 0.82 - ETA: 0s - loss: 0.3289 - accuracy: 0.8328 - f1_metric: 0.83 - ETA: 0s - loss: 0.3279 - accuracy: 0.8366 - f1_metric: 0.83 - ETA: 0s - loss: 0.3307 - accuracy: 0.8313 - f1_metric: 0.83 - ETA: 0s - loss: 0.3289 - accuracy: 0.8343 - f1_metric: 0.83 - ETA: 0s - loss: 0.3295 - accuracy: 0.8357 - f1_metric: 0.83 - ETA: 0s - loss: 0.3251 - accuracy: 0.8364 - f1_metric: 0.83 - ETA: 0s - loss: 0.3254 - accuracy: 0.8377 - f1_metric: 0.83 - ETA: 0s - loss: 0.3230 - accuracy: 0.8369 - f1_metric: 0.83 - ETA: 0s - loss: 0.3164 - accuracy: 0.8385 - f1_metric: 0.8387\n",
      "Epoch 00065: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3164 - accuracy: 0.8379 - f1_metric: 0.8385 - val_loss: 0.4572 - val_accuracy: 0.8315 - val_f1_metric: 0.8305 - lr: 0.0010\n",
      "Epoch 66/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2964 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2589 - accuracy: 0.9141 - f1_metric: 0.91 - ETA: 0s - loss: 0.2725 - accuracy: 0.8996 - f1_metric: 0.89 - ETA: 0s - loss: 0.2770 - accuracy: 0.8764 - f1_metric: 0.87 - ETA: 0s - loss: 0.2818 - accuracy: 0.8650 - f1_metric: 0.86 - ETA: 0s - loss: 0.2838 - accuracy: 0.8658 - f1_metric: 0.86 - ETA: 0s - loss: 0.2870 - accuracy: 0.8586 - f1_metric: 0.85 - ETA: 0s - loss: 0.2927 - accuracy: 0.8580 - f1_metric: 0.85 - ETA: 0s - loss: 0.2981 - accuracy: 0.8534 - f1_metric: 0.85 - ETA: 0s - loss: 0.2970 - accuracy: 0.8491 - f1_metric: 0.84 - ETA: 0s - loss: 0.2981 - accuracy: 0.8452 - f1_metric: 0.84 - ETA: 0s - loss: 0.2991 - accuracy: 0.8442 - f1_metric: 0.84 - ETA: 0s - loss: 0.2966 - accuracy: 0.8466 - f1_metric: 0.84 - ETA: 0s - loss: 0.3014 - accuracy: 0.8475 - f1_metric: 0.8469\n",
      "Epoch 00066: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.3025 - accuracy: 0.8484 - f1_metric: 0.8484 - val_loss: 0.5030 - val_accuracy: 0.8206 - val_f1_metric: 0.8201 - lr: 0.0010\n",
      "Epoch 67/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3089 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2714 - accuracy: 0.9023 - f1_metric: 0.90 - ETA: 0s - loss: 0.3029 - accuracy: 0.8504 - f1_metric: 0.85 - ETA: 0s - loss: 0.3087 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3279 - accuracy: 0.8257 - f1_metric: 0.82 - ETA: 0s - loss: 0.3316 - accuracy: 0.8193 - f1_metric: 0.81 - ETA: 0s - loss: 0.3363 - accuracy: 0.8150 - f1_metric: 0.81 - ETA: 0s - loss: 0.3368 - accuracy: 0.8146 - f1_metric: 0.81 - ETA: 0s - loss: 0.3345 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3320 - accuracy: 0.8218 - f1_metric: 0.82 - ETA: 0s - loss: 0.3285 - accuracy: 0.8276 - f1_metric: 0.82 - ETA: 0s - loss: 0.3240 - accuracy: 0.8305 - f1_metric: 0.83 - ETA: 0s - loss: 0.3178 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.3236 - accuracy: 0.8357 - f1_metric: 0.83 - ETA: 0s - loss: 0.3170 - accuracy: 0.8363 - f1_metric: 0.8365\n",
      "Epoch 00067: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.3178 - accuracy: 0.8358 - f1_metric: 0.8352 - val_loss: 0.5115 - val_accuracy: 0.8173 - val_f1_metric: 0.8180 - lr: 0.0010\n",
      "Epoch 68/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2931 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.2994 - accuracy: 0.8418 - f1_metric: 0.84 - ETA: 0s - loss: 0.3179 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.3143 - accuracy: 0.8354 - f1_metric: 0.83 - ETA: 0s - loss: 0.3088 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2974 - accuracy: 0.8296 - f1_metric: 0.82 - ETA: 0s - loss: 0.3007 - accuracy: 0.8372 - f1_metric: 0.83 - ETA: 0s - loss: 0.3008 - accuracy: 0.8377 - f1_metric: 0.83 - ETA: 0s - loss: 0.3051 - accuracy: 0.8416 - f1_metric: 0.84 - ETA: 0s - loss: 0.3009 - accuracy: 0.8389 - f1_metric: 0.83 - ETA: 0s - loss: 0.2994 - accuracy: 0.8402 - f1_metric: 0.83 - ETA: 0s - loss: 0.3008 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3007 - accuracy: 0.8415 - f1_metric: 0.84 - ETA: 0s - loss: 0.2979 - accuracy: 0.8412 - f1_metric: 0.8414\n",
      "Epoch 00068: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2979 - accuracy: 0.8412 - f1_metric: 0.8414 - val_loss: 0.5255 - val_accuracy: 0.8156 - val_f1_metric: 0.8147 - lr: 0.0010\n",
      "Epoch 69/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3556 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3205 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3168 - accuracy: 0.8203 - f1_metric: 0.82 - ETA: 0s - loss: 0.3225 - accuracy: 0.8237 - f1_metric: 0.82 - ETA: 0s - loss: 0.3203 - accuracy: 0.8263 - f1_metric: 0.82 - ETA: 0s - loss: 0.3219 - accuracy: 0.8297 - f1_metric: 0.82 - ETA: 0s - loss: 0.3184 - accuracy: 0.8288 - f1_metric: 0.82 - ETA: 0s - loss: 0.3177 - accuracy: 0.8319 - f1_metric: 0.83 - ETA: 0s - loss: 0.3167 - accuracy: 0.8298 - f1_metric: 0.82 - ETA: 0s - loss: 0.3192 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.3131 - accuracy: 0.8248 - f1_metric: 0.82 - ETA: 0s - loss: 0.3150 - accuracy: 0.8268 - f1_metric: 0.82 - ETA: 0s - loss: 0.3120 - accuracy: 0.8310 - f1_metric: 0.83 - ETA: 0s - loss: 0.3076 - accuracy: 0.8338 - f1_metric: 0.83 - ETA: 0s - loss: 0.3088 - accuracy: 0.8344 - f1_metric: 0.8342\n",
      "Epoch 00069: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3088 - accuracy: 0.8344 - f1_metric: 0.8342 - val_loss: 0.4633 - val_accuracy: 0.8240 - val_f1_metric: 0.8224 - lr: 0.0010\n",
      "Epoch 70/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2437 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2513 - accuracy: 0.9115 - f1_metric: 0.91 - ETA: 0s - loss: 0.2812 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2801 - accuracy: 0.8819 - f1_metric: 0.88 - ETA: 0s - loss: 0.2735 - accuracy: 0.8763 - f1_metric: 0.87 - ETA: 0s - loss: 0.2726 - accuracy: 0.8615 - f1_metric: 0.86 - ETA: 0s - loss: 0.2870 - accuracy: 0.8568 - f1_metric: 0.85 - ETA: 0s - loss: 0.2866 - accuracy: 0.8571 - f1_metric: 0.85 - ETA: 0s - loss: 0.2909 - accuracy: 0.8535 - f1_metric: 0.85 - ETA: 0s - loss: 0.2890 - accuracy: 0.8565 - f1_metric: 0.85 - ETA: 0s - loss: 0.2969 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.2973 - accuracy: 0.8504 - f1_metric: 0.85 - ETA: 0s - loss: 0.2951 - accuracy: 0.8472 - f1_metric: 0.84 - ETA: 0s - loss: 0.2958 - accuracy: 0.8470 - f1_metric: 0.84 - ETA: 0s - loss: 0.2998 - accuracy: 0.8460 - f1_metric: 0.8457\n",
      "Epoch 00070: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3019 - accuracy: 0.8448 - f1_metric: 0.8444 - val_loss: 0.5447 - val_accuracy: 0.7988 - val_f1_metric: 0.7965 - lr: 0.0010\n",
      "Epoch 71/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3151 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3244 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3150 - accuracy: 0.8192 - f1_metric: 0.81 - ETA: 0s - loss: 0.3225 - accuracy: 0.8212 - f1_metric: 0.82 - ETA: 0s - loss: 0.3187 - accuracy: 0.8294 - f1_metric: 0.82 - ETA: 0s - loss: 0.3132 - accuracy: 0.8302 - f1_metric: 0.82 - ETA: 0s - loss: 0.3197 - accuracy: 0.8377 - f1_metric: 0.83 - ETA: 0s - loss: 0.3170 - accuracy: 0.8378 - f1_metric: 0.83 - ETA: 0s - loss: 0.3107 - accuracy: 0.8379 - f1_metric: 0.83 - ETA: 0s - loss: 0.3111 - accuracy: 0.8362 - f1_metric: 0.83 - ETA: 0s - loss: 0.3127 - accuracy: 0.8375 - f1_metric: 0.83 - ETA: 0s - loss: 0.3052 - accuracy: 0.8419 - f1_metric: 0.84 - ETA: 0s - loss: 0.3075 - accuracy: 0.8407 - f1_metric: 0.84 - ETA: 0s - loss: 0.3045 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3092 - accuracy: 0.8397 - f1_metric: 0.8390\n",
      "Epoch 00071: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3062 - accuracy: 0.8401 - f1_metric: 0.8389 - val_loss: 0.5164 - val_accuracy: 0.8080 - val_f1_metric: 0.8077 - lr: 0.0010\n",
      "Epoch 72/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.2950 - accuracy: 0.8398 - f1_metric: 0.84 - ETA: 0s - loss: 0.2967 - accuracy: 0.8527 - f1_metric: 0.85 - ETA: 0s - loss: 0.2956 - accuracy: 0.8500 - f1_metric: 0.85 - ETA: 0s - loss: 0.3123 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.3074 - accuracy: 0.8474 - f1_metric: 0.84 - ETA: 0s - loss: 0.3079 - accuracy: 0.8461 - f1_metric: 0.84 - ETA: 0s - loss: 0.3086 - accuracy: 0.8424 - f1_metric: 0.84 - ETA: 0s - loss: 0.3046 - accuracy: 0.8419 - f1_metric: 0.84 - ETA: 0s - loss: 0.3070 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3048 - accuracy: 0.8486 - f1_metric: 0.84 - ETA: 0s - loss: 0.3005 - accuracy: 0.8502 - f1_metric: 0.84 - ETA: 0s - loss: 0.3016 - accuracy: 0.8485 - f1_metric: 0.84 - ETA: 0s - loss: 0.3004 - accuracy: 0.8475 - f1_metric: 0.84 - ETA: 0s - loss: 0.2948 - accuracy: 0.8506 - f1_metric: 0.85 - ETA: 0s - loss: 0.2999 - accuracy: 0.8505 - f1_metric: 0.8505\n",
      "Epoch 00072: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2999 - accuracy: 0.8505 - f1_metric: 0.8505 - val_loss: 0.4322 - val_accuracy: 0.8449 - val_f1_metric: 0.8429 - lr: 0.0010\n",
      "Epoch 73/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2262 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2918 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2866 - accuracy: 0.8457 - f1_metric: 0.84 - ETA: 0s - loss: 0.2847 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2800 - accuracy: 0.8552 - f1_metric: 0.85 - ETA: 0s - loss: 0.2973 - accuracy: 0.8569 - f1_metric: 0.85 - ETA: 0s - loss: 0.2994 - accuracy: 0.8530 - f1_metric: 0.85 - ETA: 0s - loss: 0.2988 - accuracy: 0.8419 - f1_metric: 0.84 - ETA: 0s - loss: 0.2954 - accuracy: 0.8432 - f1_metric: 0.84 - ETA: 0s - loss: 0.2929 - accuracy: 0.8443 - f1_metric: 0.84 - ETA: 0s - loss: 0.2936 - accuracy: 0.8462 - f1_metric: 0.84 - ETA: 0s - loss: 0.2944 - accuracy: 0.8456 - f1_metric: 0.84 - ETA: 0s - loss: 0.2948 - accuracy: 0.8472 - f1_metric: 0.84 - ETA: 0s - loss: 0.2940 - accuracy: 0.8479 - f1_metric: 0.84 - ETA: 0s - loss: 0.2893 - accuracy: 0.8525 - f1_metric: 0.85 - ETA: 0s - loss: 0.2884 - accuracy: 0.8538 - f1_metric: 0.8541\n",
      "Epoch 00073: val_f1_metric did not improve from 0.85097\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2884 - accuracy: 0.8538 - f1_metric: 0.8541 - val_loss: 0.4701 - val_accuracy: 0.8298 - val_f1_metric: 0.8292 - lr: 0.0010\n",
      "Epoch 74/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.3077 - accuracy: 0.8477 - f1_metric: 0.84 - ETA: 0s - loss: 0.2896 - accuracy: 0.8661 - f1_metric: 0.86 - ETA: 0s - loss: 0.2915 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2807 - accuracy: 0.8685 - f1_metric: 0.86 - ETA: 0s - loss: 0.2773 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2735 - accuracy: 0.8741 - f1_metric: 0.87 - ETA: 0s - loss: 0.2760 - accuracy: 0.8735 - f1_metric: 0.87 - ETA: 0s - loss: 0.2786 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2764 - accuracy: 0.8684 - f1_metric: 0.86 - ETA: 0s - loss: 0.2811 - accuracy: 0.8677 - f1_metric: 0.86 - ETA: 0s - loss: 0.2808 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2811 - accuracy: 0.8692 - f1_metric: 0.86 - ETA: 0s - loss: 0.2821 - accuracy: 0.8651 - f1_metric: 0.86 - ETA: 0s - loss: 0.2810 - accuracy: 0.8647 - f1_metric: 0.86 - ETA: 0s - loss: 0.2791 - accuracy: 0.8670 - f1_metric: 0.8678\n",
      "Epoch 00074: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2791 - accuracy: 0.8670 - f1_metric: 0.8678 - val_loss: 0.4744 - val_accuracy: 0.8290 - val_f1_metric: 0.8288 - lr: 1.0000e-04\n",
      "Epoch 75/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2644 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2482 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2635 - accuracy: 0.8693 - f1_metric: 0.86 - ETA: 0s - loss: 0.2622 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2693 - accuracy: 0.8667 - f1_metric: 0.86 - ETA: 0s - loss: 0.2750 - accuracy: 0.8703 - f1_metric: 0.86 - ETA: 0s - loss: 0.2769 - accuracy: 0.8730 - f1_metric: 0.87 - ETA: 0s - loss: 0.2751 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2739 - accuracy: 0.8694 - f1_metric: 0.86 - ETA: 0s - loss: 0.2754 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2760 - accuracy: 0.8625 - f1_metric: 0.86 - ETA: 0s - loss: 0.2752 - accuracy: 0.8606 - f1_metric: 0.86 - ETA: 0s - loss: 0.2772 - accuracy: 0.8579 - f1_metric: 0.85 - ETA: 0s - loss: 0.2779 - accuracy: 0.8581 - f1_metric: 0.8585\n",
      "Epoch 00075: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2779 - accuracy: 0.8581 - f1_metric: 0.8585 - val_loss: 0.4772 - val_accuracy: 0.8240 - val_f1_metric: 0.8239 - lr: 1.0000e-04\n",
      "Epoch 76/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2589 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2429 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2596 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2694 - accuracy: 0.8652 - f1_metric: 0.86 - ETA: 0s - loss: 0.2659 - accuracy: 0.8736 - f1_metric: 0.87 - ETA: 0s - loss: 0.2671 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2701 - accuracy: 0.8732 - f1_metric: 0.87 - ETA: 0s - loss: 0.2689 - accuracy: 0.8725 - f1_metric: 0.87 - ETA: 0s - loss: 0.2661 - accuracy: 0.8743 - f1_metric: 0.87 - ETA: 0s - loss: 0.2663 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2685 - accuracy: 0.8705 - f1_metric: 0.87 - ETA: 0s - loss: 0.2687 - accuracy: 0.8703 - f1_metric: 0.87 - ETA: 0s - loss: 0.2703 - accuracy: 0.8684 - f1_metric: 0.86 - ETA: 0s - loss: 0.2704 - accuracy: 0.8683 - f1_metric: 0.86 - ETA: 0s - loss: 0.2765 - accuracy: 0.8661 - f1_metric: 0.86 - ETA: 0s - loss: 0.2738 - accuracy: 0.8660 - f1_metric: 0.86 - ETA: 0s - loss: 0.2719 - accuracy: 0.8683 - f1_metric: 0.8685\n",
      "Epoch 00076: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2716 - accuracy: 0.8692 - f1_metric: 0.8699 - val_loss: 0.4712 - val_accuracy: 0.8265 - val_f1_metric: 0.8275 - lr: 1.0000e-04\n",
      "Epoch 77/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2733 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2700 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2741 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2634 - accuracy: 0.8574 - f1_metric: 0.85 - ETA: 0s - loss: 0.2725 - accuracy: 0.8531 - f1_metric: 0.85 - ETA: 0s - loss: 0.2775 - accuracy: 0.8642 - f1_metric: 0.86 - ETA: 0s - loss: 0.2720 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2706 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2719 - accuracy: 0.8653 - f1_metric: 0.86 - ETA: 0s - loss: 0.2698 - accuracy: 0.8607 - f1_metric: 0.85 - ETA: 0s - loss: 0.2668 - accuracy: 0.8617 - f1_metric: 0.86 - ETA: 0s - loss: 0.2656 - accuracy: 0.8630 - f1_metric: 0.86 - ETA: 0s - loss: 0.2698 - accuracy: 0.8670 - f1_metric: 0.86 - ETA: 0s - loss: 0.2662 - accuracy: 0.8689 - f1_metric: 0.86 - ETA: 0s - loss: 0.2646 - accuracy: 0.8702 - f1_metric: 0.86 - ETA: 0s - loss: 0.2649 - accuracy: 0.8698 - f1_metric: 0.8694\n",
      "Epoch 00077: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2663 - accuracy: 0.8696 - f1_metric: 0.8684 - val_loss: 0.4560 - val_accuracy: 0.8349 - val_f1_metric: 0.8357 - lr: 1.0000e-04\n",
      "Epoch 78/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.2715 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2496 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2459 - accuracy: 0.8839 - f1_metric: 0.88 - ETA: 0s - loss: 0.2523 - accuracy: 0.8766 - f1_metric: 0.87 - ETA: 0s - loss: 0.2557 - accuracy: 0.8738 - f1_metric: 0.87 - ETA: 0s - loss: 0.2582 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2595 - accuracy: 0.8635 - f1_metric: 0.86 - ETA: 0s - loss: 0.2570 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2623 - accuracy: 0.8607 - f1_metric: 0.86 - ETA: 0s - loss: 0.2632 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2694 - accuracy: 0.8609 - f1_metric: 0.86 - ETA: 0s - loss: 0.2701 - accuracy: 0.8640 - f1_metric: 0.86 - ETA: 0s - loss: 0.2758 - accuracy: 0.8636 - f1_metric: 0.86 - ETA: 0s - loss: 0.2725 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2731 - accuracy: 0.8659 - f1_metric: 0.8660\n",
      "Epoch 00078: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2720 - accuracy: 0.8670 - f1_metric: 0.8683 - val_loss: 0.4736 - val_accuracy: 0.8240 - val_f1_metric: 0.8239 - lr: 1.0000e-04\n",
      "Epoch 79/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2582 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2583 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.2774 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2801 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2687 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2814 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2833 - accuracy: 0.8550 - f1_metric: 0.85 - ETA: 0s - loss: 0.2778 - accuracy: 0.8570 - f1_metric: 0.85 - ETA: 0s - loss: 0.2726 - accuracy: 0.8565 - f1_metric: 0.85 - ETA: 0s - loss: 0.2723 - accuracy: 0.8575 - f1_metric: 0.85 - ETA: 0s - loss: 0.2713 - accuracy: 0.8577 - f1_metric: 0.85 - ETA: 0s - loss: 0.2733 - accuracy: 0.8599 - f1_metric: 0.85 - ETA: 0s - loss: 0.2713 - accuracy: 0.8627 - f1_metric: 0.86 - ETA: 0s - loss: 0.2728 - accuracy: 0.8655 - f1_metric: 0.86 - ETA: 0s - loss: 0.2750 - accuracy: 0.8658 - f1_metric: 0.86 - ETA: 0s - loss: 0.2727 - accuracy: 0.8674 - f1_metric: 0.86 - ETA: 0s - loss: 0.2738 - accuracy: 0.8660 - f1_metric: 0.8643\n",
      "Epoch 00079: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2738 - accuracy: 0.8660 - f1_metric: 0.8643 - val_loss: 0.4740 - val_accuracy: 0.8248 - val_f1_metric: 0.8245 - lr: 1.0000e-04\n",
      "Epoch 80/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2991 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2474 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2605 - accuracy: 0.8683 - f1_metric: 0.86 - ETA: 0s - loss: 0.2605 - accuracy: 0.8580 - f1_metric: 0.85 - ETA: 0s - loss: 0.2551 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2494 - accuracy: 0.8623 - f1_metric: 0.86 - ETA: 0s - loss: 0.2574 - accuracy: 0.8559 - f1_metric: 0.85 - ETA: 0s - loss: 0.2610 - accuracy: 0.8557 - f1_metric: 0.85 - ETA: 0s - loss: 0.2543 - accuracy: 0.8614 - f1_metric: 0.86 - ETA: 0s - loss: 0.2577 - accuracy: 0.8630 - f1_metric: 0.86 - ETA: 0s - loss: 0.2554 - accuracy: 0.8655 - f1_metric: 0.86 - ETA: 0s - loss: 0.2551 - accuracy: 0.8661 - f1_metric: 0.86 - ETA: 0s - loss: 0.2570 - accuracy: 0.8667 - f1_metric: 0.86 - ETA: 0s - loss: 0.2585 - accuracy: 0.8661 - f1_metric: 0.86 - ETA: 0s - loss: 0.2570 - accuracy: 0.8682 - f1_metric: 0.86 - ETA: 0s - loss: 0.2597 - accuracy: 0.8670 - f1_metric: 0.86 - ETA: 0s - loss: 0.2616 - accuracy: 0.8676 - f1_metric: 0.8675\n",
      "Epoch 00080: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2625 - accuracy: 0.8688 - f1_metric: 0.8688 - val_loss: 0.4437 - val_accuracy: 0.8407 - val_f1_metric: 0.8399 - lr: 1.0000e-04\n",
      "Epoch 81/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3272 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2994 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2741 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2686 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2739 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2671 - accuracy: 0.8705 - f1_metric: 0.87 - ETA: 0s - loss: 0.2633 - accuracy: 0.8768 - f1_metric: 0.87 - ETA: 0s - loss: 0.2630 - accuracy: 0.8775 - f1_metric: 0.87 - ETA: 0s - loss: 0.2660 - accuracy: 0.8714 - f1_metric: 0.87 - ETA: 0s - loss: 0.2622 - accuracy: 0.8706 - f1_metric: 0.87 - ETA: 0s - loss: 0.2593 - accuracy: 0.8722 - f1_metric: 0.87 - ETA: 0s - loss: 0.2600 - accuracy: 0.8730 - f1_metric: 0.87 - ETA: 0s - loss: 0.2584 - accuracy: 0.8722 - f1_metric: 0.87 - ETA: 0s - loss: 0.2573 - accuracy: 0.8716 - f1_metric: 0.87 - ETA: 0s - loss: 0.2593 - accuracy: 0.8730 - f1_metric: 0.87 - ETA: 0s - loss: 0.2585 - accuracy: 0.8750 - f1_metric: 0.8748\n",
      "Epoch 00081: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2595 - accuracy: 0.8746 - f1_metric: 0.8743 - val_loss: 0.4532 - val_accuracy: 0.8349 - val_f1_metric: 0.8350 - lr: 1.0000e-04\n",
      "Epoch 82/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2521 - accuracy: 0.8555 - f1_metric: 0.85 - ETA: 0s - loss: 0.2503 - accuracy: 0.8616 - f1_metric: 0.86 - ETA: 0s - loss: 0.2542 - accuracy: 0.8687 - f1_metric: 0.86 - ETA: 0s - loss: 0.2593 - accuracy: 0.8630 - f1_metric: 0.86 - ETA: 0s - loss: 0.2565 - accuracy: 0.8701 - f1_metric: 0.87 - ETA: 0s - loss: 0.2696 - accuracy: 0.8692 - f1_metric: 0.86 - ETA: 0s - loss: 0.2711 - accuracy: 0.8700 - f1_metric: 0.87 - ETA: 0s - loss: 0.2760 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2713 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2757 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2727 - accuracy: 0.8741 - f1_metric: 0.87 - ETA: 0s - loss: 0.2728 - accuracy: 0.8725 - f1_metric: 0.87 - ETA: 0s - loss: 0.2709 - accuracy: 0.8703 - f1_metric: 0.87 - ETA: 0s - loss: 0.2682 - accuracy: 0.8717 - f1_metric: 0.8717\n",
      "Epoch 00082: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2676 - accuracy: 0.8724 - f1_metric: 0.8732 - val_loss: 0.4697 - val_accuracy: 0.8256 - val_f1_metric: 0.8258 - lr: 1.0000e-04\n",
      "Epoch 83/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2326 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2836 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.2759 - accuracy: 0.8326 - f1_metric: 0.83 - ETA: 0s - loss: 0.2803 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2772 - accuracy: 0.8477 - f1_metric: 0.84 - ETA: 0s - loss: 0.2776 - accuracy: 0.8448 - f1_metric: 0.84 - ETA: 0s - loss: 0.2782 - accuracy: 0.8455 - f1_metric: 0.84 - ETA: 0s - loss: 0.2775 - accuracy: 0.8484 - f1_metric: 0.84 - ETA: 0s - loss: 0.2737 - accuracy: 0.8546 - f1_metric: 0.85 - ETA: 0s - loss: 0.2731 - accuracy: 0.8575 - f1_metric: 0.85 - ETA: 0s - loss: 0.2817 - accuracy: 0.8616 - f1_metric: 0.86 - ETA: 0s - loss: 0.2800 - accuracy: 0.8599 - f1_metric: 0.86 - ETA: 0s - loss: 0.2780 - accuracy: 0.8589 - f1_metric: 0.85 - ETA: 0s - loss: 0.2774 - accuracy: 0.8555 - f1_metric: 0.85 - ETA: 0s - loss: 0.2731 - accuracy: 0.8602 - f1_metric: 0.86 - ETA: 0s - loss: 0.2688 - accuracy: 0.8616 - f1_metric: 0.8617\n",
      "Epoch 00083: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2722 - accuracy: 0.8609 - f1_metric: 0.8616 - val_loss: 0.4592 - val_accuracy: 0.8298 - val_f1_metric: 0.8299 - lr: 1.0000e-04\n",
      "Epoch 84/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2804 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2670 - accuracy: 0.8770 - f1_metric: 0.87 - ETA: 0s - loss: 0.2724 - accuracy: 0.8703 - f1_metric: 0.87 - ETA: 0s - loss: 0.2709 - accuracy: 0.8737 - f1_metric: 0.87 - ETA: 0s - loss: 0.2705 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2685 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2715 - accuracy: 0.8663 - f1_metric: 0.86 - ETA: 0s - loss: 0.2696 - accuracy: 0.8680 - f1_metric: 0.86 - ETA: 0s - loss: 0.2695 - accuracy: 0.8675 - f1_metric: 0.86 - ETA: 0s - loss: 0.2703 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2688 - accuracy: 0.8642 - f1_metric: 0.86 - ETA: 0s - loss: 0.2716 - accuracy: 0.8662 - f1_metric: 0.86 - ETA: 0s - loss: 0.2722 - accuracy: 0.8634 - f1_metric: 0.86 - ETA: 0s - loss: 0.2703 - accuracy: 0.8657 - f1_metric: 0.86 - ETA: 0s - loss: 0.2679 - accuracy: 0.8648 - f1_metric: 0.86 - ETA: 0s - loss: 0.2671 - accuracy: 0.8656 - f1_metric: 0.8657\n",
      "Epoch 00084: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2688 - accuracy: 0.8656 - f1_metric: 0.8658 - val_loss: 0.4630 - val_accuracy: 0.8248 - val_f1_metric: 0.8233 - lr: 1.0000e-04\n",
      "Epoch 85/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2472 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2976 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2857 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.2846 - accuracy: 0.8609 - f1_metric: 0.86 - ETA: 0s - loss: 0.2778 - accuracy: 0.8642 - f1_metric: 0.86 - ETA: 0s - loss: 0.2744 - accuracy: 0.8652 - f1_metric: 0.86 - ETA: 0s - loss: 0.2812 - accuracy: 0.8561 - f1_metric: 0.85 - ETA: 0s - loss: 0.2774 - accuracy: 0.8608 - f1_metric: 0.86 - ETA: 0s - loss: 0.2753 - accuracy: 0.8568 - f1_metric: 0.85 - ETA: 0s - loss: 0.2710 - accuracy: 0.8634 - f1_metric: 0.86 - ETA: 0s - loss: 0.2723 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2660 - accuracy: 0.8674 - f1_metric: 0.86 - ETA: 0s - loss: 0.2664 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2691 - accuracy: 0.8686 - f1_metric: 0.86 - ETA: 0s - loss: 0.2670 - accuracy: 0.8679 - f1_metric: 0.8672\n",
      "Epoch 00085: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2678 - accuracy: 0.8678 - f1_metric: 0.8668 - val_loss: 0.4551 - val_accuracy: 0.8340 - val_f1_metric: 0.8336 - lr: 1.0000e-04\n",
      "Epoch 86/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2564 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2392 - accuracy: 0.8839 - f1_metric: 0.88 - ETA: 0s - loss: 0.2504 - accuracy: 0.8835 - f1_metric: 0.88 - ETA: 0s - loss: 0.2443 - accuracy: 0.8884 - f1_metric: 0.88 - ETA: 0s - loss: 0.2548 - accuracy: 0.8778 - f1_metric: 0.87 - ETA: 0s - loss: 0.2557 - accuracy: 0.8805 - f1_metric: 0.88 - ETA: 0s - loss: 0.2546 - accuracy: 0.8770 - f1_metric: 0.87 - ETA: 0s - loss: 0.2586 - accuracy: 0.8756 - f1_metric: 0.87 - ETA: 0s - loss: 0.2550 - accuracy: 0.8782 - f1_metric: 0.87 - ETA: 0s - loss: 0.2582 - accuracy: 0.8774 - f1_metric: 0.87 - ETA: 0s - loss: 0.2586 - accuracy: 0.8767 - f1_metric: 0.87 - ETA: 0s - loss: 0.2609 - accuracy: 0.8762 - f1_metric: 0.87 - ETA: 0s - loss: 0.2601 - accuracy: 0.8765 - f1_metric: 0.87 - ETA: 0s - loss: 0.2611 - accuracy: 0.8750 - f1_metric: 0.8743\n",
      "Epoch 00086: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2611 - accuracy: 0.8750 - f1_metric: 0.8743 - val_loss: 0.4658 - val_accuracy: 0.8282 - val_f1_metric: 0.8279 - lr: 1.0000e-04\n",
      "Epoch 87/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2737 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2634 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2576 - accuracy: 0.8691 - f1_metric: 0.87 - ETA: 0s - loss: 0.2540 - accuracy: 0.8722 - f1_metric: 0.87 - ETA: 0s - loss: 0.2674 - accuracy: 0.8738 - f1_metric: 0.87 - ETA: 0s - loss: 0.2654 - accuracy: 0.8771 - f1_metric: 0.87 - ETA: 0s - loss: 0.2607 - accuracy: 0.8787 - f1_metric: 0.87 - ETA: 0s - loss: 0.2621 - accuracy: 0.8791 - f1_metric: 0.87 - ETA: 0s - loss: 0.2599 - accuracy: 0.8814 - f1_metric: 0.88 - ETA: 0s - loss: 0.2588 - accuracy: 0.8819 - f1_metric: 0.88 - ETA: 0s - loss: 0.2528 - accuracy: 0.8848 - f1_metric: 0.88 - ETA: 0s - loss: 0.2572 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2575 - accuracy: 0.8794 - f1_metric: 0.87 - ETA: 0s - loss: 0.2573 - accuracy: 0.8764 - f1_metric: 0.87 - ETA: 0s - loss: 0.2565 - accuracy: 0.8790 - f1_metric: 0.87 - ETA: 0s - loss: 0.2629 - accuracy: 0.8788 - f1_metric: 0.87 - ETA: 0s - loss: 0.2622 - accuracy: 0.8806 - f1_metric: 0.88 - ETA: 0s - loss: 0.2594 - accuracy: 0.8819 - f1_metric: 0.88 - ETA: 0s - loss: 0.2630 - accuracy: 0.8807 - f1_metric: 0.8818\n",
      "Epoch 00087: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2630 - accuracy: 0.8807 - f1_metric: 0.8818 - val_loss: 0.4693 - val_accuracy: 0.8315 - val_f1_metric: 0.8300 - lr: 1.0000e-04\n",
      "Epoch 88/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3761 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2727 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2581 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2581 - accuracy: 0.8652 - f1_metric: 0.86 - ETA: 0s - loss: 0.2713 - accuracy: 0.8622 - f1_metric: 0.86 - ETA: 0s - loss: 0.2568 - accuracy: 0.8705 - f1_metric: 0.87 - ETA: 0s - loss: 0.2557 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.2517 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2574 - accuracy: 0.8631 - f1_metric: 0.86 - ETA: 0s - loss: 0.2584 - accuracy: 0.8641 - f1_metric: 0.86 - ETA: 0s - loss: 0.2613 - accuracy: 0.8681 - f1_metric: 0.86 - ETA: 0s - loss: 0.2596 - accuracy: 0.8717 - f1_metric: 0.87 - ETA: 0s - loss: 0.2604 - accuracy: 0.8693 - f1_metric: 0.86 - ETA: 0s - loss: 0.2716 - accuracy: 0.8670 - f1_metric: 0.86 - ETA: 0s - loss: 0.2696 - accuracy: 0.8679 - f1_metric: 0.86 - ETA: 0s - loss: 0.2685 - accuracy: 0.8708 - f1_metric: 0.87 - ETA: 0s - loss: 0.2669 - accuracy: 0.8738 - f1_metric: 0.87 - ETA: 0s - loss: 0.2678 - accuracy: 0.8750 - f1_metric: 0.8746\n",
      "Epoch 00088: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2675 - accuracy: 0.8746 - f1_metric: 0.8745 - val_loss: 0.4725 - val_accuracy: 0.8248 - val_f1_metric: 0.8245 - lr: 1.0000e-04\n",
      "Epoch 89/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2716 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2588 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.2712 - accuracy: 0.8613 - f1_metric: 0.86 - ETA: 0s - loss: 0.2603 - accuracy: 0.8707 - f1_metric: 0.86 - ETA: 0s - loss: 0.2624 - accuracy: 0.8616 - f1_metric: 0.86 - ETA: 0s - loss: 0.2563 - accuracy: 0.8658 - f1_metric: 0.86 - ETA: 0s - loss: 0.2568 - accuracy: 0.8664 - f1_metric: 0.86 - ETA: 0s - loss: 0.2588 - accuracy: 0.8658 - f1_metric: 0.86 - ETA: 0s - loss: 0.2627 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2631 - accuracy: 0.8644 - f1_metric: 0.86 - ETA: 0s - loss: 0.2612 - accuracy: 0.8654 - f1_metric: 0.86 - ETA: 0s - loss: 0.2587 - accuracy: 0.8686 - f1_metric: 0.86 - ETA: 0s - loss: 0.2580 - accuracy: 0.8707 - f1_metric: 0.87 - ETA: 0s - loss: 0.2625 - accuracy: 0.8706 - f1_metric: 0.87 - ETA: 0s - loss: 0.2637 - accuracy: 0.8720 - f1_metric: 0.8716\n",
      "Epoch 00089: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2638 - accuracy: 0.8721 - f1_metric: 0.8713 - val_loss: 0.4567 - val_accuracy: 0.8332 - val_f1_metric: 0.8325 - lr: 1.0000e-04\n",
      "Epoch 90/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.2599 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2665 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2667 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2558 - accuracy: 0.8764 - f1_metric: 0.87 - ETA: 0s - loss: 0.2457 - accuracy: 0.8761 - f1_metric: 0.87 - ETA: 0s - loss: 0.2397 - accuracy: 0.8793 - f1_metric: 0.87 - ETA: 0s - loss: 0.2502 - accuracy: 0.8844 - f1_metric: 0.88 - ETA: 0s - loss: 0.2512 - accuracy: 0.8857 - f1_metric: 0.88 - ETA: 0s - loss: 0.2512 - accuracy: 0.8863 - f1_metric: 0.88 - ETA: 0s - loss: 0.2564 - accuracy: 0.8834 - f1_metric: 0.88 - ETA: 0s - loss: 0.2582 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2580 - accuracy: 0.8816 - f1_metric: 0.88 - ETA: 0s - loss: 0.2593 - accuracy: 0.8835 - f1_metric: 0.88 - ETA: 0s - loss: 0.2638 - accuracy: 0.8771 - f1_metric: 0.87 - ETA: 0s - loss: 0.2635 - accuracy: 0.8780 - f1_metric: 0.87 - ETA: 0s - loss: 0.2643 - accuracy: 0.8779 - f1_metric: 0.8777\n",
      "Epoch 00090: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2663 - accuracy: 0.8775 - f1_metric: 0.8768 - val_loss: 0.4753 - val_accuracy: 0.8223 - val_f1_metric: 0.8229 - lr: 1.0000e-04\n",
      "Epoch 91/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2614 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2596 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2576 - accuracy: 0.8464 - f1_metric: 0.84 - ETA: 0s - loss: 0.2690 - accuracy: 0.8368 - f1_metric: 0.83 - ETA: 0s - loss: 0.2638 - accuracy: 0.8409 - f1_metric: 0.84 - ETA: 0s - loss: 0.2582 - accuracy: 0.8404 - f1_metric: 0.83 - ETA: 0s - loss: 0.2571 - accuracy: 0.8447 - f1_metric: 0.84 - ETA: 0s - loss: 0.2622 - accuracy: 0.8539 - f1_metric: 0.85 - ETA: 0s - loss: 0.2614 - accuracy: 0.8573 - f1_metric: 0.85 - ETA: 0s - loss: 0.2613 - accuracy: 0.8576 - f1_metric: 0.85 - ETA: 0s - loss: 0.2651 - accuracy: 0.8599 - f1_metric: 0.85 - ETA: 0s - loss: 0.2628 - accuracy: 0.8604 - f1_metric: 0.85 - ETA: 0s - loss: 0.2624 - accuracy: 0.8603 - f1_metric: 0.85 - ETA: 0s - loss: 0.2638 - accuracy: 0.8621 - f1_metric: 0.86 - ETA: 0s - loss: 0.2646 - accuracy: 0.8623 - f1_metric: 0.86 - ETA: 0s - loss: 0.2660 - accuracy: 0.8606 - f1_metric: 0.86 - ETA: 0s - loss: 0.2679 - accuracy: 0.8612 - f1_metric: 0.8608\n",
      "Epoch 00091: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2655 - accuracy: 0.8631 - f1_metric: 0.8632 - val_loss: 0.4610 - val_accuracy: 0.8315 - val_f1_metric: 0.8309 - lr: 1.0000e-04\n",
      "Epoch 92/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2601 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2696 - accuracy: 0.8781 - f1_metric: 0.87 - ETA: 0s - loss: 0.2655 - accuracy: 0.8661 - f1_metric: 0.86 - ETA: 0s - loss: 0.2613 - accuracy: 0.8663 - f1_metric: 0.86 - ETA: 0s - loss: 0.2603 - accuracy: 0.8707 - f1_metric: 0.87 - ETA: 0s - loss: 0.2602 - accuracy: 0.8738 - f1_metric: 0.87 - ETA: 0s - loss: 0.2662 - accuracy: 0.8708 - f1_metric: 0.87 - ETA: 0s - loss: 0.2656 - accuracy: 0.8681 - f1_metric: 0.86 - ETA: 0s - loss: 0.2636 - accuracy: 0.8648 - f1_metric: 0.86 - ETA: 0s - loss: 0.2648 - accuracy: 0.8658 - f1_metric: 0.86 - ETA: 0s - loss: 0.2675 - accuracy: 0.8678 - f1_metric: 0.86 - ETA: 0s - loss: 0.2700 - accuracy: 0.8660 - f1_metric: 0.86 - ETA: 0s - loss: 0.2751 - accuracy: 0.8655 - f1_metric: 0.86 - ETA: 0s - loss: 0.2758 - accuracy: 0.8667 - f1_metric: 0.86 - ETA: 0s - loss: 0.2747 - accuracy: 0.8674 - f1_metric: 0.86 - ETA: 0s - loss: 0.2724 - accuracy: 0.8674 - f1_metric: 0.86 - ETA: 0s - loss: 0.2690 - accuracy: 0.8699 - f1_metric: 0.87 - ETA: 0s - loss: 0.2749 - accuracy: 0.8668 - f1_metric: 0.86 - ETA: 0s - loss: 0.2759 - accuracy: 0.8685 - f1_metric: 0.8689\n",
      "Epoch 00092: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2756 - accuracy: 0.8685 - f1_metric: 0.8689 - val_loss: 0.4834 - val_accuracy: 0.8231 - val_f1_metric: 0.8221 - lr: 1.0000e-04\n",
      "Epoch 93/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2465 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2698 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2723 - accuracy: 0.8817 - f1_metric: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2566 - accuracy: 0.8776 - f1_metric: 0.87 - ETA: 0s - loss: 0.2534 - accuracy: 0.8771 - f1_metric: 0.87 - ETA: 0s - loss: 0.2513 - accuracy: 0.8778 - f1_metric: 0.87 - ETA: 0s - loss: 0.2474 - accuracy: 0.8791 - f1_metric: 0.87 - ETA: 0s - loss: 0.2462 - accuracy: 0.8824 - f1_metric: 0.88 - ETA: 0s - loss: 0.2492 - accuracy: 0.8804 - f1_metric: 0.88 - ETA: 0s - loss: 0.2481 - accuracy: 0.8806 - f1_metric: 0.88 - ETA: 0s - loss: 0.2473 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2516 - accuracy: 0.8745 - f1_metric: 0.87 - ETA: 0s - loss: 0.2542 - accuracy: 0.8726 - f1_metric: 0.87 - ETA: 0s - loss: 0.2552 - accuracy: 0.8704 - f1_metric: 0.87 - ETA: 0s - loss: 0.2586 - accuracy: 0.8682 - f1_metric: 0.86 - ETA: 0s - loss: 0.2580 - accuracy: 0.8687 - f1_metric: 0.86 - ETA: 0s - loss: 0.2580 - accuracy: 0.8709 - f1_metric: 0.8710\n",
      "Epoch 00093: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2577 - accuracy: 0.8696 - f1_metric: 0.8693 - val_loss: 0.4378 - val_accuracy: 0.8474 - val_f1_metric: 0.8470 - lr: 1.0000e-04\n",
      "Epoch 94/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.2179 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2522 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.2583 - accuracy: 0.8504 - f1_metric: 0.85 - ETA: 0s - loss: 0.2672 - accuracy: 0.8490 - f1_metric: 0.84 - ETA: 0s - loss: 0.2729 - accuracy: 0.8551 - f1_metric: 0.85 - ETA: 0s - loss: 0.2723 - accuracy: 0.8582 - f1_metric: 0.85 - ETA: 0s - loss: 0.2704 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2634 - accuracy: 0.8612 - f1_metric: 0.86 - ETA: 0s - loss: 0.2634 - accuracy: 0.8643 - f1_metric: 0.86 - ETA: 0s - loss: 0.2607 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2587 - accuracy: 0.8679 - f1_metric: 0.86 - ETA: 0s - loss: 0.2575 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2543 - accuracy: 0.8726 - f1_metric: 0.87 - ETA: 0s - loss: 0.2567 - accuracy: 0.8756 - f1_metric: 0.87 - ETA: 0s - loss: 0.2565 - accuracy: 0.8760 - f1_metric: 0.87 - ETA: 0s - loss: 0.2568 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2604 - accuracy: 0.8795 - f1_metric: 0.87 - ETA: 0s - loss: 0.2628 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2653 - accuracy: 0.8754 - f1_metric: 0.87 - ETA: 0s - loss: 0.2658 - accuracy: 0.8750 - f1_metric: 0.8746\n",
      "Epoch 00094: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 32ms/step - loss: 0.2662 - accuracy: 0.8746 - f1_metric: 0.8730 - val_loss: 0.4938 - val_accuracy: 0.8173 - val_f1_metric: 0.8187 - lr: 1.0000e-04\n",
      "Epoch 95/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2682 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2660 - accuracy: 0.8616 - f1_metric: 0.86 - ETA: 0s - loss: 0.2655 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2679 - accuracy: 0.8608 - f1_metric: 0.86 - ETA: 0s - loss: 0.2616 - accuracy: 0.8642 - f1_metric: 0.86 - ETA: 0s - loss: 0.2595 - accuracy: 0.8604 - f1_metric: 0.86 - ETA: 0s - loss: 0.2598 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2527 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2538 - accuracy: 0.8641 - f1_metric: 0.86 - ETA: 0s - loss: 0.2557 - accuracy: 0.8624 - f1_metric: 0.86 - ETA: 0s - loss: 0.2551 - accuracy: 0.8610 - f1_metric: 0.86 - ETA: 0s - loss: 0.2566 - accuracy: 0.8609 - f1_metric: 0.86 - ETA: 0s - loss: 0.2556 - accuracy: 0.8604 - f1_metric: 0.86 - ETA: 0s - loss: 0.2580 - accuracy: 0.8616 - f1_metric: 0.86 - ETA: 0s - loss: 0.2588 - accuracy: 0.8602 - f1_metric: 0.86 - ETA: 0s - loss: 0.2632 - accuracy: 0.8621 - f1_metric: 0.86 - ETA: 0s - loss: 0.2609 - accuracy: 0.8623 - f1_metric: 0.8622\n",
      "Epoch 00095: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2600 - accuracy: 0.8635 - f1_metric: 0.8646 - val_loss: 0.4512 - val_accuracy: 0.8374 - val_f1_metric: 0.8348 - lr: 1.0000e-04\n",
      "Epoch 96/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2721 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2618 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2525 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2566 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2662 - accuracy: 0.8862 - f1_metric: 0.88 - ETA: 0s - loss: 0.2626 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2597 - accuracy: 0.8863 - f1_metric: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.8820 - f1_metric: 0.88 - ETA: 0s - loss: 0.2628 - accuracy: 0.8800 - f1_metric: 0.88 - ETA: 0s - loss: 0.2579 - accuracy: 0.8809 - f1_metric: 0.88 - ETA: 0s - loss: 0.2536 - accuracy: 0.8840 - f1_metric: 0.88 - ETA: 0s - loss: 0.2533 - accuracy: 0.8825 - f1_metric: 0.88 - ETA: 0s - loss: 0.2534 - accuracy: 0.8826 - f1_metric: 0.88 - ETA: 0s - loss: 0.2649 - accuracy: 0.8810 - f1_metric: 0.88 - ETA: 0s - loss: 0.2652 - accuracy: 0.8811 - f1_metric: 0.88 - ETA: 0s - loss: 0.2663 - accuracy: 0.8791 - f1_metric: 0.87 - ETA: 0s - loss: 0.2656 - accuracy: 0.8773 - f1_metric: 0.87 - ETA: 0s - loss: 0.2646 - accuracy: 0.8775 - f1_metric: 0.8777\n",
      "Epoch 00096: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.2637 - accuracy: 0.8785 - f1_metric: 0.8798 - val_loss: 0.4572 - val_accuracy: 0.8340 - val_f1_metric: 0.8335 - lr: 1.0000e-04\n",
      "Epoch 97/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3017 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.2593 - accuracy: 0.8398 - f1_metric: 0.84 - ETA: 0s - loss: 0.2605 - accuracy: 0.8620 - f1_metric: 0.86 - ETA: 0s - loss: 0.2648 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2679 - accuracy: 0.8707 - f1_metric: 0.87 - ETA: 0s - loss: 0.2655 - accuracy: 0.8726 - f1_metric: 0.87 - ETA: 0s - loss: 0.2595 - accuracy: 0.8740 - f1_metric: 0.87 - ETA: 0s - loss: 0.2535 - accuracy: 0.8787 - f1_metric: 0.87 - ETA: 0s - loss: 0.2528 - accuracy: 0.8840 - f1_metric: 0.88 - ETA: 0s - loss: 0.2508 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2514 - accuracy: 0.8822 - f1_metric: 0.88 - ETA: 0s - loss: 0.2502 - accuracy: 0.8848 - f1_metric: 0.88 - ETA: 0s - loss: 0.2513 - accuracy: 0.8858 - f1_metric: 0.88 - ETA: 0s - loss: 0.2544 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2591 - accuracy: 0.8835 - f1_metric: 0.88 - ETA: 0s - loss: 0.2636 - accuracy: 0.8818 - f1_metric: 0.88 - ETA: 0s - loss: 0.2629 - accuracy: 0.8813 - f1_metric: 0.8814\n",
      "Epoch 00097: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2616 - accuracy: 0.8814 - f1_metric: 0.8821 - val_loss: 0.4870 - val_accuracy: 0.8189 - val_f1_metric: 0.8172 - lr: 1.0000e-04\n",
      "Epoch 98/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9375 - f1_metric: 0.93 - ETA: 0s - loss: 0.2691 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2738 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2678 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2711 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2653 - accuracy: 0.8620 - f1_metric: 0.86 - ETA: 0s - loss: 0.2690 - accuracy: 0.8583 - f1_metric: 0.85 - ETA: 0s - loss: 0.2679 - accuracy: 0.8613 - f1_metric: 0.86 - ETA: 0s - loss: 0.2673 - accuracy: 0.8628 - f1_metric: 0.86 - ETA: 0s - loss: 0.2669 - accuracy: 0.8609 - f1_metric: 0.86 - ETA: 0s - loss: 0.2647 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2657 - accuracy: 0.8581 - f1_metric: 0.85 - ETA: 0s - loss: 0.2617 - accuracy: 0.8618 - f1_metric: 0.86 - ETA: 0s - loss: 0.2599 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.2620 - accuracy: 0.8654 - f1_metric: 0.86 - ETA: 0s - loss: 0.2674 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2714 - accuracy: 0.8628 - f1_metric: 0.86 - ETA: 0s - loss: 0.2703 - accuracy: 0.8623 - f1_metric: 0.86 - ETA: 0s - loss: 0.2689 - accuracy: 0.8629 - f1_metric: 0.86 - ETA: 0s - loss: 0.2692 - accuracy: 0.8634 - f1_metric: 0.8629\n",
      "Epoch 00098: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2689 - accuracy: 0.8638 - f1_metric: 0.8638 - val_loss: 0.4645 - val_accuracy: 0.8298 - val_f1_metric: 0.8301 - lr: 1.0000e-04\n",
      "Epoch 99/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.2513 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2817 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2807 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2697 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2670 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2720 - accuracy: 0.8841 - f1_metric: 0.88 - ETA: 0s - loss: 0.2823 - accuracy: 0.8862 - f1_metric: 0.88 - ETA: 0s - loss: 0.2731 - accuracy: 0.8838 - f1_metric: 0.88 - ETA: 0s - loss: 0.2778 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2765 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2783 - accuracy: 0.8786 - f1_metric: 0.87 - ETA: 0s - loss: 0.2757 - accuracy: 0.8794 - f1_metric: 0.87 - ETA: 0s - loss: 0.2712 - accuracy: 0.8802 - f1_metric: 0.87 - ETA: 0s - loss: 0.2715 - accuracy: 0.8798 - f1_metric: 0.87 - ETA: 0s - loss: 0.2680 - accuracy: 0.8794 - f1_metric: 0.87 - ETA: 0s - loss: 0.2683 - accuracy: 0.8768 - f1_metric: 0.87 - ETA: 0s - loss: 0.2658 - accuracy: 0.8799 - f1_metric: 0.87 - ETA: 0s - loss: 0.2665 - accuracy: 0.8792 - f1_metric: 0.8789\n",
      "Epoch 00099: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.2676 - accuracy: 0.8768 - f1_metric: 0.8761 - val_loss: 0.4633 - val_accuracy: 0.8298 - val_f1_metric: 0.8305 - lr: 1.0000e-04\n",
      "Epoch 100/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.2816 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2601 - accuracy: 0.8504 - f1_metric: 0.85 - ETA: 0s - loss: 0.2551 - accuracy: 0.8524 - f1_metric: 0.85 - ETA: 0s - loss: 0.2577 - accuracy: 0.8551 - f1_metric: 0.85 - ETA: 0s - loss: 0.2614 - accuracy: 0.8618 - f1_metric: 0.86 - ETA: 0s - loss: 0.2571 - accuracy: 0.8701 - f1_metric: 0.87 - ETA: 0s - loss: 0.2579 - accuracy: 0.8681 - f1_metric: 0.86 - ETA: 0s - loss: 0.2609 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2581 - accuracy: 0.8707 - f1_metric: 0.87 - ETA: 0s - loss: 0.2584 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2611 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2584 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2581 - accuracy: 0.8731 - f1_metric: 0.87 - ETA: 0s - loss: 0.2593 - accuracy: 0.8705 - f1_metric: 0.87 - ETA: 0s - loss: 0.2645 - accuracy: 0.8688 - f1_metric: 0.86 - ETA: 0s - loss: 0.2640 - accuracy: 0.8712 - f1_metric: 0.87 - ETA: 0s - loss: 0.2644 - accuracy: 0.8706 - f1_metric: 0.8707\n",
      "Epoch 00100: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2634 - accuracy: 0.8714 - f1_metric: 0.8722 - val_loss: 0.4707 - val_accuracy: 0.8290 - val_f1_metric: 0.8276 - lr: 1.0000e-04\n",
      "Epoch 101/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2572 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2221 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2559 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2568 - accuracy: 0.8715 - f1_metric: 0.87 - ETA: 0s - loss: 0.2641 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2783 - accuracy: 0.8687 - f1_metric: 0.86 - ETA: 0s - loss: 0.2766 - accuracy: 0.8713 - f1_metric: 0.87 - ETA: 0s - loss: 0.2746 - accuracy: 0.8742 - f1_metric: 0.87 - ETA: 0s - loss: 0.2758 - accuracy: 0.8757 - f1_metric: 0.87 - ETA: 0s - loss: 0.2705 - accuracy: 0.8768 - f1_metric: 0.87 - ETA: 0s - loss: 0.2678 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2718 - accuracy: 0.8688 - f1_metric: 0.86 - ETA: 0s - loss: 0.2673 - accuracy: 0.8715 - f1_metric: 0.87 - ETA: 0s - loss: 0.2666 - accuracy: 0.8717 - f1_metric: 0.87 - ETA: 0s - loss: 0.2656 - accuracy: 0.8723 - f1_metric: 0.87 - ETA: 0s - loss: 0.2658 - accuracy: 0.8731 - f1_metric: 0.8731\n",
      "Epoch 00101: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2650 - accuracy: 0.8721 - f1_metric: 0.8717 - val_loss: 0.4626 - val_accuracy: 0.8298 - val_f1_metric: 0.8302 - lr: 1.0000e-04\n",
      "Epoch 102/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2620 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2637 - accuracy: 0.8625 - f1_metric: 0.86 - ETA: 0s - loss: 0.2719 - accuracy: 0.8616 - f1_metric: 0.86 - ETA: 0s - loss: 0.2680 - accuracy: 0.8663 - f1_metric: 0.86 - ETA: 0s - loss: 0.2764 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2718 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2715 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2652 - accuracy: 0.8781 - f1_metric: 0.87 - ETA: 0s - loss: 0.2616 - accuracy: 0.8800 - f1_metric: 0.88 - ETA: 0s - loss: 0.2569 - accuracy: 0.8838 - f1_metric: 0.88 - ETA: 0s - loss: 0.2591 - accuracy: 0.8779 - f1_metric: 0.87 - ETA: 0s - loss: 0.2626 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2604 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2606 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2601 - accuracy: 0.8718 - f1_metric: 0.87 - ETA: 0s - loss: 0.2613 - accuracy: 0.8723 - f1_metric: 0.87 - ETA: 0s - loss: 0.2607 - accuracy: 0.8725 - f1_metric: 0.8726\n",
      "Epoch 00102: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2612 - accuracy: 0.8728 - f1_metric: 0.8733 - val_loss: 0.4519 - val_accuracy: 0.8382 - val_f1_metric: 0.8375 - lr: 1.0000e-04\n",
      "Epoch 103/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2466 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2103 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2546 - accuracy: 0.8844 - f1_metric: 0.88 - ETA: 0s - loss: 0.2475 - accuracy: 0.8839 - f1_metric: 0.88 - ETA: 0s - loss: 0.2477 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2490 - accuracy: 0.8807 - f1_metric: 0.88 - ETA: 0s - loss: 0.2492 - accuracy: 0.8786 - f1_metric: 0.87 - ETA: 0s - loss: 0.2526 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2522 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2556 - accuracy: 0.8852 - f1_metric: 0.88 - ETA: 0s - loss: 0.2577 - accuracy: 0.8871 - f1_metric: 0.88 - ETA: 0s - loss: 0.2599 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2564 - accuracy: 0.8876 - f1_metric: 0.88 - ETA: 0s - loss: 0.2567 - accuracy: 0.8850 - f1_metric: 0.88 - ETA: 0s - loss: 0.2558 - accuracy: 0.8839 - f1_metric: 0.88 - ETA: 0s - loss: 0.2583 - accuracy: 0.8848 - f1_metric: 0.88 - ETA: 0s - loss: 0.2571 - accuracy: 0.8810 - f1_metric: 0.88 - ETA: 0s - loss: 0.2563 - accuracy: 0.8811 - f1_metric: 0.88 - ETA: 0s - loss: 0.2549 - accuracy: 0.8808 - f1_metric: 0.88 - ETA: 0s - loss: 0.2538 - accuracy: 0.8801 - f1_metric: 0.87 - ETA: 0s - loss: 0.2534 - accuracy: 0.8810 - f1_metric: 0.8807\n",
      "Epoch 00103: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.2551 - accuracy: 0.8814 - f1_metric: 0.8814 - val_loss: 0.4524 - val_accuracy: 0.8365 - val_f1_metric: 0.8368 - lr: 1.0000e-04\n",
      "Epoch 104/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2290 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2534 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2649 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2781 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2771 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2700 - accuracy: 0.8628 - f1_metric: 0.86 - ETA: 0s - loss: 0.2653 - accuracy: 0.8707 - f1_metric: 0.87 - ETA: 0s - loss: 0.2647 - accuracy: 0.8700 - f1_metric: 0.87 - ETA: 0s - loss: 0.2660 - accuracy: 0.8689 - f1_metric: 0.86 - ETA: 0s - loss: 0.2671 - accuracy: 0.8690 - f1_metric: 0.86 - ETA: 0s - loss: 0.2660 - accuracy: 0.8699 - f1_metric: 0.86 - ETA: 0s - loss: 0.2645 - accuracy: 0.8716 - f1_metric: 0.87 - ETA: 0s - loss: 0.2626 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2612 - accuracy: 0.8709 - f1_metric: 0.8709\n",
      "Epoch 00104: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2594 - accuracy: 0.8724 - f1_metric: 0.8731 - val_loss: 0.4606 - val_accuracy: 0.8315 - val_f1_metric: 0.8308 - lr: 1.0000e-04\n",
      "Epoch 105/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2185 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2458 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2447 - accuracy: 0.8817 - f1_metric: 0.88 - ETA: 0s - loss: 0.2476 - accuracy: 0.8889 - f1_metric: 0.88 - ETA: 0s - loss: 0.2461 - accuracy: 0.8919 - f1_metric: 0.89 - ETA: 0s - loss: 0.2463 - accuracy: 0.8927 - f1_metric: 0.89 - ETA: 0s - loss: 0.2469 - accuracy: 0.8889 - f1_metric: 0.88 - ETA: 0s - loss: 0.2533 - accuracy: 0.8875 - f1_metric: 0.88 - ETA: 0s - loss: 0.2560 - accuracy: 0.8878 - f1_metric: 0.88 - ETA: 0s - loss: 0.2508 - accuracy: 0.8919 - f1_metric: 0.89 - ETA: 0s - loss: 0.2506 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2511 - accuracy: 0.8885 - f1_metric: 0.88 - ETA: 0s - loss: 0.2540 - accuracy: 0.8866 - f1_metric: 0.88 - ETA: 0s - loss: 0.2553 - accuracy: 0.8878 - f1_metric: 0.88 - ETA: 0s - loss: 0.2565 - accuracy: 0.8866 - f1_metric: 0.88 - ETA: 0s - loss: 0.2562 - accuracy: 0.8832 - f1_metric: 0.88 - ETA: 0s - loss: 0.2574 - accuracy: 0.8824 - f1_metric: 0.88 - ETA: 0s - loss: 0.2575 - accuracy: 0.8821 - f1_metric: 0.8815\n",
      "Epoch 00105: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2592 - accuracy: 0.8807 - f1_metric: 0.8796 - val_loss: 0.4651 - val_accuracy: 0.8282 - val_f1_metric: 0.8272 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2291 - accuracy: 0.9219 - f1_metric: 0.92 - ETA: 0s - loss: 0.2500 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2528 - accuracy: 0.8661 - f1_metric: 0.86 - ETA: 0s - loss: 0.2529 - accuracy: 0.8628 - f1_metric: 0.86 - ETA: 0s - loss: 0.2649 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2590 - accuracy: 0.8603 - f1_metric: 0.86 - ETA: 0s - loss: 0.2606 - accuracy: 0.8648 - f1_metric: 0.86 - ETA: 0s - loss: 0.2587 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2562 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2577 - accuracy: 0.8722 - f1_metric: 0.87 - ETA: 0s - loss: 0.2578 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2550 - accuracy: 0.8736 - f1_metric: 0.87 - ETA: 0s - loss: 0.2567 - accuracy: 0.8702 - f1_metric: 0.87 - ETA: 0s - loss: 0.2604 - accuracy: 0.8678 - f1_metric: 0.86 - ETA: 0s - loss: 0.2611 - accuracy: 0.8698 - f1_metric: 0.8701\n",
      "Epoch 00106: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2618 - accuracy: 0.8696 - f1_metric: 0.8703 - val_loss: 0.4589 - val_accuracy: 0.8324 - val_f1_metric: 0.8323 - lr: 1.0000e-04\n",
      "Epoch 107/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.9375 - f1_metric: 0.93 - ETA: 0s - loss: 0.2776 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2688 - accuracy: 0.8304 - f1_metric: 0.83 - ETA: 0s - loss: 0.2678 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2541 - accuracy: 0.8642 - f1_metric: 0.86 - ETA: 0s - loss: 0.2512 - accuracy: 0.8682 - f1_metric: 0.86 - ETA: 0s - loss: 0.2506 - accuracy: 0.8681 - f1_metric: 0.86 - ETA: 0s - loss: 0.2484 - accuracy: 0.8713 - f1_metric: 0.87 - ETA: 0s - loss: 0.2514 - accuracy: 0.8702 - f1_metric: 0.87 - ETA: 0s - loss: 0.2495 - accuracy: 0.8726 - f1_metric: 0.87 - ETA: 0s - loss: 0.2468 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2511 - accuracy: 0.8720 - f1_metric: 0.87 - ETA: 0s - loss: 0.2537 - accuracy: 0.8736 - f1_metric: 0.87 - ETA: 0s - loss: 0.2535 - accuracy: 0.8742 - f1_metric: 0.87 - ETA: 0s - loss: 0.2546 - accuracy: 0.8730 - f1_metric: 0.87 - ETA: 0s - loss: 0.2553 - accuracy: 0.8721 - f1_metric: 0.8722\n",
      "Epoch 00107: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2564 - accuracy: 0.8710 - f1_metric: 0.8699 - val_loss: 0.4647 - val_accuracy: 0.8332 - val_f1_metric: 0.8325 - lr: 1.0000e-04\n",
      "Epoch 108/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3376 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.2728 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2632 - accuracy: 0.8482 - f1_metric: 0.84 - ETA: 0s - loss: 0.2596 - accuracy: 0.8576 - f1_metric: 0.85 - ETA: 0s - loss: 0.2729 - accuracy: 0.8490 - f1_metric: 0.84 - ETA: 0s - loss: 0.2670 - accuracy: 0.8510 - f1_metric: 0.85 - ETA: 0s - loss: 0.2638 - accuracy: 0.8559 - f1_metric: 0.85 - ETA: 0s - loss: 0.2577 - accuracy: 0.8638 - f1_metric: 0.86 - ETA: 0s - loss: 0.2508 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2502 - accuracy: 0.8762 - f1_metric: 0.87 - ETA: 0s - loss: 0.2534 - accuracy: 0.8734 - f1_metric: 0.87 - ETA: 0s - loss: 0.2587 - accuracy: 0.8764 - f1_metric: 0.87 - ETA: 0s - loss: 0.2602 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2595 - accuracy: 0.8794 - f1_metric: 0.87 - ETA: 0s - loss: 0.2600 - accuracy: 0.8806 - f1_metric: 0.8802\n",
      "Epoch 00108: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2610 - accuracy: 0.8807 - f1_metric: 0.8802 - val_loss: 0.4727 - val_accuracy: 0.8307 - val_f1_metric: 0.8307 - lr: 1.0000e-04\n",
      "Epoch 109/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2702 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2543 - accuracy: 0.8844 - f1_metric: 0.88 - ETA: 0s - loss: 0.2622 - accuracy: 0.8817 - f1_metric: 0.88 - ETA: 0s - loss: 0.2635 - accuracy: 0.8641 - f1_metric: 0.86 - ETA: 0s - loss: 0.2713 - accuracy: 0.8568 - f1_metric: 0.85 - ETA: 0s - loss: 0.2778 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2625 - accuracy: 0.8550 - f1_metric: 0.85 - ETA: 0s - loss: 0.2640 - accuracy: 0.8539 - f1_metric: 0.85 - ETA: 0s - loss: 0.2660 - accuracy: 0.8546 - f1_metric: 0.85 - ETA: 0s - loss: 0.2653 - accuracy: 0.8588 - f1_metric: 0.85 - ETA: 0s - loss: 0.2673 - accuracy: 0.8610 - f1_metric: 0.86 - ETA: 0s - loss: 0.2638 - accuracy: 0.8649 - f1_metric: 0.86 - ETA: 0s - loss: 0.2675 - accuracy: 0.8660 - f1_metric: 0.86 - ETA: 0s - loss: 0.2660 - accuracy: 0.8679 - f1_metric: 0.86 - ETA: 0s - loss: 0.2640 - accuracy: 0.8699 - f1_metric: 0.87 - ETA: 0s - loss: 0.2609 - accuracy: 0.8715 - f1_metric: 0.87 - ETA: 0s - loss: 0.2593 - accuracy: 0.8714 - f1_metric: 0.8718\n",
      "Epoch 00109: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2587 - accuracy: 0.8710 - f1_metric: 0.8711 - val_loss: 0.4698 - val_accuracy: 0.8307 - val_f1_metric: 0.8297 - lr: 1.0000e-04\n",
      "Epoch 110/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4443 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.3311 - accuracy: 0.8490 - f1_metric: 0.84 - ETA: 0s - loss: 0.2799 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2744 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2804 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2695 - accuracy: 0.8737 - f1_metric: 0.87 - ETA: 0s - loss: 0.2698 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2709 - accuracy: 0.8701 - f1_metric: 0.87 - ETA: 0s - loss: 0.2647 - accuracy: 0.8692 - f1_metric: 0.86 - ETA: 0s - loss: 0.2673 - accuracy: 0.8714 - f1_metric: 0.87 - ETA: 0s - loss: 0.2662 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2651 - accuracy: 0.8715 - f1_metric: 0.87 - ETA: 0s - loss: 0.2608 - accuracy: 0.8740 - f1_metric: 0.87 - ETA: 0s - loss: 0.2587 - accuracy: 0.8769 - f1_metric: 0.87 - ETA: 0s - loss: 0.2561 - accuracy: 0.8795 - f1_metric: 0.87 - ETA: 0s - loss: 0.2550 - accuracy: 0.8791 - f1_metric: 0.87 - ETA: 0s - loss: 0.2534 - accuracy: 0.8807 - f1_metric: 0.88 - ETA: 0s - loss: 0.2565 - accuracy: 0.8796 - f1_metric: 0.8797\n",
      "Epoch 00110: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.2565 - accuracy: 0.8796 - f1_metric: 0.8797 - val_loss: 0.4556 - val_accuracy: 0.8357 - val_f1_metric: 0.8352 - lr: 1.0000e-04\n",
      "Epoch 111/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2348 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2399 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2598 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2552 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2543 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2755 - accuracy: 0.8729 - f1_metric: 0.87 - ETA: 0s - loss: 0.2755 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2807 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2810 - accuracy: 0.8757 - f1_metric: 0.87 - ETA: 0s - loss: 0.2814 - accuracy: 0.8702 - f1_metric: 0.86 - ETA: 0s - loss: 0.2807 - accuracy: 0.8675 - f1_metric: 0.86 - ETA: 0s - loss: 0.2768 - accuracy: 0.8682 - f1_metric: 0.86 - ETA: 0s - loss: 0.2718 - accuracy: 0.8683 - f1_metric: 0.86 - ETA: 0s - loss: 0.2697 - accuracy: 0.8697 - f1_metric: 0.86 - ETA: 0s - loss: 0.2705 - accuracy: 0.8652 - f1_metric: 0.86 - ETA: 0s - loss: 0.2687 - accuracy: 0.8653 - f1_metric: 0.8652\n",
      "Epoch 00111: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2694 - accuracy: 0.8642 - f1_metric: 0.8641 - val_loss: 0.4643 - val_accuracy: 0.8315 - val_f1_metric: 0.8310 - lr: 1.0000e-04\n",
      "Epoch 112/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2039 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2305 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2464 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2743 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2707 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2678 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2594 - accuracy: 0.8770 - f1_metric: 0.87 - ETA: 0s - loss: 0.2567 - accuracy: 0.8766 - f1_metric: 0.87 - ETA: 0s - loss: 0.2523 - accuracy: 0.8793 - f1_metric: 0.87 - ETA: 0s - loss: 0.2595 - accuracy: 0.8776 - f1_metric: 0.87 - ETA: 0s - loss: 0.2666 - accuracy: 0.8814 - f1_metric: 0.88 - ETA: 0s - loss: 0.2628 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2609 - accuracy: 0.8821 - f1_metric: 0.88 - ETA: 0s - loss: 0.2587 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2585 - accuracy: 0.8822 - f1_metric: 0.88 - ETA: 0s - loss: 0.2613 - accuracy: 0.8821 - f1_metric: 0.8815\n",
      "Epoch 00112: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2625 - accuracy: 0.8818 - f1_metric: 0.8807 - val_loss: 0.4629 - val_accuracy: 0.8365 - val_f1_metric: 0.8366 - lr: 1.0000e-04\n",
      "Epoch 113/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2511 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2515 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2693 - accuracy: 0.8638 - f1_metric: 0.86 - ETA: 0s - loss: 0.2696 - accuracy: 0.8703 - f1_metric: 0.87 - ETA: 0s - loss: 0.2693 - accuracy: 0.8694 - f1_metric: 0.86 - ETA: 0s - loss: 0.2644 - accuracy: 0.8667 - f1_metric: 0.86 - ETA: 0s - loss: 0.2663 - accuracy: 0.8643 - f1_metric: 0.86 - ETA: 0s - loss: 0.2633 - accuracy: 0.8665 - f1_metric: 0.86 - ETA: 0s - loss: 0.2654 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2634 - accuracy: 0.8690 - f1_metric: 0.86 - ETA: 0s - loss: 0.2590 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2599 - accuracy: 0.8693 - f1_metric: 0.86 - ETA: 0s - loss: 0.2583 - accuracy: 0.8717 - f1_metric: 0.87 - ETA: 0s - loss: 0.2580 - accuracy: 0.8732 - f1_metric: 0.87 - ETA: 0s - loss: 0.2568 - accuracy: 0.8758 - f1_metric: 0.87 - ETA: 0s - loss: 0.2582 - accuracy: 0.8758 - f1_metric: 0.87 - ETA: 0s - loss: 0.2579 - accuracy: 0.8783 - f1_metric: 0.8781\n",
      "Epoch 00113: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2562 - accuracy: 0.8785 - f1_metric: 0.8778 - val_loss: 0.4533 - val_accuracy: 0.8391 - val_f1_metric: 0.8387 - lr: 1.0000e-04\n",
      "Epoch 114/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2801 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2278 - accuracy: 0.9010 - f1_metric: 0.90 - ETA: 0s - loss: 0.2315 - accuracy: 0.8938 - f1_metric: 0.89 - ETA: 0s - loss: 0.2358 - accuracy: 0.9023 - f1_metric: 0.90 - ETA: 0s - loss: 0.2351 - accuracy: 0.9000 - f1_metric: 0.90 - ETA: 0s - loss: 0.2353 - accuracy: 0.8945 - f1_metric: 0.89 - ETA: 0s - loss: 0.2347 - accuracy: 0.8951 - f1_metric: 0.89 - ETA: 0s - loss: 0.2465 - accuracy: 0.8925 - f1_metric: 0.89 - ETA: 0s - loss: 0.2459 - accuracy: 0.8898 - f1_metric: 0.88 - ETA: 0s - loss: 0.2431 - accuracy: 0.8891 - f1_metric: 0.88 - ETA: 0s - loss: 0.2423 - accuracy: 0.8859 - f1_metric: 0.88 - ETA: 0s - loss: 0.2410 - accuracy: 0.8863 - f1_metric: 0.88 - ETA: 0s - loss: 0.2403 - accuracy: 0.8860 - f1_metric: 0.88 - ETA: 0s - loss: 0.2433 - accuracy: 0.8879 - f1_metric: 0.88 - ETA: 0s - loss: 0.2482 - accuracy: 0.8877 - f1_metric: 0.88 - ETA: 0s - loss: 0.2457 - accuracy: 0.8888 - f1_metric: 0.88 - ETA: 0s - loss: 0.2442 - accuracy: 0.8872 - f1_metric: 0.88 - ETA: 0s - loss: 0.2472 - accuracy: 0.8858 - f1_metric: 0.88 - ETA: 0s - loss: 0.2512 - accuracy: 0.8858 - f1_metric: 0.8855\n",
      "Epoch 00114: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2535 - accuracy: 0.8825 - f1_metric: 0.8814 - val_loss: 0.4598 - val_accuracy: 0.8332 - val_f1_metric: 0.8326 - lr: 1.0000e-04\n",
      "Epoch 115/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2252 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2452 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2344 - accuracy: 0.8938 - f1_metric: 0.89 - ETA: 0s - loss: 0.2385 - accuracy: 0.8884 - f1_metric: 0.88 - ETA: 0s - loss: 0.2425 - accuracy: 0.8797 - f1_metric: 0.87 - ETA: 0s - loss: 0.2394 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2417 - accuracy: 0.8839 - f1_metric: 0.88 - ETA: 0s - loss: 0.2461 - accuracy: 0.8842 - f1_metric: 0.88 - ETA: 0s - loss: 0.2523 - accuracy: 0.8816 - f1_metric: 0.88 - ETA: 0s - loss: 0.2541 - accuracy: 0.8765 - f1_metric: 0.87 - ETA: 0s - loss: 0.2579 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2575 - accuracy: 0.8768 - f1_metric: 0.87 - ETA: 0s - loss: 0.2562 - accuracy: 0.8761 - f1_metric: 0.87 - ETA: 0s - loss: 0.2588 - accuracy: 0.8760 - f1_metric: 0.87 - ETA: 0s - loss: 0.2575 - accuracy: 0.8769 - f1_metric: 0.87 - ETA: 0s - loss: 0.2567 - accuracy: 0.8790 - f1_metric: 0.87 - ETA: 0s - loss: 0.2555 - accuracy: 0.8788 - f1_metric: 0.87 - ETA: 0s - loss: 0.2558 - accuracy: 0.8778 - f1_metric: 0.87 - ETA: 0s - loss: 0.2578 - accuracy: 0.8765 - f1_metric: 0.8761\n",
      "Epoch 00115: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2572 - accuracy: 0.8782 - f1_metric: 0.8785 - val_loss: 0.4598 - val_accuracy: 0.8307 - val_f1_metric: 0.8309 - lr: 1.0000e-04\n",
      "Epoch 116/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2614 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2555 - accuracy: 0.8504 - f1_metric: 0.85 - ETA: 0s - loss: 0.2423 - accuracy: 0.8734 - f1_metric: 0.87 - ETA: 0s - loss: 0.2432 - accuracy: 0.8654 - f1_metric: 0.86 - ETA: 0s - loss: 0.2439 - accuracy: 0.8760 - f1_metric: 0.87 - ETA: 0s - loss: 0.2477 - accuracy: 0.8758 - f1_metric: 0.87 - ETA: 0s - loss: 0.2511 - accuracy: 0.8729 - f1_metric: 0.87 - ETA: 0s - loss: 0.2482 - accuracy: 0.8730 - f1_metric: 0.87 - ETA: 0s - loss: 0.2531 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2507 - accuracy: 0.8774 - f1_metric: 0.87 - ETA: 0s - loss: 0.2504 - accuracy: 0.8777 - f1_metric: 0.87 - ETA: 0s - loss: 0.2513 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2524 - accuracy: 0.8803 - f1_metric: 0.88 - ETA: 0s - loss: 0.2551 - accuracy: 0.8771 - f1_metric: 0.8753\n",
      "Epoch 00116: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2551 - accuracy: 0.8771 - f1_metric: 0.8753 - val_loss: 0.4683 - val_accuracy: 0.8273 - val_f1_metric: 0.8272 - lr: 1.0000e-04\n",
      "Epoch 117/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2638 - accuracy: 0.8398 - f1_metric: 0.83 - ETA: 0s - loss: 0.2437 - accuracy: 0.8638 - f1_metric: 0.86 - ETA: 0s - loss: 0.2501 - accuracy: 0.8687 - f1_metric: 0.86 - ETA: 0s - loss: 0.2537 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2545 - accuracy: 0.8625 - f1_metric: 0.86 - ETA: 0s - loss: 0.2610 - accuracy: 0.8550 - f1_metric: 0.85 - ETA: 0s - loss: 0.2621 - accuracy: 0.8601 - f1_metric: 0.86 - ETA: 0s - loss: 0.2590 - accuracy: 0.8601 - f1_metric: 0.86 - ETA: 0s - loss: 0.2588 - accuracy: 0.8618 - f1_metric: 0.86 - ETA: 0s - loss: 0.2576 - accuracy: 0.8637 - f1_metric: 0.86 - ETA: 0s - loss: 0.2638 - accuracy: 0.8604 - f1_metric: 0.86 - ETA: 0s - loss: 0.2655 - accuracy: 0.8603 - f1_metric: 0.86 - ETA: 0s - loss: 0.2602 - accuracy: 0.8638 - f1_metric: 0.86 - ETA: 0s - loss: 0.2577 - accuracy: 0.8664 - f1_metric: 0.86 - ETA: 0s - loss: 0.2571 - accuracy: 0.8639 - f1_metric: 0.86 - ETA: 0s - loss: 0.2560 - accuracy: 0.8666 - f1_metric: 0.8672\n",
      "Epoch 00117: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2554 - accuracy: 0.8670 - f1_metric: 0.8680 - val_loss: 0.4578 - val_accuracy: 0.8340 - val_f1_metric: 0.8346 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2488 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3020 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.2931 - accuracy: 0.8371 - f1_metric: 0.83 - ETA: 0s - loss: 0.2716 - accuracy: 0.8609 - f1_metric: 0.86 - ETA: 0s - loss: 0.2647 - accuracy: 0.8678 - f1_metric: 0.86 - ETA: 0s - loss: 0.2570 - accuracy: 0.8721 - f1_metric: 0.87 - ETA: 0s - loss: 0.2568 - accuracy: 0.8717 - f1_metric: 0.87 - ETA: 0s - loss: 0.2633 - accuracy: 0.8689 - f1_metric: 0.86 - ETA: 0s - loss: 0.2603 - accuracy: 0.8712 - f1_metric: 0.87 - ETA: 0s - loss: 0.2613 - accuracy: 0.8721 - f1_metric: 0.87 - ETA: 0s - loss: 0.2598 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2609 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2584 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2569 - accuracy: 0.8714 - f1_metric: 0.87 - ETA: 0s - loss: 0.2557 - accuracy: 0.8702 - f1_metric: 0.8703\n",
      "Epoch 00118: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2566 - accuracy: 0.8710 - f1_metric: 0.8715 - val_loss: 0.4543 - val_accuracy: 0.8349 - val_f1_metric: 0.8345 - lr: 1.0000e-04\n",
      "Epoch 119/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3342 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2666 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2601 - accuracy: 0.8535 - f1_metric: 0.85 - ETA: 0s - loss: 0.2550 - accuracy: 0.8693 - f1_metric: 0.86 - ETA: 0s - loss: 0.2549 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2581 - accuracy: 0.8741 - f1_metric: 0.87 - ETA: 0s - loss: 0.2543 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2572 - accuracy: 0.8757 - f1_metric: 0.87 - ETA: 0s - loss: 0.2607 - accuracy: 0.8737 - f1_metric: 0.87 - ETA: 0s - loss: 0.2607 - accuracy: 0.8762 - f1_metric: 0.87 - ETA: 0s - loss: 0.2607 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2592 - accuracy: 0.8706 - f1_metric: 0.87 - ETA: 0s - loss: 0.2570 - accuracy: 0.8701 - f1_metric: 0.87 - ETA: 0s - loss: 0.2570 - accuracy: 0.8704 - f1_metric: 0.87 - ETA: 0s - loss: 0.2579 - accuracy: 0.8684 - f1_metric: 0.86 - ETA: 0s - loss: 0.2582 - accuracy: 0.8679 - f1_metric: 0.8682\n",
      "Epoch 00119: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2580 - accuracy: 0.8685 - f1_metric: 0.8684 - val_loss: 0.4647 - val_accuracy: 0.8332 - val_f1_metric: 0.8326 - lr: 1.0000e-04\n",
      "Epoch 120/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2282 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2419 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2364 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2189 - accuracy: 0.8848 - f1_metric: 0.88 - ETA: 0s - loss: 0.2217 - accuracy: 0.8859 - f1_metric: 0.88 - ETA: 0s - loss: 0.2296 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2379 - accuracy: 0.8862 - f1_metric: 0.88 - ETA: 0s - loss: 0.2414 - accuracy: 0.8842 - f1_metric: 0.88 - ETA: 0s - loss: 0.2417 - accuracy: 0.8820 - f1_metric: 0.88 - ETA: 0s - loss: 0.2410 - accuracy: 0.8818 - f1_metric: 0.88 - ETA: 0s - loss: 0.2430 - accuracy: 0.8786 - f1_metric: 0.87 - ETA: 0s - loss: 0.2443 - accuracy: 0.8778 - f1_metric: 0.87 - ETA: 0s - loss: 0.2475 - accuracy: 0.8790 - f1_metric: 0.87 - ETA: 0s - loss: 0.2478 - accuracy: 0.8764 - f1_metric: 0.87 - ETA: 0s - loss: 0.2489 - accuracy: 0.8768 - f1_metric: 0.87 - ETA: 0s - loss: 0.2468 - accuracy: 0.8788 - f1_metric: 0.87 - ETA: 0s - loss: 0.2493 - accuracy: 0.8770 - f1_metric: 0.87 - ETA: 0s - loss: 0.2498 - accuracy: 0.8773 - f1_metric: 0.87 - ETA: 0s - loss: 0.2505 - accuracy: 0.8754 - f1_metric: 0.8757\n",
      "Epoch 00120: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.2503 - accuracy: 0.8742 - f1_metric: 0.8734 - val_loss: 0.4481 - val_accuracy: 0.8399 - val_f1_metric: 0.8400 - lr: 1.0000e-04\n",
      "Epoch 121/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2875 - accuracy: 0.8750 - f1_metric: 0.85 - ETA: 0s - loss: 0.2203 - accuracy: 0.9102 - f1_metric: 0.90 - ETA: 0s - loss: 0.2387 - accuracy: 0.8951 - f1_metric: 0.89 - ETA: 0s - loss: 0.2509 - accuracy: 0.8941 - f1_metric: 0.89 - ETA: 0s - loss: 0.2394 - accuracy: 0.8997 - f1_metric: 0.89 - ETA: 0s - loss: 0.2372 - accuracy: 0.8990 - f1_metric: 0.89 - ETA: 0s - loss: 0.2357 - accuracy: 0.9026 - f1_metric: 0.90 - ETA: 0s - loss: 0.2333 - accuracy: 0.9023 - f1_metric: 0.90 - ETA: 0s - loss: 0.2416 - accuracy: 0.8995 - f1_metric: 0.89 - ETA: 0s - loss: 0.2433 - accuracy: 0.8994 - f1_metric: 0.89 - ETA: 0s - loss: 0.2399 - accuracy: 0.9001 - f1_metric: 0.89 - ETA: 0s - loss: 0.2375 - accuracy: 0.8984 - f1_metric: 0.89 - ETA: 0s - loss: 0.2397 - accuracy: 0.8950 - f1_metric: 0.89 - ETA: 0s - loss: 0.2439 - accuracy: 0.8929 - f1_metric: 0.89 - ETA: 0s - loss: 0.2446 - accuracy: 0.8919 - f1_metric: 0.89 - ETA: 0s - loss: 0.2452 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2448 - accuracy: 0.8903 - f1_metric: 0.8898\n",
      "Epoch 00121: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2462 - accuracy: 0.8904 - f1_metric: 0.8901 - val_loss: 0.4456 - val_accuracy: 0.8407 - val_f1_metric: 0.8408 - lr: 1.0000e-04\n",
      "Epoch 122/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.2985 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2483 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2323 - accuracy: 0.8776 - f1_metric: 0.87 - ETA: 0s - loss: 0.2528 - accuracy: 0.8767 - f1_metric: 0.87 - ETA: 0s - loss: 0.2483 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2502 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2476 - accuracy: 0.8879 - f1_metric: 0.88 - ETA: 0s - loss: 0.2465 - accuracy: 0.8808 - f1_metric: 0.88 - ETA: 0s - loss: 0.2441 - accuracy: 0.8793 - f1_metric: 0.87 - ETA: 0s - loss: 0.2443 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2402 - accuracy: 0.8808 - f1_metric: 0.88 - ETA: 0s - loss: 0.2477 - accuracy: 0.8755 - f1_metric: 0.87 - ETA: 0s - loss: 0.2459 - accuracy: 0.8821 - f1_metric: 0.88 - ETA: 0s - loss: 0.2455 - accuracy: 0.8824 - f1_metric: 0.88 - ETA: 0s - loss: 0.2466 - accuracy: 0.8830 - f1_metric: 0.88 - ETA: 0s - loss: 0.2492 - accuracy: 0.8807 - f1_metric: 0.8812\n",
      "Epoch 00122: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2493 - accuracy: 0.8803 - f1_metric: 0.8812 - val_loss: 0.4523 - val_accuracy: 0.8374 - val_f1_metric: 0.8372 - lr: 1.0000e-04\n",
      "Epoch 123/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2374 - accuracy: 0.8945 - f1_metric: 0.89 - ETA: 0s - loss: 0.2465 - accuracy: 0.8884 - f1_metric: 0.88 - ETA: 0s - loss: 0.2505 - accuracy: 0.9016 - f1_metric: 0.90 - ETA: 0s - loss: 0.2551 - accuracy: 0.8958 - f1_metric: 0.89 - ETA: 0s - loss: 0.2531 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2499 - accuracy: 0.8915 - f1_metric: 0.89 - ETA: 0s - loss: 0.2494 - accuracy: 0.8891 - f1_metric: 0.88 - ETA: 0s - loss: 0.2476 - accuracy: 0.8857 - f1_metric: 0.88 - ETA: 0s - loss: 0.2490 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2502 - accuracy: 0.8831 - f1_metric: 0.88 - ETA: 0s - loss: 0.2519 - accuracy: 0.8825 - f1_metric: 0.88 - ETA: 0s - loss: 0.2509 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2515 - accuracy: 0.8805 - f1_metric: 0.88 - ETA: 0s - loss: 0.2557 - accuracy: 0.8784 - f1_metric: 0.87 - ETA: 0s - loss: 0.2570 - accuracy: 0.8770 - f1_metric: 0.87 - ETA: 0s - loss: 0.2560 - accuracy: 0.8772 - f1_metric: 0.8771\n",
      "Epoch 00123: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2557 - accuracy: 0.8778 - f1_metric: 0.8785 - val_loss: 0.4522 - val_accuracy: 0.8374 - val_f1_metric: 0.8371 - lr: 1.0000e-04\n",
      "Epoch 124/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2337 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2739 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2897 - accuracy: 0.8568 - f1_metric: 0.85 - ETA: 0s - loss: 0.2819 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.2837 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2688 - accuracy: 0.8606 - f1_metric: 0.86 - ETA: 0s - loss: 0.2618 - accuracy: 0.8635 - f1_metric: 0.86 - ETA: 0s - loss: 0.2638 - accuracy: 0.8621 - f1_metric: 0.86 - ETA: 0s - loss: 0.2613 - accuracy: 0.8618 - f1_metric: 0.86 - ETA: 0s - loss: 0.2584 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2559 - accuracy: 0.8675 - f1_metric: 0.86 - ETA: 0s - loss: 0.2547 - accuracy: 0.8706 - f1_metric: 0.87 - ETA: 0s - loss: 0.2532 - accuracy: 0.8773 - f1_metric: 0.87 - ETA: 0s - loss: 0.2515 - accuracy: 0.8772 - f1_metric: 0.87 - ETA: 0s - loss: 0.2494 - accuracy: 0.8800 - f1_metric: 0.87 - ETA: 0s - loss: 0.2513 - accuracy: 0.8812 - f1_metric: 0.88 - ETA: 0s - loss: 0.2530 - accuracy: 0.8777 - f1_metric: 0.87 - ETA: 0s - loss: 0.2521 - accuracy: 0.8771 - f1_metric: 0.87 - ETA: 0s - loss: 0.2541 - accuracy: 0.8781 - f1_metric: 0.87 - ETA: 0s - loss: 0.2542 - accuracy: 0.8780 - f1_metric: 0.8777\n",
      "Epoch 00124: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.2540 - accuracy: 0.8768 - f1_metric: 0.8776 - val_loss: 0.4606 - val_accuracy: 0.8349 - val_f1_metric: 0.8349 - lr: 1.0000e-04\n",
      "Epoch 125/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2743 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.3112 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.2862 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2822 - accuracy: 0.8535 - f1_metric: 0.85 - ETA: 0s - loss: 0.2699 - accuracy: 0.8641 - f1_metric: 0.86 - ETA: 0s - loss: 0.2672 - accuracy: 0.8620 - f1_metric: 0.86 - ETA: 0s - loss: 0.2738 - accuracy: 0.8625 - f1_metric: 0.86 - ETA: 0s - loss: 0.2703 - accuracy: 0.8603 - f1_metric: 0.85 - ETA: 0s - loss: 0.2704 - accuracy: 0.8602 - f1_metric: 0.85 - ETA: 0s - loss: 0.2734 - accuracy: 0.8648 - f1_metric: 0.86 - ETA: 0s - loss: 0.2678 - accuracy: 0.8700 - f1_metric: 0.86 - ETA: 0s - loss: 0.2681 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2671 - accuracy: 0.8745 - f1_metric: 0.87 - ETA: 0s - loss: 0.2659 - accuracy: 0.8745 - f1_metric: 0.87 - ETA: 0s - loss: 0.2641 - accuracy: 0.8759 - f1_metric: 0.87 - ETA: 0s - loss: 0.2645 - accuracy: 0.8737 - f1_metric: 0.87 - ETA: 0s - loss: 0.2642 - accuracy: 0.8734 - f1_metric: 0.87 - ETA: 0s - loss: 0.2641 - accuracy: 0.8738 - f1_metric: 0.87 - ETA: 0s - loss: 0.2655 - accuracy: 0.8754 - f1_metric: 0.8748\n",
      "Epoch 00125: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2635 - accuracy: 0.8764 - f1_metric: 0.8761 - val_loss: 0.4690 - val_accuracy: 0.8273 - val_f1_metric: 0.8272 - lr: 1.0000e-04\n",
      "Epoch 126/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.1940 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2091 - accuracy: 0.8984 - f1_metric: 0.89 - ETA: 0s - loss: 0.2265 - accuracy: 0.8951 - f1_metric: 0.89 - ETA: 0s - loss: 0.2399 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2475 - accuracy: 0.8642 - f1_metric: 0.86 - ETA: 0s - loss: 0.2540 - accuracy: 0.8722 - f1_metric: 0.87 - ETA: 0s - loss: 0.2546 - accuracy: 0.8703 - f1_metric: 0.87 - ETA: 0s - loss: 0.2536 - accuracy: 0.8668 - f1_metric: 0.86 - ETA: 0s - loss: 0.2528 - accuracy: 0.8666 - f1_metric: 0.86 - ETA: 0s - loss: 0.2510 - accuracy: 0.8685 - f1_metric: 0.86 - ETA: 0s - loss: 0.2477 - accuracy: 0.8706 - f1_metric: 0.87 - ETA: 0s - loss: 0.2492 - accuracy: 0.8732 - f1_metric: 0.87 - ETA: 0s - loss: 0.2512 - accuracy: 0.8725 - f1_metric: 0.87 - ETA: 0s - loss: 0.2559 - accuracy: 0.8715 - f1_metric: 0.87 - ETA: 0s - loss: 0.2552 - accuracy: 0.8721 - f1_metric: 0.8724\n",
      "Epoch 00126: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2551 - accuracy: 0.8721 - f1_metric: 0.8724 - val_loss: 0.4552 - val_accuracy: 0.8391 - val_f1_metric: 0.8389 - lr: 1.0000e-04\n",
      "Epoch 127/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2494 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2528 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2523 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2359 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2468 - accuracy: 0.8893 - f1_metric: 0.88 - ETA: 0s - loss: 0.2496 - accuracy: 0.8885 - f1_metric: 0.88 - ETA: 0s - loss: 0.2583 - accuracy: 0.8776 - f1_metric: 0.87 - ETA: 0s - loss: 0.2600 - accuracy: 0.8742 - f1_metric: 0.87 - ETA: 0s - loss: 0.2544 - accuracy: 0.8723 - f1_metric: 0.87 - ETA: 0s - loss: 0.2536 - accuracy: 0.8738 - f1_metric: 0.87 - ETA: 0s - loss: 0.2517 - accuracy: 0.8745 - f1_metric: 0.87 - ETA: 0s - loss: 0.2507 - accuracy: 0.8726 - f1_metric: 0.87 - ETA: 0s - loss: 0.2468 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2489 - accuracy: 0.8720 - f1_metric: 0.87 - ETA: 0s - loss: 0.2512 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2500 - accuracy: 0.8698 - f1_metric: 0.8699\n",
      "Epoch 00127: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2502 - accuracy: 0.8703 - f1_metric: 0.8708 - val_loss: 0.4360 - val_accuracy: 0.8433 - val_f1_metric: 0.8430 - lr: 1.0000e-04\n",
      "Epoch 128/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2239 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2279 - accuracy: 0.8839 - f1_metric: 0.88 - ETA: 0s - loss: 0.2184 - accuracy: 0.8941 - f1_metric: 0.89 - ETA: 0s - loss: 0.2245 - accuracy: 0.8942 - f1_metric: 0.89 - ETA: 0s - loss: 0.2273 - accuracy: 0.8943 - f1_metric: 0.89 - ETA: 0s - loss: 0.2337 - accuracy: 0.8977 - f1_metric: 0.89 - ETA: 0s - loss: 0.2327 - accuracy: 0.9008 - f1_metric: 0.90 - ETA: 0s - loss: 0.2420 - accuracy: 0.9028 - f1_metric: 0.90 - ETA: 0s - loss: 0.2449 - accuracy: 0.9005 - f1_metric: 0.90 - ETA: 0s - loss: 0.2467 - accuracy: 0.9009 - f1_metric: 0.90 - ETA: 0s - loss: 0.2438 - accuracy: 0.9035 - f1_metric: 0.90 - ETA: 0s - loss: 0.2435 - accuracy: 0.9024 - f1_metric: 0.90 - ETA: 0s - loss: 0.2426 - accuracy: 0.9000 - f1_metric: 0.89 - ETA: 0s - loss: 0.2430 - accuracy: 0.8992 - f1_metric: 0.8988\n",
      "Epoch 00128: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2412 - accuracy: 0.8990 - f1_metric: 0.8983 - val_loss: 0.4671 - val_accuracy: 0.8298 - val_f1_metric: 0.8279 - lr: 1.0000e-04\n",
      "Epoch 129/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2000 - accuracy: 0.9375 - f1_metric: 0.93 - ETA: 0s - loss: 0.2537 - accuracy: 0.8555 - f1_metric: 0.85 - ETA: 0s - loss: 0.2551 - accuracy: 0.8683 - f1_metric: 0.86 - ETA: 0s - loss: 0.2474 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2509 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.2496 - accuracy: 0.8729 - f1_metric: 0.87 - ETA: 0s - loss: 0.2546 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2573 - accuracy: 0.8722 - f1_metric: 0.87 - ETA: 0s - loss: 0.2541 - accuracy: 0.8762 - f1_metric: 0.87 - ETA: 0s - loss: 0.2518 - accuracy: 0.8778 - f1_metric: 0.87 - ETA: 0s - loss: 0.2510 - accuracy: 0.8790 - f1_metric: 0.87 - ETA: 0s - loss: 0.2483 - accuracy: 0.8814 - f1_metric: 0.88 - ETA: 0s - loss: 0.2501 - accuracy: 0.8834 - f1_metric: 0.88 - ETA: 0s - loss: 0.2512 - accuracy: 0.8838 - f1_metric: 0.88 - ETA: 0s - loss: 0.2498 - accuracy: 0.8832 - f1_metric: 0.8830\n",
      "Epoch 00129: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2506 - accuracy: 0.8825 - f1_metric: 0.8818 - val_loss: 0.4577 - val_accuracy: 0.8374 - val_f1_metric: 0.8372 - lr: 1.0000e-04\n",
      "Epoch 130/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.9375 - f1_metric: 0.93 - ETA: 0s - loss: 0.2367 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2583 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2564 - accuracy: 0.8797 - f1_metric: 0.87 - ETA: 0s - loss: 0.2501 - accuracy: 0.8817 - f1_metric: 0.88 - ETA: 0s - loss: 0.2566 - accuracy: 0.8814 - f1_metric: 0.88 - ETA: 0s - loss: 0.2526 - accuracy: 0.8875 - f1_metric: 0.88 - ETA: 0s - loss: 0.2495 - accuracy: 0.8893 - f1_metric: 0.88 - ETA: 0s - loss: 0.2507 - accuracy: 0.8822 - f1_metric: 0.88 - ETA: 0s - loss: 0.2492 - accuracy: 0.8798 - f1_metric: 0.88 - ETA: 0s - loss: 0.2529 - accuracy: 0.8706 - f1_metric: 0.87 - ETA: 0s - loss: 0.2527 - accuracy: 0.8737 - f1_metric: 0.87 - ETA: 0s - loss: 0.2543 - accuracy: 0.8746 - f1_metric: 0.87 - ETA: 0s - loss: 0.2539 - accuracy: 0.8727 - f1_metric: 0.87 - ETA: 0s - loss: 0.2529 - accuracy: 0.8760 - f1_metric: 0.8770\n",
      "Epoch 00130: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2529 - accuracy: 0.8760 - f1_metric: 0.8770 - val_loss: 0.4385 - val_accuracy: 0.8483 - val_f1_metric: 0.8491 - lr: 1.0000e-04\n",
      "Epoch 131/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2358 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2395 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2295 - accuracy: 0.8973 - f1_metric: 0.89 - ETA: 0s - loss: 0.2442 - accuracy: 0.8906 - f1_metric: 0.88 - ETA: 0s - loss: 0.2381 - accuracy: 0.8951 - f1_metric: 0.89 - ETA: 0s - loss: 0.2377 - accuracy: 0.8943 - f1_metric: 0.89 - ETA: 0s - loss: 0.2432 - accuracy: 0.8945 - f1_metric: 0.89 - ETA: 0s - loss: 0.2447 - accuracy: 0.8885 - f1_metric: 0.88 - ETA: 0s - loss: 0.2501 - accuracy: 0.8844 - f1_metric: 0.88 - ETA: 0s - loss: 0.2492 - accuracy: 0.8823 - f1_metric: 0.88 - ETA: 0s - loss: 0.2491 - accuracy: 0.8823 - f1_metric: 0.88 - ETA: 0s - loss: 0.2476 - accuracy: 0.8807 - f1_metric: 0.88 - ETA: 0s - loss: 0.2500 - accuracy: 0.8802 - f1_metric: 0.87 - ETA: 0s - loss: 0.2541 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2550 - accuracy: 0.8802 - f1_metric: 0.8798\n",
      "Epoch 00131: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2553 - accuracy: 0.8811 - f1_metric: 0.8813 - val_loss: 0.4629 - val_accuracy: 0.8349 - val_f1_metric: 0.8351 - lr: 1.0000e-04\n",
      "Epoch 132/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2403 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2498 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2496 - accuracy: 0.8703 - f1_metric: 0.87 - ETA: 0s - loss: 0.2726 - accuracy: 0.8762 - f1_metric: 0.87 - ETA: 0s - loss: 0.2649 - accuracy: 0.8809 - f1_metric: 0.88 - ETA: 0s - loss: 0.2562 - accuracy: 0.8840 - f1_metric: 0.88 - ETA: 0s - loss: 0.2570 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2623 - accuracy: 0.8763 - f1_metric: 0.87 - ETA: 0s - loss: 0.2609 - accuracy: 0.8780 - f1_metric: 0.87 - ETA: 0s - loss: 0.2594 - accuracy: 0.8745 - f1_metric: 0.87 - ETA: 0s - loss: 0.2589 - accuracy: 0.8774 - f1_metric: 0.87 - ETA: 0s - loss: 0.2591 - accuracy: 0.8782 - f1_metric: 0.87 - ETA: 0s - loss: 0.2606 - accuracy: 0.8792 - f1_metric: 0.87 - ETA: 0s - loss: 0.2578 - accuracy: 0.8794 - f1_metric: 0.87 - ETA: 0s - loss: 0.2563 - accuracy: 0.8792 - f1_metric: 0.87 - ETA: 0s - loss: 0.2547 - accuracy: 0.8789 - f1_metric: 0.8787\n",
      "Epoch 00132: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2547 - accuracy: 0.8789 - f1_metric: 0.8787 - val_loss: 0.4502 - val_accuracy: 0.8382 - val_f1_metric: 0.8381 - lr: 1.0000e-04\n",
      "Epoch 133/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3020 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2695 - accuracy: 0.9010 - f1_metric: 0.90 - ETA: 0s - loss: 0.2561 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2517 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2513 - accuracy: 0.8734 - f1_metric: 0.87 - ETA: 0s - loss: 0.2519 - accuracy: 0.8666 - f1_metric: 0.86 - ETA: 0s - loss: 0.2511 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2497 - accuracy: 0.8741 - f1_metric: 0.87 - ETA: 0s - loss: 0.2469 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2451 - accuracy: 0.8810 - f1_metric: 0.88 - ETA: 0s - loss: 0.2422 - accuracy: 0.8802 - f1_metric: 0.87 - ETA: 0s - loss: 0.2438 - accuracy: 0.8831 - f1_metric: 0.88 - ETA: 0s - loss: 0.2398 - accuracy: 0.8833 - f1_metric: 0.88 - ETA: 0s - loss: 0.2414 - accuracy: 0.8830 - f1_metric: 0.88 - ETA: 0s - loss: 0.2442 - accuracy: 0.8824 - f1_metric: 0.88 - ETA: 0s - loss: 0.2492 - accuracy: 0.8830 - f1_metric: 0.88 - ETA: 0s - loss: 0.2498 - accuracy: 0.8832 - f1_metric: 0.8829\n",
      "Epoch 00133: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2482 - accuracy: 0.8843 - f1_metric: 0.8846 - val_loss: 0.4517 - val_accuracy: 0.8424 - val_f1_metric: 0.8420 - lr: 1.0000e-04\n",
      "Epoch 134/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2234 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2549 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2727 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2635 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2652 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2645 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2593 - accuracy: 0.8713 - f1_metric: 0.87 - ETA: 0s - loss: 0.2622 - accuracy: 0.8680 - f1_metric: 0.86 - ETA: 0s - loss: 0.2608 - accuracy: 0.8662 - f1_metric: 0.86 - ETA: 0s - loss: 0.2614 - accuracy: 0.8660 - f1_metric: 0.86 - ETA: 0s - loss: 0.2612 - accuracy: 0.8664 - f1_metric: 0.86 - ETA: 0s - loss: 0.2608 - accuracy: 0.8687 - f1_metric: 0.86 - ETA: 0s - loss: 0.2580 - accuracy: 0.8714 - f1_metric: 0.87 - ETA: 0s - loss: 0.2565 - accuracy: 0.8688 - f1_metric: 0.86 - ETA: 0s - loss: 0.2545 - accuracy: 0.8691 - f1_metric: 0.86 - ETA: 0s - loss: 0.2534 - accuracy: 0.8706 - f1_metric: 0.8704\n",
      "Epoch 00134: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2554 - accuracy: 0.8710 - f1_metric: 0.8712 - val_loss: 0.4523 - val_accuracy: 0.8407 - val_f1_metric: 0.8405 - lr: 1.0000e-04\n",
      "Epoch 135/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2243 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2431 - accuracy: 0.8884 - f1_metric: 0.88 - ETA: 0s - loss: 0.2519 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2504 - accuracy: 0.8802 - f1_metric: 0.87 - ETA: 0s - loss: 0.2493 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2534 - accuracy: 0.8759 - f1_metric: 0.87 - ETA: 0s - loss: 0.2481 - accuracy: 0.8713 - f1_metric: 0.87 - ETA: 0s - loss: 0.2498 - accuracy: 0.8689 - f1_metric: 0.86 - ETA: 0s - loss: 0.2502 - accuracy: 0.8706 - f1_metric: 0.87 - ETA: 0s - loss: 0.2530 - accuracy: 0.8704 - f1_metric: 0.87 - ETA: 0s - loss: 0.2509 - accuracy: 0.8740 - f1_metric: 0.87 - ETA: 0s - loss: 0.2477 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2482 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2498 - accuracy: 0.8818 - f1_metric: 0.88 - ETA: 0s - loss: 0.2479 - accuracy: 0.8819 - f1_metric: 0.88 - ETA: 0s - loss: 0.2468 - accuracy: 0.8836 - f1_metric: 0.8843\n",
      "Epoch 00135: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2468 - accuracy: 0.8836 - f1_metric: 0.8843 - val_loss: 0.4501 - val_accuracy: 0.8407 - val_f1_metric: 0.8400 - lr: 1.0000e-04\n",
      "Epoch 136/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2514 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2439 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2425 - accuracy: 0.8719 - f1_metric: 0.87 - ETA: 0s - loss: 0.2330 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2432 - accuracy: 0.8767 - f1_metric: 0.87 - ETA: 0s - loss: 0.2356 - accuracy: 0.8864 - f1_metric: 0.88 - ETA: 0s - loss: 0.2434 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2403 - accuracy: 0.8779 - f1_metric: 0.87 - ETA: 0s - loss: 0.2405 - accuracy: 0.8791 - f1_metric: 0.87 - ETA: 0s - loss: 0.2400 - accuracy: 0.8786 - f1_metric: 0.87 - ETA: 0s - loss: 0.2432 - accuracy: 0.8809 - f1_metric: 0.88 - ETA: 0s - loss: 0.2497 - accuracy: 0.8796 - f1_metric: 0.87 - ETA: 0s - loss: 0.2487 - accuracy: 0.8839 - f1_metric: 0.88 - ETA: 0s - loss: 0.2455 - accuracy: 0.8864 - f1_metric: 0.88 - ETA: 0s - loss: 0.2432 - accuracy: 0.8880 - f1_metric: 0.88 - ETA: 0s - loss: 0.2435 - accuracy: 0.8902 - f1_metric: 0.89 - ETA: 0s - loss: 0.2444 - accuracy: 0.8895 - f1_metric: 0.88 - ETA: 0s - loss: 0.2462 - accuracy: 0.8888 - f1_metric: 0.8889\n",
      "Epoch 00136: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2465 - accuracy: 0.8886 - f1_metric: 0.8885 - val_loss: 0.4598 - val_accuracy: 0.8332 - val_f1_metric: 0.8331 - lr: 1.0000e-04\n",
      "Epoch 137/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2606 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2571 - accuracy: 0.8661 - f1_metric: 0.86 - ETA: 0s - loss: 0.2598 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2506 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2654 - accuracy: 0.8650 - f1_metric: 0.86 - ETA: 0s - loss: 0.2646 - accuracy: 0.8631 - f1_metric: 0.86 - ETA: 0s - loss: 0.2603 - accuracy: 0.8680 - f1_metric: 0.86 - ETA: 0s - loss: 0.2565 - accuracy: 0.8716 - f1_metric: 0.87 - ETA: 0s - loss: 0.2559 - accuracy: 0.8712 - f1_metric: 0.87 - ETA: 0s - loss: 0.2563 - accuracy: 0.8717 - f1_metric: 0.87 - ETA: 0s - loss: 0.2584 - accuracy: 0.8729 - f1_metric: 0.87 - ETA: 0s - loss: 0.2542 - accuracy: 0.8770 - f1_metric: 0.87 - ETA: 0s - loss: 0.2566 - accuracy: 0.8768 - f1_metric: 0.87 - ETA: 0s - loss: 0.2541 - accuracy: 0.8762 - f1_metric: 0.87 - ETA: 0s - loss: 0.2527 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2501 - accuracy: 0.8779 - f1_metric: 0.8781\n",
      "Epoch 00137: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2501 - accuracy: 0.8782 - f1_metric: 0.8786 - val_loss: 0.4493 - val_accuracy: 0.8391 - val_f1_metric: 0.8389 - lr: 1.0000e-04\n",
      "Epoch 138/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2415 - accuracy: 0.9180 - f1_metric: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.9141 - f1_metric: 0.91 - ETA: 0s - loss: 0.2415 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2389 - accuracy: 0.9089 - f1_metric: 0.90 - ETA: 0s - loss: 0.2345 - accuracy: 0.9040 - f1_metric: 0.90 - ETA: 0s - loss: 0.2438 - accuracy: 0.9004 - f1_metric: 0.90 - ETA: 0s - loss: 0.2477 - accuracy: 0.8964 - f1_metric: 0.89 - ETA: 0s - loss: 0.2570 - accuracy: 0.8935 - f1_metric: 0.89 - ETA: 0s - loss: 0.2558 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2562 - accuracy: 0.8934 - f1_metric: 0.89 - ETA: 0s - loss: 0.2566 - accuracy: 0.8876 - f1_metric: 0.88 - ETA: 0s - loss: 0.2532 - accuracy: 0.8862 - f1_metric: 0.88 - ETA: 0s - loss: 0.2524 - accuracy: 0.8824 - f1_metric: 0.88 - ETA: 0s - loss: 0.2554 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2550 - accuracy: 0.8815 - f1_metric: 0.8817\n",
      "Epoch 00138: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2545 - accuracy: 0.8811 - f1_metric: 0.8807 - val_loss: 0.4512 - val_accuracy: 0.8365 - val_f1_metric: 0.8359 - lr: 1.0000e-04\n",
      "Epoch 139/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4511 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2865 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2908 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2824 - accuracy: 0.8766 - f1_metric: 0.87 - ETA: 0s - loss: 0.2690 - accuracy: 0.8678 - f1_metric: 0.86 - ETA: 0s - loss: 0.2643 - accuracy: 0.8711 - f1_metric: 0.87 - ETA: 0s - loss: 0.2650 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2621 - accuracy: 0.8784 - f1_metric: 0.87 - ETA: 0s - loss: 0.2605 - accuracy: 0.8769 - f1_metric: 0.87 - ETA: 0s - loss: 0.2563 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2573 - accuracy: 0.8766 - f1_metric: 0.87 - ETA: 0s - loss: 0.2581 - accuracy: 0.8726 - f1_metric: 0.87 - ETA: 0s - loss: 0.2618 - accuracy: 0.8728 - f1_metric: 0.87 - ETA: 0s - loss: 0.2601 - accuracy: 0.8754 - f1_metric: 0.87 - ETA: 0s - loss: 0.2582 - accuracy: 0.8742 - f1_metric: 0.87 - ETA: 0s - loss: 0.2572 - accuracy: 0.8732 - f1_metric: 0.8722\n",
      "Epoch 00139: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2572 - accuracy: 0.8732 - f1_metric: 0.8722 - val_loss: 0.4540 - val_accuracy: 0.8357 - val_f1_metric: 0.8351 - lr: 1.0000e-04\n",
      "Epoch 140/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2755 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2601 - accuracy: 0.8616 - f1_metric: 0.86 - ETA: 0s - loss: 0.2756 - accuracy: 0.8663 - f1_metric: 0.86 - ETA: 0s - loss: 0.2640 - accuracy: 0.8778 - f1_metric: 0.87 - ETA: 0s - loss: 0.2596 - accuracy: 0.8822 - f1_metric: 0.88 - ETA: 0s - loss: 0.2629 - accuracy: 0.8771 - f1_metric: 0.87 - ETA: 0s - loss: 0.2575 - accuracy: 0.8811 - f1_metric: 0.88 - ETA: 0s - loss: 0.2542 - accuracy: 0.8810 - f1_metric: 0.88 - ETA: 0s - loss: 0.2544 - accuracy: 0.8838 - f1_metric: 0.88 - ETA: 0s - loss: 0.2530 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2530 - accuracy: 0.8844 - f1_metric: 0.88 - ETA: 0s - loss: 0.2505 - accuracy: 0.8848 - f1_metric: 0.88 - ETA: 0s - loss: 0.2493 - accuracy: 0.8865 - f1_metric: 0.88 - ETA: 0s - loss: 0.2493 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2511 - accuracy: 0.8865 - f1_metric: 0.88 - ETA: 0s - loss: 0.2502 - accuracy: 0.8880 - f1_metric: 0.88 - ETA: 0s - loss: 0.2481 - accuracy: 0.8892 - f1_metric: 0.8891\n",
      "Epoch 00140: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.2507 - accuracy: 0.8886 - f1_metric: 0.8880 - val_loss: 0.4533 - val_accuracy: 0.8365 - val_f1_metric: 0.8367 - lr: 1.0000e-04\n",
      "Epoch 141/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2114 - accuracy: 0.9010 - f1_metric: 0.90 - ETA: 0s - loss: 0.2379 - accuracy: 0.8984 - f1_metric: 0.89 - ETA: 0s - loss: 0.2550 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2643 - accuracy: 0.8685 - f1_metric: 0.86 - ETA: 0s - loss: 0.2548 - accuracy: 0.8677 - f1_metric: 0.86 - ETA: 0s - loss: 0.2545 - accuracy: 0.8635 - f1_metric: 0.86 - ETA: 0s - loss: 0.2554 - accuracy: 0.8631 - f1_metric: 0.86 - ETA: 0s - loss: 0.2518 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2501 - accuracy: 0.8628 - f1_metric: 0.86 - ETA: 0s - loss: 0.2504 - accuracy: 0.8626 - f1_metric: 0.86 - ETA: 0s - loss: 0.2493 - accuracy: 0.8657 - f1_metric: 0.86 - ETA: 0s - loss: 0.2542 - accuracy: 0.8670 - f1_metric: 0.86 - ETA: 0s - loss: 0.2505 - accuracy: 0.8713 - f1_metric: 0.87 - ETA: 0s - loss: 0.2474 - accuracy: 0.8731 - f1_metric: 0.87 - ETA: 0s - loss: 0.2494 - accuracy: 0.8728 - f1_metric: 0.8726\n",
      "Epoch 00141: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2494 - accuracy: 0.8728 - f1_metric: 0.8726 - val_loss: 0.4363 - val_accuracy: 0.8433 - val_f1_metric: 0.8437 - lr: 1.0000e-04\n",
      "Epoch 142/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9375 - f1_metric: 0.93 - ETA: 0s - loss: 0.2106 - accuracy: 0.8945 - f1_metric: 0.89 - ETA: 0s - loss: 0.2202 - accuracy: 0.9040 - f1_metric: 0.90 - ETA: 0s - loss: 0.2308 - accuracy: 0.8969 - f1_metric: 0.89 - ETA: 0s - loss: 0.2405 - accuracy: 0.8918 - f1_metric: 0.89 - ETA: 0s - loss: 0.2303 - accuracy: 0.8989 - f1_metric: 0.89 - ETA: 0s - loss: 0.2281 - accuracy: 0.9000 - f1_metric: 0.90 - ETA: 0s - loss: 0.2326 - accuracy: 0.8940 - f1_metric: 0.89 - ETA: 0s - loss: 0.2373 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2355 - accuracy: 0.8884 - f1_metric: 0.88 - ETA: 0s - loss: 0.2409 - accuracy: 0.8911 - f1_metric: 0.89 - ETA: 0s - loss: 0.2436 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2470 - accuracy: 0.8898 - f1_metric: 0.89 - ETA: 0s - loss: 0.2466 - accuracy: 0.8895 - f1_metric: 0.88 - ETA: 0s - loss: 0.2478 - accuracy: 0.8877 - f1_metric: 0.8879\n",
      "Epoch 00142: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2477 - accuracy: 0.8875 - f1_metric: 0.8875 - val_loss: 0.4672 - val_accuracy: 0.8307 - val_f1_metric: 0.8305 - lr: 1.0000e-04\n",
      "Epoch 143/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2245 - accuracy: 0.9375 - f1_metric: 0.93 - ETA: 0s - loss: 0.2376 - accuracy: 0.9010 - f1_metric: 0.90 - ETA: 0s - loss: 0.2710 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2691 - accuracy: 0.8524 - f1_metric: 0.85 - ETA: 0s - loss: 0.2658 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.2706 - accuracy: 0.8504 - f1_metric: 0.85 - ETA: 0s - loss: 0.2646 - accuracy: 0.8559 - f1_metric: 0.85 - ETA: 0s - loss: 0.2604 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2550 - accuracy: 0.8662 - f1_metric: 0.86 - ETA: 0s - loss: 0.2578 - accuracy: 0.8681 - f1_metric: 0.86 - ETA: 0s - loss: 0.2558 - accuracy: 0.8733 - f1_metric: 0.87 - ETA: 0s - loss: 0.2543 - accuracy: 0.8780 - f1_metric: 0.87 - ETA: 0s - loss: 0.2537 - accuracy: 0.8796 - f1_metric: 0.87 - ETA: 0s - loss: 0.2525 - accuracy: 0.8801 - f1_metric: 0.87 - ETA: 0s - loss: 0.2539 - accuracy: 0.8793 - f1_metric: 0.87 - ETA: 0s - loss: 0.2519 - accuracy: 0.8791 - f1_metric: 0.8789\n",
      "Epoch 00143: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2501 - accuracy: 0.8785 - f1_metric: 0.8783 - val_loss: 0.4482 - val_accuracy: 0.8466 - val_f1_metric: 0.8458 - lr: 1.0000e-04\n",
      "Epoch 144/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.9375 - f1_metric: 0.92 - ETA: 0s - loss: 0.2508 - accuracy: 0.9062 - f1_metric: 0.90 - ETA: 0s - loss: 0.2529 - accuracy: 0.8932 - f1_metric: 0.89 - ETA: 0s - loss: 0.2403 - accuracy: 0.9010 - f1_metric: 0.89 - ETA: 0s - loss: 0.2372 - accuracy: 0.8945 - f1_metric: 0.89 - ETA: 0s - loss: 0.2353 - accuracy: 0.8929 - f1_metric: 0.89 - ETA: 0s - loss: 0.2436 - accuracy: 0.8838 - f1_metric: 0.88 - ETA: 0s - loss: 0.2426 - accuracy: 0.8889 - f1_metric: 0.88 - ETA: 0s - loss: 0.2447 - accuracy: 0.8884 - f1_metric: 0.88 - ETA: 0s - loss: 0.2428 - accuracy: 0.8913 - f1_metric: 0.89 - ETA: 0s - loss: 0.2406 - accuracy: 0.8900 - f1_metric: 0.88 - ETA: 0s - loss: 0.2402 - accuracy: 0.8917 - f1_metric: 0.89 - ETA: 0s - loss: 0.2434 - accuracy: 0.8896 - f1_metric: 0.88 - ETA: 0s - loss: 0.2450 - accuracy: 0.8897 - f1_metric: 0.88 - ETA: 0s - loss: 0.2481 - accuracy: 0.8880 - f1_metric: 0.88 - ETA: 0s - loss: 0.2477 - accuracy: 0.8858 - f1_metric: 0.88 - ETA: 0s - loss: 0.2500 - accuracy: 0.8861 - f1_metric: 0.88 - ETA: 0s - loss: 0.2509 - accuracy: 0.8830 - f1_metric: 0.8827\n",
      "Epoch 00144: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.2510 - accuracy: 0.8836 - f1_metric: 0.8839 - val_loss: 0.4674 - val_accuracy: 0.8324 - val_f1_metric: 0.8318 - lr: 1.0000e-04\n",
      "Epoch 145/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2545 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2412 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2418 - accuracy: 0.8571 - f1_metric: 0.85 - ETA: 0s - loss: 0.2493 - accuracy: 0.8656 - f1_metric: 0.86 - ETA: 0s - loss: 0.2515 - accuracy: 0.8654 - f1_metric: 0.86 - ETA: 0s - loss: 0.2499 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2462 - accuracy: 0.8724 - f1_metric: 0.87 - ETA: 0s - loss: 0.2451 - accuracy: 0.8766 - f1_metric: 0.87 - ETA: 0s - loss: 0.2445 - accuracy: 0.8777 - f1_metric: 0.87 - ETA: 0s - loss: 0.2441 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2455 - accuracy: 0.8761 - f1_metric: 0.87 - ETA: 0s - loss: 0.2483 - accuracy: 0.8807 - f1_metric: 0.88 - ETA: 0s - loss: 0.2482 - accuracy: 0.8819 - f1_metric: 0.88 - ETA: 0s - loss: 0.2471 - accuracy: 0.8844 - f1_metric: 0.88 - ETA: 0s - loss: 0.2484 - accuracy: 0.8844 - f1_metric: 0.8846\n",
      "Epoch 00145: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2473 - accuracy: 0.8850 - f1_metric: 0.8857 - val_loss: 0.4592 - val_accuracy: 0.8382 - val_f1_metric: 0.8381 - lr: 1.0000e-04\n",
      "Epoch 146/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2392 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2490 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2504 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2418 - accuracy: 0.8815 - f1_metric: 0.88 - ETA: 0s - loss: 0.2431 - accuracy: 0.8795 - f1_metric: 0.88 - ETA: 0s - loss: 0.2484 - accuracy: 0.8759 - f1_metric: 0.87 - ETA: 0s - loss: 0.2430 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2445 - accuracy: 0.8791 - f1_metric: 0.87 - ETA: 0s - loss: 0.2450 - accuracy: 0.8775 - f1_metric: 0.87 - ETA: 0s - loss: 0.2557 - accuracy: 0.8705 - f1_metric: 0.87 - ETA: 0s - loss: 0.2543 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2531 - accuracy: 0.8717 - f1_metric: 0.87 - ETA: 0s - loss: 0.2511 - accuracy: 0.8741 - f1_metric: 0.87 - ETA: 0s - loss: 0.2546 - accuracy: 0.8746 - f1_metric: 0.87 - ETA: 0s - loss: 0.2549 - accuracy: 0.8727 - f1_metric: 0.87 - ETA: 0s - loss: 0.2513 - accuracy: 0.8742 - f1_metric: 0.8750\n",
      "Epoch 00146: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2513 - accuracy: 0.8742 - f1_metric: 0.8750 - val_loss: 0.4621 - val_accuracy: 0.8340 - val_f1_metric: 0.8339 - lr: 1.0000e-04\n",
      "Epoch 147/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3134 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2776 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.2512 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2675 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2667 - accuracy: 0.8774 - f1_metric: 0.87 - ETA: 0s - loss: 0.2682 - accuracy: 0.8760 - f1_metric: 0.87 - ETA: 0s - loss: 0.2635 - accuracy: 0.8808 - f1_metric: 0.87 - ETA: 0s - loss: 0.2633 - accuracy: 0.8787 - f1_metric: 0.87 - ETA: 0s - loss: 0.2630 - accuracy: 0.8763 - f1_metric: 0.87 - ETA: 0s - loss: 0.2585 - accuracy: 0.8774 - f1_metric: 0.87 - ETA: 0s - loss: 0.2538 - accuracy: 0.8798 - f1_metric: 0.87 - ETA: 0s - loss: 0.2511 - accuracy: 0.8821 - f1_metric: 0.88 - ETA: 0s - loss: 0.2534 - accuracy: 0.8793 - f1_metric: 0.87 - ETA: 0s - loss: 0.2520 - accuracy: 0.8789 - f1_metric: 0.87 - ETA: 0s - loss: 0.2497 - accuracy: 0.8786 - f1_metric: 0.87 - ETA: 0s - loss: 0.2520 - accuracy: 0.8765 - f1_metric: 0.8759\n",
      "Epoch 00147: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2530 - accuracy: 0.8753 - f1_metric: 0.8754 - val_loss: 0.4421 - val_accuracy: 0.8433 - val_f1_metric: 0.8433 - lr: 1.0000e-04\n",
      "Epoch 148/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2500 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2529 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2485 - accuracy: 0.8809 - f1_metric: 0.88 - ETA: 0s - loss: 0.2482 - accuracy: 0.8793 - f1_metric: 0.87 - ETA: 0s - loss: 0.2454 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2485 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2432 - accuracy: 0.8882 - f1_metric: 0.88 - ETA: 0s - loss: 0.2487 - accuracy: 0.8864 - f1_metric: 0.88 - ETA: 0s - loss: 0.2480 - accuracy: 0.8869 - f1_metric: 0.88 - ETA: 0s - loss: 0.2479 - accuracy: 0.8883 - f1_metric: 0.88 - ETA: 0s - loss: 0.2495 - accuracy: 0.8836 - f1_metric: 0.88 - ETA: 0s - loss: 0.2535 - accuracy: 0.8826 - f1_metric: 0.88 - ETA: 0s - loss: 0.2531 - accuracy: 0.8791 - f1_metric: 0.87 - ETA: 0s - loss: 0.2517 - accuracy: 0.8834 - f1_metric: 0.88 - ETA: 0s - loss: 0.2512 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2486 - accuracy: 0.8826 - f1_metric: 0.8826\n",
      "Epoch 00148: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2483 - accuracy: 0.8825 - f1_metric: 0.8824 - val_loss: 0.4541 - val_accuracy: 0.8416 - val_f1_metric: 0.8413 - lr: 1.0000e-04\n",
      "Epoch 149/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2223 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2322 - accuracy: 0.9023 - f1_metric: 0.90 - ETA: 0s - loss: 0.2378 - accuracy: 0.8862 - f1_metric: 0.88 - ETA: 0s - loss: 0.2435 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2475 - accuracy: 0.8894 - f1_metric: 0.88 - ETA: 0s - loss: 0.2464 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2486 - accuracy: 0.8875 - f1_metric: 0.88 - ETA: 0s - loss: 0.2438 - accuracy: 0.8886 - f1_metric: 0.88 - ETA: 0s - loss: 0.2467 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2488 - accuracy: 0.8885 - f1_metric: 0.88 - ETA: 0s - loss: 0.2514 - accuracy: 0.8843 - f1_metric: 0.88 - ETA: 0s - loss: 0.2483 - accuracy: 0.8848 - f1_metric: 0.88 - ETA: 0s - loss: 0.2471 - accuracy: 0.8856 - f1_metric: 0.88 - ETA: 0s - loss: 0.2449 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2452 - accuracy: 0.8888 - f1_metric: 0.8890\n",
      "Epoch 00149: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2451 - accuracy: 0.8897 - f1_metric: 0.8908 - val_loss: 0.4464 - val_accuracy: 0.8424 - val_f1_metric: 0.8421 - lr: 1.0000e-04\n",
      "Epoch 150/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2919 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2680 - accuracy: 0.8795 - f1_metric: 0.87 - ETA: 0s - loss: 0.2444 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2423 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2383 - accuracy: 0.8955 - f1_metric: 0.89 - ETA: 0s - loss: 0.2387 - accuracy: 0.8939 - f1_metric: 0.89 - ETA: 0s - loss: 0.2332 - accuracy: 0.8984 - f1_metric: 0.89 - ETA: 0s - loss: 0.2387 - accuracy: 0.8981 - f1_metric: 0.89 - ETA: 0s - loss: 0.2415 - accuracy: 0.8951 - f1_metric: 0.89 - ETA: 0s - loss: 0.2395 - accuracy: 0.8911 - f1_metric: 0.89 - ETA: 0s - loss: 0.2431 - accuracy: 0.8884 - f1_metric: 0.88 - ETA: 0s - loss: 0.2451 - accuracy: 0.8890 - f1_metric: 0.88 - ETA: 0s - loss: 0.2451 - accuracy: 0.8883 - f1_metric: 0.88 - ETA: 0s - loss: 0.2420 - accuracy: 0.8911 - f1_metric: 0.8920\n",
      "Epoch 00150: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2420 - accuracy: 0.8911 - f1_metric: 0.8920 - val_loss: 0.4413 - val_accuracy: 0.8458 - val_f1_metric: 0.8444 - lr: 1.0000e-04\n",
      "Epoch 151/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2573 - accuracy: 0.8802 - f1_metric: 0.88 - ETA: 0s - loss: 0.2423 - accuracy: 0.8844 - f1_metric: 0.88 - ETA: 0s - loss: 0.2478 - accuracy: 0.8770 - f1_metric: 0.87 - ETA: 0s - loss: 0.2480 - accuracy: 0.8821 - f1_metric: 0.88 - ETA: 0s - loss: 0.2519 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2510 - accuracy: 0.8768 - f1_metric: 0.87 - ETA: 0s - loss: 0.2513 - accuracy: 0.8766 - f1_metric: 0.87 - ETA: 0s - loss: 0.2519 - accuracy: 0.8798 - f1_metric: 0.87 - ETA: 0s - loss: 0.2494 - accuracy: 0.8813 - f1_metric: 0.88 - ETA: 0s - loss: 0.2482 - accuracy: 0.8837 - f1_metric: 0.88 - ETA: 0s - loss: 0.2460 - accuracy: 0.8865 - f1_metric: 0.88 - ETA: 0s - loss: 0.2436 - accuracy: 0.8854 - f1_metric: 0.88 - ETA: 0s - loss: 0.2432 - accuracy: 0.8806 - f1_metric: 0.88 - ETA: 0s - loss: 0.2451 - accuracy: 0.8803 - f1_metric: 0.87 - ETA: 0s - loss: 0.2430 - accuracy: 0.8830 - f1_metric: 0.88 - ETA: 0s - loss: 0.2430 - accuracy: 0.8839 - f1_metric: 0.8838\n",
      "Epoch 00151: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.2430 - accuracy: 0.8839 - f1_metric: 0.8838 - val_loss: 0.4359 - val_accuracy: 0.8500 - val_f1_metric: 0.8496 - lr: 1.0000e-04\n",
      "Epoch 152/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9531 - f1_metric: 0.95 - ETA: 0s - loss: 0.2406 - accuracy: 0.8867 - f1_metric: 0.88 - ETA: 0s - loss: 0.2301 - accuracy: 0.9018 - f1_metric: 0.90 - ETA: 0s - loss: 0.2442 - accuracy: 0.8941 - f1_metric: 0.89 - ETA: 0s - loss: 0.2436 - accuracy: 0.8949 - f1_metric: 0.89 - ETA: 0s - loss: 0.2353 - accuracy: 0.9002 - f1_metric: 0.89 - ETA: 0s - loss: 0.2351 - accuracy: 0.8969 - f1_metric: 0.89 - ETA: 0s - loss: 0.2400 - accuracy: 0.8961 - f1_metric: 0.89 - ETA: 0s - loss: 0.2398 - accuracy: 0.8947 - f1_metric: 0.89 - ETA: 0s - loss: 0.2385 - accuracy: 0.8958 - f1_metric: 0.89 - ETA: 0s - loss: 0.2377 - accuracy: 0.8961 - f1_metric: 0.89 - ETA: 0s - loss: 0.2387 - accuracy: 0.8938 - f1_metric: 0.89 - ETA: 0s - loss: 0.2410 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2405 - accuracy: 0.8901 - f1_metric: 0.89 - ETA: 0s - loss: 0.2427 - accuracy: 0.8916 - f1_metric: 0.89 - ETA: 0s - loss: 0.2473 - accuracy: 0.8883 - f1_metric: 0.88 - ETA: 0s - loss: 0.2507 - accuracy: 0.8872 - f1_metric: 0.88 - ETA: 0s - loss: 0.2510 - accuracy: 0.8874 - f1_metric: 0.88 - ETA: 0s - loss: 0.2510 - accuracy: 0.8864 - f1_metric: 0.88 - ETA: 0s - loss: 0.2553 - accuracy: 0.8814 - f1_metric: 0.8806\n",
      "Epoch 00152: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.2553 - accuracy: 0.8814 - f1_metric: 0.8806 - val_loss: 0.4729 - val_accuracy: 0.8298 - val_f1_metric: 0.8298 - lr: 1.0000e-04\n",
      "Epoch 153/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2607 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.2835 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2599 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2551 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2522 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.2512 - accuracy: 0.8717 - f1_metric: 0.87 - ETA: 0s - loss: 0.2458 - accuracy: 0.8740 - f1_metric: 0.87 - ETA: 0s - loss: 0.2443 - accuracy: 0.8741 - f1_metric: 0.87 - ETA: 0s - loss: 0.2456 - accuracy: 0.8734 - f1_metric: 0.87 - ETA: 0s - loss: 0.2469 - accuracy: 0.8771 - f1_metric: 0.87 - ETA: 0s - loss: 0.2466 - accuracy: 0.8796 - f1_metric: 0.87 - ETA: 0s - loss: 0.2478 - accuracy: 0.8798 - f1_metric: 0.87 - ETA: 0s - loss: 0.2424 - accuracy: 0.8825 - f1_metric: 0.88 - ETA: 0s - loss: 0.2448 - accuracy: 0.8809 - f1_metric: 0.88 - ETA: 0s - loss: 0.2452 - accuracy: 0.8808 - f1_metric: 0.88 - ETA: 0s - loss: 0.2468 - accuracy: 0.8783 - f1_metric: 0.87 - ETA: 0s - loss: 0.2469 - accuracy: 0.8785 - f1_metric: 0.87 - ETA: 0s - loss: 0.2506 - accuracy: 0.8783 - f1_metric: 0.8783\n",
      "Epoch 00153: val_f1_metric did not improve from 0.85097\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.2522 - accuracy: 0.8782 - f1_metric: 0.8781 - val_loss: 0.4509 - val_accuracy: 0.8391 - val_f1_metric: 0.8392 - lr: 1.0000e-04\n",
      "Epoch 00153: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras evaluate= [0.4031935930252075, 0.8582914471626282, 0.8627076745033264]\n",
      "size of test set 995\n",
      "TP class counts (array([0, 1, 2], dtype=int64), array([ 45,  54, 755], dtype=int64))\n",
      "True class counts (array([0, 1, 2], dtype=int64), array([ 59,  60, 876], dtype=int64))\n",
      "Pred class counts (array([0, 1, 2], dtype=int64), array([100, 120, 775], dtype=int64))\n",
      "baseline acc: 88.04020100502512\n",
      "[[ 45   0  14]\n",
      " [  0  54   6]\n",
      " [ 55  66 755]]\n",
      "F1 score (weighted) 0.8749581765954672\n",
      "F1 score (macro) 0.6935449832196229\n",
      "F1 score (micro) 0.8582914572864322\n",
      "cohen's Kappa 0.5292509017699858\n",
      "precision of class 0 = 0.76\n",
      "precision of class 1 = 0.9\n",
      "precision of class 2 = 0.86\n",
      "precision avg 0.84\n",
      "labels already calculated\n",
      "Today we should Sell !!!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c8cd9685a875>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Today we should'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'!!!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m \u001b[0mstop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "#Applyng CNN for predicting Petrobras BUY SELL HOLD pattern\n",
    "# source https://towardsdatascience.com/stock-market-action-prediction-with-convnet-8689238feae3\n",
    "\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pandas_datareader import data as wb\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "\n",
    "\n",
    "# Load Pebrobras data from 2010 01 01 until now\n",
    "\n",
    "dataset=wb.DataReader('PETR4.SA', data_source='yahoo', start='2000-1-1')    #2005 best so far\n",
    "\n",
    "dataset = dataset.reset_index() #turn index into column\n",
    "\n",
    "###renaming columns to match the original code\n",
    "\n",
    "dataset.rename(columns={'Date':'timestamp'}, inplace=True)\n",
    "dataset.rename(columns={'Open':'open'}, inplace=True)\n",
    "dataset.rename(columns={'High':'high'}, inplace=True)\n",
    "dataset.rename(columns={'Close':'close'}, inplace=True)\n",
    "dataset.rename(columns={'Volume':'volume'}, inplace=True)\n",
    "dataset.rename(columns={'Low':'low'}, inplace=True)\n",
    "dataset.rename(columns={'Adj Close':'adjusted_close'}, inplace=True)\n",
    "\n",
    "colorder=['timestamp', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
    "\n",
    "dataset=dataset[colorder]\n",
    "\n",
    "#####create new features to construct the image\n",
    "\n",
    "DataGenerator('petra',dfinput=dataset.copy())  #save to a directory named 'outputs'\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "df=pd.read_csv('outputs\\\\df_petra.csv')\n",
    "\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "\n",
    "if 'dividend_amount' in df.columns:\n",
    "    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################# NORMALIZATION AND DEFINITION OF TRAIN TEST AND VALIDATION \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "list_features = list(df.loc[:, 'open':'eom_26'].columns)\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26'].values, df['labels'].values, train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n",
    "                                                    \n",
    "\n",
    "if 0.7*x_train.shape[0] < 2500:\n",
    "    train_split = 0.8\n",
    "else:\n",
    "    train_split = 0.7\n",
    "# train_split = 0.7\n",
    "print('train_split =',train_split)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=train_split, test_size=1-train_split, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "                                                \n",
    "from pickle import dump\n",
    "\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)             \n",
    "\n",
    "x_main = x_train.copy()\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))\n",
    "\n",
    "dump(mm_scaler, open('scaler.pkl', 'wb'))  #saving scaler for using on new data\n",
    "\n",
    "\n",
    "num_features = 225  # should be a perfect square\n",
    "selection_method = 'all'\n",
    "topk = 330 if selection_method == 'all' else num_features\n",
    "# if train_split >= 0.8:\n",
    "#     topk = 400\n",
    "# else:\n",
    "#     topk = 300\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "if selection_method == 'anova' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(f_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)  \n",
    "    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(selected_features_anova)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "if selection_method == 'mutual_info' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(len(selected_features_mic), selected_features_mic)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    \n",
    "    \n",
    "    \n",
    "if selection_method == 'all':\n",
    "    common = list(set(selected_features_anova).intersection(selected_features_mic))\n",
    "    print(\"common selected featues\", len(common), common)\n",
    "    if len(common) < num_features:\n",
    "        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n",
    "    feat_idx = []\n",
    "    for c in common:\n",
    "        feat_idx.append(list_features.index(c))\n",
    "    feat_idx = sorted(feat_idx[0:num_features])            # This is to ensure that related features are in close proximity in the image, since I had appended similar type of indicators closely. Feature selection significantly improved the performance of the model.\n",
    "    print(feat_idx)\n",
    "        \n",
    "    \n",
    "if selection_method == 'all':\n",
    "    x_train = x_train[:, feat_idx]\n",
    "    x_cv = x_cv[:, feat_idx]\n",
    "    x_test = x_test[:, feat_idx]\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, \n",
    "                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))                                                                                                                        \n",
    "                                                             \n",
    "_labels, _counts = np.unique(y_train, return_counts=True)\n",
    "print(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]/len(y_train) * 100, _counts[1]/len(y_train) * 100))        \n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "def get_sample_weights(y):\n",
    "    y = y.astype(int)  # compute_class_weight needs int labels\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y), y)    \n",
    "    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n",
    "    print(\"value_counts\", np.unique(y, return_counts=True))\n",
    "    sample_weights = y.copy().astype(float)\n",
    "    for i in np.unique(y):\n",
    "        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n",
    "        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n",
    "    return sample_weights\n",
    "\n",
    "def reshape_as_image(x, img_width, img_height):\n",
    "    x_temp = np.zeros((len(x), img_height, img_width))\n",
    "    for i in range(x.shape[0]):\n",
    "        # print(type(x), type(x_temp), x.shape)\n",
    "        x_temp[i] = np.reshape(x[i], (img_height, img_width))\n",
    "    return x_temp\n",
    "\n",
    "def f1_weighted(y_true, y_pred):\n",
    "    y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)  # can use conf_mat[0, :], tf.slice()\n",
    "    # precision = TP/TP+FP, recall = TP/TP+FN\n",
    "    rows, cols = conf_mat.get_shape()\n",
    "    size = y_true_class.get_shape()[0]\n",
    "    precision = tf.constant([0, 0, 0])  # change this to use rows/cols as size\n",
    "    recall = tf.constant([0, 0, 0])\n",
    "    class_counts = tf.constant([0, 0, 0])\n",
    "    def get_precision(i, conf_mat):\n",
    "        print(\"prec check\", conf_mat, conf_mat[i, i], tf.reduce_sum(conf_mat[:, i]))\n",
    "        precision[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[:, i]))\n",
    "        recall[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[i, :]))\n",
    "        tf.add(i, 1)\n",
    "        return i, conf_mat, precision, recall\n",
    "    def tf_count(i):\n",
    "        elements_equal_to_value = tf.equal(y_true_class, i)\n",
    "        as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "        count = tf.reduce_sum(as_ints)\n",
    "        class_counts[i].assign(count)\n",
    "        tf.add(i, 1)\n",
    "        return count\n",
    "    def condition(i, conf_mat):\n",
    "        return tf.less(i, 3)\n",
    "    i = tf.constant(3)\n",
    "    i, conf_mat = tf.while_loop(condition, get_precision, [i, conf_mat])\n",
    "    i = tf.constant(3)\n",
    "    c = lambda i: tf.less(i, 3)\n",
    "    b = tf_count(i)\n",
    "    tf.while_loop(c, b, [i])\n",
    "    weights = tf.math.divide(class_counts, size)\n",
    "    numerators = tf.math.multiply(tf.math.multiply(precision, recall), tf.constant(2))\n",
    "    denominators = tf.math.add(precision, recall)\n",
    "    f1s = tf.math.divide(numerators, denominators)\n",
    "    weighted_f1 = tf.reduce_sum(f.math.multiply(f1s, weights))\n",
    "    return weighted_f1\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    this calculates precision & recall \n",
    "    \"\"\"\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # mistake: y_pred of 0.3 is also considered 1\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    # y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    # y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    # conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)\n",
    "    # tf.Print(conf_mat, [conf_mat], \"confusion_matrix\")\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "get_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted})\n",
    "\n",
    "\n",
    "sample_weights = get_sample_weights(y_train)\n",
    "print(\"Test sample_weights\")\n",
    "rand_idx = np.random.randint(0, 1000, 30)\n",
    "print(y_train[rand_idx])\n",
    "print(sample_weights[rand_idx])\n",
    "\n",
    "one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\n",
    "y_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(\"y_train\",y_train.shape)\n",
    "y_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dim = int(np.sqrt(num_features))\n",
    "x_train = reshape_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "print(\"final shape of x, y train/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = rows = 3\n",
    "for i in range(1, columns*rows +1):\n",
    "    index = np.random.randint(len(x_train))\n",
    "    img = x_train[index]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.01, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.01, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from functools import *\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def f1_custom(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=1)\n",
    "    y_p = np.argmax(y_pred, axis=1)\n",
    "    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n",
    "\n",
    "def create_model_cnn(params):\n",
    "    model = Sequential()\n",
    "    print(\"Training with params {}\".format(params))\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    # x_train, y_train = get_data_cnn(df, df.head(1).iloc[0][\"timestamp\"])[0:2]\n",
    "    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n",
    "                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n",
    "                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n",
    "                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n",
    "                           padding='valid',activation=\"relu\", use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           input_shape=(x_train[0].shape[0],\n",
    "                                        x_train[0].shape[1], x_train[0].shape[2]))\n",
    "    model.add(conv2d_layer1)\n",
    "    if params[\"conv2d_layers\"]['conv2d_mp_1'] == 1:\n",
    "        model.add(MaxPool2D(pool_size=2))\n",
    "    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n",
    "    if params[\"conv2d_layers\"]['layers'] == 'two':\n",
    "        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n",
    "                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n",
    "                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n",
    "                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n",
    "                               padding='valid',activation=\"relu\", use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform')\n",
    "        model.add(conv2d_layer2)\n",
    "        if params[\"conv2d_layers\"]['conv2d_mp_2'] == 1:\n",
    "            model.add(MaxPool2D(pool_size=2))\n",
    "        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n",
    "    model.add(Dropout(params['dense_layers']['dense_do_1']))\n",
    "    if params['dense_layers'][\"layers\"] == 'two':\n",
    "        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n",
    "                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n",
    "        model.add(Dropout(params['dense_layers']['dense_do_2']))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    if params[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    elif params[\"optimizer\"] == 'sgd':\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif params[\"optimizer\"] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n",
    "    # from keras.utils.vis_utils import plot_model use this too for diagram with plot\n",
    "    # model.summary(print_fn=lambda x: print(x + '\\n'))\n",
    "    return model\n",
    "\n",
    "def check_baseline(pred, y_test):\n",
    "    print(\"size of test set\", len(y_test))\n",
    "    e = np.equal(pred, y_test)\n",
    "    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n",
    "    print(\"True class counts\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n",
    "    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n",
    "    print(\"baseline acc:\", (holds/len(y_test)*100))\n",
    "    \n",
    "    \n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "\n",
    "model = create_model_cnn(params)      \n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras_petrobras')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric\n",
    "                      \n",
    "#training model\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=64, shuffle=True,\n",
    "                            # validation_split=0.3,\n",
    "                            validation_data=(x_cv, y_cv),\n",
    "                            callbacks=[mcp, rlp, es]\n",
    "                            , sample_weight=sample_weights)\n",
    "                            \n",
    "                            \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "test_res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"keras evaluate=\", test_res)\n",
    "pred = model.predict(x_test)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "check_baseline(pred_classes, y_test_classes)\n",
    "conf_mat = confusion_matrix(y_test_classes, pred_classes)\n",
    "print(conf_mat)\n",
    "labels = [0,1,2]\n",
    "# ax = sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "# ax.xaxis.set_ticks_position('top')\n",
    "f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='weighted', sample_weight=None)\n",
    "print(\"F1 score (weighted)\", f1_weighted)\n",
    "print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='macro', sample_weight=None))\n",
    "print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "prec = []\n",
    "for i, row in enumerate(conf_mat):\n",
    "    prec.append(np.round(row[i]/np.sum(row), 2))\n",
    "    print(\"precision of class {} = {}\".format(i, prec[i]))\n",
    "print(\"precision avg\", sum(prec)/len(prec))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TESTING FOR TODAY\n",
    "\n",
    "import os\n",
    "                            \n",
    "dataset=wb.DataReader('PETR4.SA', data_source='yahoo', start='2020-02-05')    \n",
    "\n",
    "dataset = dataset.reset_index() #turn index into column\n",
    "\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras_petrobras')\n",
    "\n",
    "\n",
    "#renaming columns to match the code\n",
    "\n",
    "dataset.rename(columns={'Date':'timestamp'}, inplace=True)\n",
    "dataset.rename(columns={'Open':'open'}, inplace=True)\n",
    "dataset.rename(columns={'High':'high'}, inplace=True)\n",
    "dataset.rename(columns={'Close':'close'}, inplace=True)\n",
    "dataset.rename(columns={'Volume':'volume'}, inplace=True)\n",
    "dataset.rename(columns={'Low':'low'}, inplace=True)\n",
    "dataset.rename(columns={'Adj Close':'adjusted_close'}, inplace=True)                            \n",
    "                            \n",
    "colorder=['timestamp', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
    "\n",
    "dataset=dataset[colorder]\n",
    "                            \n",
    "                            \n",
    "DataGenerator('petra2',dfinput=dataset.copy())\n",
    "\n",
    "df=pd.read_csv('outputs\\\\df_petra2.csv')\n",
    "\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "\n",
    "if 'dividend_amount' in df.columns:\n",
    "    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "                            \n",
    "list_features = list(df.loc[:, 'open':'eom_26'].columns)\n",
    "                            \n",
    "x_new=df.loc[:,list_features].values  \n",
    "y_new=df.loc[:,'labels'].values\n",
    "                            \n",
    "#load scaler\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "loaded_scaler=load(open('scaler.pkl', 'rb'))\n",
    "\n",
    "x_new_norm=loaded_scaler.transform(x_new) \n",
    "\n",
    "x_new_data=x_new_norm[:,feat_idx]\n",
    "                            \n",
    "dim = int(np.sqrt(num_features))\n",
    "\n",
    "x_new_data= reshape_as_image(x_new_data, dim, dim)\n",
    "\n",
    "x_new_data= np.stack((x_new_data,) * 3, axis=-1)\n",
    "\n",
    "\n",
    "#model = load_model(best_model_path)\n",
    "\n",
    "\n",
    "pred = model.predict(x_new_data)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "\n",
    "confusion_matrix(y_new, pred_classes)\n",
    "\n",
    "\n",
    "labels_output={1: 'BUY', 0: 'SELL', 2: 'HOLD'}\n",
    "\n",
    "\n",
    "print('TODAY WE SHOULD',labels_output[pred_classes[-1]],'!!!')\n",
    "                            \n",
    "\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                 \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
