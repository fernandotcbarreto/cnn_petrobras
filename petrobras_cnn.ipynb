{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels already calculated\n",
      "Total number of features 447\n",
      "train_split = 0.7\n",
      "Shape of x, y train/cv/test (2784, 447) (2784,) (1194, 447) (1194,) (995, 447) (995,)\n",
      "('volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_16', 'rsi_17', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_22', 'rsi_23', 'rsi_24', 'rsi_25', 'rsi_26', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_17', 'mfi_18', 'mfi_19', 'mfi_20', 'mfi_21', 'mfi_22', 'mfi_23', 'mfi_24', 'mfi_25', 'mfi_26', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_14', 'roc_15', 'roc_19', 'roc_20', 'roc_21', 'roc_22', 'roc_23', 'roc_24', 'roc_25', 'roc_26', 'cmf_6', 'cmf_7', 'cmf_8', 'cmf_9', 'cmf_10', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_20', 'cmf_21', 'cmf_22', 'cmf_23', 'cmf_24', 'cmf_25', 'cmf_26', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_18', 'cmo_19', 'cmo_20', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_24', 'cmo_25', 'cmo_26', 'close_sma_6', 'close_sma_7', 'close_sma_8', 'close_sma_9', 'close_sma_10', 'close_sma_11', 'open_sma_6', 'open_sma_7', 'open_sma_8', 'open_sma_9', 'open_sma_10', 'open_sma_11', 'wma_8', 'wma_9', 'wma_10', 'wma_11', 'wma_12', 'wma_13', 'wma_14', 'wma_15', 'hma_6', 'hma_7', 'hma_8', 'hma_9', 'hma_10', 'hma_11', 'hma_12', 'hma_13', 'hma_14', 'hma_15', 'hma_16', 'hma_17', 'hma_18', 'hma_19', 'hma_20', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_11', 'trix_12', 'trix_13', 'trix_14', 'trix_15', 'trix_16', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'bb_9', 'bb_10', 'bb_11', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26')\n",
      "[  5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  77  78\n",
      "  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 153 154 155 156 157 158 197 198 199 200 201 202 203 204 222 223 224\n",
      " 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\n",
      " 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
      " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
      " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296\n",
      " 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
      " 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 363 364 365\n",
      " 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383\n",
      " 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425]\n",
      "****************************************\n",
      "330 ('volume', 'rsi_6', 'rsi_7', 'rsi_8', 'rsi_9', 'rsi_10', 'rsi_11', 'rsi_12', 'rsi_13', 'rsi_14', 'rsi_15', 'rsi_18', 'rsi_19', 'rsi_20', 'rsi_21', 'rsi_23', 'rsi_25', 'wr_6', 'wr_7', 'wr_8', 'wr_9', 'wr_10', 'wr_11', 'wr_12', 'wr_13', 'wr_14', 'wr_15', 'wr_16', 'wr_17', 'wr_18', 'wr_19', 'wr_20', 'wr_21', 'wr_22', 'wr_23', 'wr_24', 'wr_25', 'wr_26', 'mfi_6', 'mfi_7', 'mfi_8', 'mfi_9', 'mfi_10', 'mfi_11', 'mfi_12', 'mfi_13', 'mfi_14', 'mfi_15', 'mfi_16', 'mfi_18', 'mfi_19', 'mfi_22', 'mfi_26', 'roc_6', 'roc_7', 'roc_8', 'roc_9', 'roc_10', 'roc_11', 'roc_12', 'roc_13', 'roc_14', 'roc_15', 'roc_17', 'roc_18', 'roc_22', 'roc_24', 'cmf_6', 'cmf_7', 'cmf_9', 'cmf_10', 'cmf_11', 'cmf_12', 'cmf_13', 'cmf_14', 'cmf_15', 'cmf_16', 'cmf_17', 'cmf_18', 'cmf_19', 'cmf_22', 'cmf_23', 'cmo_6', 'cmo_7', 'cmo_8', 'cmo_9', 'cmo_10', 'cmo_11', 'cmo_12', 'cmo_13', 'cmo_14', 'cmo_15', 'cmo_16', 'cmo_17', 'cmo_19', 'cmo_20', 'cmo_21', 'cmo_22', 'cmo_23', 'cmo_24', 'cmo_25', 'cmo_26', 'close_sma_6', 'close_sma_8', 'open_sma_13', 'ema_11', 'hma_6', 'hma_7', 'hma_8', 'hma_9', 'hma_11', 'hma_12', 'hma_13', 'hma_14', 'hma_15', 'hma_16', 'hma_17', 'hma_18', 'hma_19', 'hma_20', 'trix_6', 'trix_7', 'trix_8', 'trix_9', 'trix_10', 'trix_11', 'trix_12', 'trix_13', 'trix_14', 'trix_15', 'trix_16', 'trix_17', 'trix_18', 'trix_19', 'trix_20', 'trix_21', 'trix_22', 'trix_23', 'trix_24', 'trix_25', 'trix_26', 'cci_6', 'cci_7', 'cci_8', 'cci_9', 'cci_10', 'cci_11', 'cci_12', 'cci_13', 'cci_14', 'cci_15', 'cci_16', 'cci_17', 'cci_18', 'cci_19', 'cci_20', 'cci_21', 'cci_22', 'cci_23', 'cci_24', 'cci_25', 'cci_26', 'dpo_6', 'dpo_7', 'dpo_8', 'dpo_9', 'dpo_10', 'dpo_11', 'dpo_12', 'dpo_13', 'dpo_14', 'dpo_15', 'dpo_16', 'dpo_17', 'dpo_18', 'dpo_19', 'dpo_20', 'dpo_21', 'dpo_22', 'dpo_23', 'dpo_24', 'dpo_25', 'dpo_26', 'kst_6', 'kst_7', 'kst_8', 'kst_9', 'kst_10', 'kst_11', 'kst_12', 'kst_13', 'kst_14', 'kst_15', 'kst_16', 'kst_17', 'kst_18', 'kst_19', 'kst_20', 'kst_21', 'kst_22', 'kst_23', 'kst_24', 'kst_25', 'kst_26', 'dmi_6', 'dmi_7', 'dmi_8', 'dmi_9', 'dmi_10', 'dmi_11', 'dmi_12', 'dmi_13', 'dmi_14', 'dmi_15', 'dmi_16', 'dmi_17', 'dmi_18', 'dmi_19', 'dmi_20', 'dmi_21', 'dmi_22', 'dmi_23', 'dmi_24', 'dmi_25', 'dmi_26', 'bb_6', 'bb_7', 'bb_8', 'bb_9', 'bb_10', 'bb_11', 'bb_12', 'bb_13', 'bb_14', 'bb_15', 'bb_16', 'bb_17', 'bb_18', 'bb_19', 'bb_20', 'bb_21', 'bb_22', 'bb_23', 'bb_24', 'bb_25', 'bb_26', 'fi_6', 'fi_7', 'fi_8', 'fi_9', 'fi_10', 'fi_11', 'fi_12', 'fi_13', 'fi_14', 'fi_15', 'fi_16', 'fi_17', 'fi_18', 'fi_19', 'fi_20', 'fi_21', 'fi_22', 'fi_23', 'fi_24', 'fi_25', 'fi_26', 'rsv_6', 'kdjk_6', 'rsv_7', 'kdjk_7', 'rsv_8', 'kdjk_8', 'rsv_9', 'kdjk_9', 'rsv_10', 'kdjk_10', 'rsv_11', 'kdjk_11', 'rsv_12', 'kdjk_12', 'rsv_13', 'kdjk_13', 'rsv_14', 'kdjk_14', 'rsv_15', 'kdjk_15', 'rsv_16', 'kdjk_16', 'rsv_17', 'kdjk_17', 'rsv_18', 'kdjk_18', 'rsv_19', 'kdjk_19', 'rsv_20', 'kdjk_20', 'rsv_21', 'kdjk_21', 'rsv_22', 'kdjk_22', 'rsv_23', 'kdjk_23', 'rsv_24', 'kdjk_24', 'rsv_25', 'kdjk_25', 'rsv_26', 'kdjk_26', 'eom_6', 'eom_7', 'eom_8', 'eom_9', 'eom_10', 'eom_11', 'eom_12', 'eom_13', 'eom_14', 'eom_15', 'eom_16', 'eom_17', 'eom_18', 'eom_19', 'eom_20', 'eom_21', 'eom_22', 'eom_23', 'eom_24', 'eom_25', 'eom_26')\n",
      "[  5   6   7   8   9  10  11  12  13  14  15  18  19  20  21  23  25  27\n",
      "  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45\n",
      "  46  47  48  49  50  51  52  53  54  55  56  57  58  60  61  64  68  69\n",
      "  70  71  72  73  74  75  76  77  78  80  81  85  87  90  91  93  94  95\n",
      "  96  97  98  99 100 101 102 103 106 107 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 124 125 126 127 128 129 130 131 132 134 160 179 222 223\n",
      " 224 225 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\n",
      " 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
      " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
      " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296\n",
      " 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314\n",
      " 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
      " 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368\n",
      " 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386\n",
      " 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404\n",
      " 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 441 442 443 444 445 446]\n",
      "common selected featues 287 ['trix_13', 'mfi_11', 'bb_6', 'dpo_10', 'rsv_24', 'rsi_18', 'wr_21', 'cci_20', 'fi_19', 'trix_11', 'roc_24', 'dpo_21', 'kdjk_14', 'kst_17', 'fi_20', 'dmi_20', 'rsv_11', 'fi_10', 'cmo_8', 'cmo_15', 'cmo_17', 'rsv_16', 'rsv_19', 'volume', 'hma_13', 'cmo_11', 'hma_14', 'kdjk_26', 'bb_7', 'cci_19', 'dpo_25', 'wr_9', 'trix_10', 'kst_8', 'mfi_9', 'close_sma_8', 'cmf_22', 'roc_11', 'rsi_20', 'rsv_23', 'wr_13', 'roc_9', 'trix_8', 'cmo_10', 'cmf_9', 'bb_9', 'dpo_16', 'kst_18', 'kdjk_12', 'cci_12', 'kdjk_15', 'mfi_8', 'dpo_9', 'trix_17', 'hma_8', 'dmi_10', 'dmi_17', 'mfi_22', 'dmi_7', 'dmi_18', 'rsv_13', 'cmf_10', 'dpo_24', 'rsv_8', 'cmo_7', 'kst_26', 'mfi_16', 'rsv_17', 'trix_12', 'trix_14', 'kst_13', 'kdjk_6', 'hma_18', 'trix_24', 'roc_10', 'cmo_16', 'rsi_25', 'dpo_7', 'kst_16', 'trix_7', 'roc_8', 'wr_23', 'cmo_25', 'fi_16', 'trix_23', 'wr_25', 'kdjk_21', 'trix_22', 'dmi_11', 'roc_14', 'kst_10', 'trix_16', 'kdjk_8', 'hma_12', 'kst_20', 'cmf_7', 'wr_8', 'dmi_21', 'hma_6', 'hma_7', 'rsi_13', 'rsv_20', 'rsi_14', 'hma_11', 'cmo_26', 'kst_25', 'fi_26', 'wr_14', 'mfi_19', 'mfi_26', 'close_sma_6', 'dpo_14', 'kst_19', 'kdjk_24', 'dpo_6', 'fi_13', 'cmf_14', 'rsi_8', 'dpo_19', 'kst_12', 'kdjk_18', 'rsi_23', 'wr_24', 'cmf_18', 'cci_14', 'cci_9', 'trix_15', 'dpo_22', 'fi_25', 'dmi_6', 'kst_6', 'dmi_12', 'cmf_6', 'hma_19', 'trix_9', 'roc_22', 'trix_20', 'cmo_24', 'kst_11', 'dmi_14', 'mfi_12', 'hma_9', 'cci_25', 'cmo_23', 'dpo_13', 'dpo_17', 'cmf_16', 'hma_20', 'rsi_12', 'trix_21', 'cmf_23', 'cci_23', 'dpo_12', 'rsi_11', 'kst_22', 'wr_11', 'mfi_14', 'kst_21', 'wr_10', 'rsi_21', 'fi_6', 'kdjk_9', 'cmo_9', 'cci_13', 'cmo_22', 'rsv_9', 'fi_18', 'dmi_16', 'kst_7', 'fi_17', 'wr_15', 'dmi_22', 'cmf_12', 'mfi_15', 'dpo_18', 'dpo_26', 'kst_9', 'kst_14', 'dpo_23', 'rsi_10', 'rsv_21', 'rsv_26', 'wr_26', 'rsv_6', 'rsv_12', 'rsi_19', 'cci_26', 'fi_24', 'rsv_18', 'rsv_14', 'cci_17', 'kdjk_22', 'wr_18', 'trix_6', 'trix_18', 'mfi_13', 'cmo_20', 'kdjk_10', 'wr_7', 'wr_20', 'rsv_15', 'wr_22', 'mfi_7', 'fi_23', 'kdjk_17', 'dpo_8', 'cmo_19', 'dmi_9', 'kdjk_13', 'cmo_14', 'cmf_19', 'fi_9', 'dmi_19', 'rsv_7', 'trix_26', 'dmi_24', 'cci_10', 'fi_14', 'rsi_6', 'kdjk_23', 'cci_11', 'wr_19', 'roc_6', 'fi_15', 'rsv_10', 'cci_24', 'wr_16', 'cmo_21', 'cci_18', 'rsi_7', 'rsv_22', 'dmi_26', 'hma_16', 'rsi_9', 'trix_25', 'cci_21', 'fi_8', 'cmf_15', 'kdjk_16', 'rsi_15', 'dpo_11', 'kdjk_20', 'kdjk_11', 'fi_7', 'mfi_6', 'cci_15', 'fi_12', 'cmo_12', 'roc_7', 'mfi_18', 'kst_15', 'fi_11', 'fi_21', 'wr_17', 'mfi_10', 'wr_12', 'cmo_6', 'dpo_15', 'kst_24', 'dmi_8', 'bb_8', 'kdjk_25', 'wr_6', 'trix_19', 'cci_16', 'cci_8', 'cmf_17', 'kst_23', 'dmi_15', 'bb_11', 'dmi_13', 'rsv_25', 'bb_10', 'kdjk_19', 'roc_15', 'cci_6', 'hma_17', 'hma_15', 'cci_22', 'fi_22', 'cmo_13', 'cci_7', 'kdjk_7', 'dmi_23', 'dmi_25', 'dpo_20', 'cmf_13']\n",
      "[5, 6, 8, 10, 11, 12, 13, 14, 18, 19, 20, 21, 23, 25, 28, 29, 30, 31, 32, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 53, 54, 55, 56, 57, 58, 61, 64, 68, 69, 71, 72, 73, 74, 77, 85, 87, 90, 91, 93, 94, 96, 98, 100, 102, 103, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 124, 125, 127, 128, 129, 130, 131, 132, 134, 222, 223, 224, 225, 227, 228, 229, 230, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 257, 261, 262, 263, 264, 265, 266, 269, 271, 272, 275, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312, 313, 314, 315, 316, 319, 320, 321, 322, 324, 325, 326, 327, 329, 331, 332, 333, 334, 335, 336, 337, 339, 342, 343, 345, 363, 366, 367, 370, 371, 372, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 385, 386, 388, 389, 390, 391, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 406, 407, 408, 409, 410, 412, 414, 415, 417, 418, 419, 420, 421, 424, 425]\n",
      "Shape of x, y train/cv/test (2784, 225) (2784,) (1194, 225) (1194,) (995, 225) (995,)\n",
      "percentage of class 0 = 5.962643678160919, class 1 = 6.070402298850574\n",
      "real class weights are [5.59036145 5.49112426 0.37893018] [0 1 2]\n",
      "value_counts (array([0, 1, 2]), array([ 166,  169, 2449], dtype=int64))\n",
      "Test sample_weights\n",
      "[0 2 2 2 2 0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[5.59036145 0.37893018 0.37893018 0.37893018 0.37893018 5.59036145\n",
      " 0.37893018 5.49112426 0.37893018 0.37893018 0.37893018 0.37893018\n",
      " 0.37893018 0.37893018 0.37893018 0.37893018 0.37893018 0.37893018\n",
      " 0.37893018 0.37893018 0.37893018 0.37893018 0.37893018 0.37893018\n",
      " 0.37893018 0.37893018 0.37893018 0.37893018 0.37893018 0.37893018]\n",
      "y_train (2784, 3)\n",
      "final shape of x, y train/test (2784, 15, 15, 3) (2784, 3) (995, 15, 15, 3) (995, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.01, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.01, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 1.1650 - accuracy: 0.8750 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.4557 - accuracy: 0.8542 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.4430 - accuracy: 0.7219 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.4177 - accuracy: 0.4922 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.3694 - accuracy: 0.3750 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.3418 - accuracy: 0.3092 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.3016 - accuracy: 0.3042 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.2987 - accuracy: 0.3789 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.2892 - accuracy: 0.4062 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.2921 - accuracy: 0.4319 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.2919 - accuracy: 0.4258 - f1_metric: 0.0000e+ - ETA: 0s - loss: 1.2734 - accuracy: 0.3916 - f1_metric: 0.0027   - ETA: 0s - loss: 1.2590 - accuracy: 0.3653 - f1_metric: 0.00 - ETA: 0s - loss: 1.2386 - accuracy: 0.3619 - f1_metric: 0.00 - ETA: 0s - loss: 1.2377 - accuracy: 0.3922 - f1_metric: 0.01 - ETA: 0s - loss: 1.2449 - accuracy: 0.4055 - f1_metric: 0.0131\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.05075, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 132ms/step - loss: 1.2410 - accuracy: 0.3980 - f1_metric: 0.0125 - val_loss: 1.3060 - val_accuracy: 0.0921 - val_f1_metric: 0.0507 - lr: 0.0010\n",
      "Epoch 2/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 1.3190 - accuracy: 0.1562 - f1_metric: 0.11 - ETA: 0s - loss: 1.1196 - accuracy: 0.0898 - f1_metric: 0.05 - ETA: 0s - loss: 1.0775 - accuracy: 0.0807 - f1_metric: 0.05 - ETA: 0s - loss: 1.0551 - accuracy: 0.0820 - f1_metric: 0.05 - ETA: 0s - loss: 1.0937 - accuracy: 0.1172 - f1_metric: 0.06 - ETA: 0s - loss: 1.0576 - accuracy: 0.1589 - f1_metric: 0.05 - ETA: 0s - loss: 1.0457 - accuracy: 0.2165 - f1_metric: 0.07 - ETA: 0s - loss: 1.0899 - accuracy: 0.2539 - f1_metric: 0.07 - ETA: 0s - loss: 1.1136 - accuracy: 0.2726 - f1_metric: 0.08 - ETA: 0s - loss: 1.1065 - accuracy: 0.2788 - f1_metric: 0.08 - ETA: 0s - loss: 1.1061 - accuracy: 0.2790 - f1_metric: 0.08 - ETA: 0s - loss: 1.0950 - accuracy: 0.2717 - f1_metric: 0.07 - ETA: 0s - loss: 1.0909 - accuracy: 0.2773 - f1_metric: 0.07 - ETA: 0s - loss: 1.0728 - accuracy: 0.2784 - f1_metric: 0.07 - ETA: 0s - loss: 1.0845 - accuracy: 0.2866 - f1_metric: 0.08 - ETA: 0s - loss: 1.0974 - accuracy: 0.2893 - f1_metric: 0.08 - ETA: 0s - loss: 1.0957 - accuracy: 0.2817 - f1_metric: 0.08 - ETA: 0s - loss: 1.0919 - accuracy: 0.2723 - f1_metric: 0.08 - ETA: 0s - loss: 1.0972 - accuracy: 0.2691 - f1_metric: 0.08 - ETA: 0s - loss: 1.0952 - accuracy: 0.2627 - f1_metric: 0.08 - ETA: 0s - loss: 1.0839 - accuracy: 0.2648 - f1_metric: 0.08 - ETA: 0s - loss: 1.0951 - accuracy: 0.2708 - f1_metric: 0.0928\n",
      "Epoch 00002: val_f1_metric improved from 0.05075 to 0.17067, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 143ms/step - loss: 1.0941 - accuracy: 0.2719 - f1_metric: 0.0931 - val_loss: 1.1579 - val_accuracy: 0.3300 - val_f1_metric: 0.1707 - lr: 0.0010\n",
      "Epoch 3/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1707 - accuracy: 0.2344 - f1_metric: 0.12 - ETA: 0s - loss: 1.0276 - accuracy: 0.2227 - f1_metric: 0.13 - ETA: 0s - loss: 1.0095 - accuracy: 0.2366 - f1_metric: 0.12 - ETA: 0s - loss: 0.9933 - accuracy: 0.2309 - f1_metric: 0.11 - ETA: 0s - loss: 1.0157 - accuracy: 0.2429 - f1_metric: 0.12 - ETA: 0s - loss: 1.0114 - accuracy: 0.2623 - f1_metric: 0.12 - ETA: 0s - loss: 1.0039 - accuracy: 0.2725 - f1_metric: 0.13 - ETA: 0s - loss: 0.9849 - accuracy: 0.2804 - f1_metric: 0.13 - ETA: 0s - loss: 0.9749 - accuracy: 0.3011 - f1_metric: 0.16 - ETA: 0s - loss: 0.9707 - accuracy: 0.3158 - f1_metric: 0.18 - ETA: 0s - loss: 0.9675 - accuracy: 0.3250 - f1_metric: 0.19 - ETA: 0s - loss: 0.9660 - accuracy: 0.3323 - f1_metric: 0.20 - ETA: 0s - loss: 0.9716 - accuracy: 0.3393 - f1_metric: 0.21 - ETA: 0s - loss: 0.9730 - accuracy: 0.3359 - f1_metric: 0.21 - ETA: 0s - loss: 0.9648 - accuracy: 0.3296 - f1_metric: 0.20 - ETA: 0s - loss: 0.9736 - accuracy: 0.3291 - f1_metric: 0.20 - ETA: 0s - loss: 0.9739 - accuracy: 0.3263 - f1_metric: 0.20 - ETA: 0s - loss: 0.9766 - accuracy: 0.3277 - f1_metric: 0.20 - ETA: 0s - loss: 0.9712 - accuracy: 0.3325 - f1_metric: 0.20 - ETA: 0s - loss: 0.9726 - accuracy: 0.3338 - f1_metric: 0.20 - ETA: 0s - loss: 0.9820 - accuracy: 0.3365 - f1_metric: 0.2063\n",
      "Epoch 00003: val_f1_metric improved from 0.17067 to 0.20576, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 127ms/step - loss: 0.9822 - accuracy: 0.3373 - f1_metric: 0.2077 - val_loss: 1.1062 - val_accuracy: 0.3668 - val_f1_metric: 0.2058 - lr: 0.0010\n",
      "Epoch 4/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 1.1036 - accuracy: 0.3594 - f1_metric: 0.21 - ETA: 0s - loss: 0.9617 - accuracy: 0.3229 - f1_metric: 0.15 - ETA: 0s - loss: 0.8736 - accuracy: 0.3333 - f1_metric: 0.15 - ETA: 0s - loss: 0.9555 - accuracy: 0.3819 - f1_metric: 0.21 - ETA: 0s - loss: 0.9173 - accuracy: 0.4304 - f1_metric: 0.25 - ETA: 0s - loss: 0.9190 - accuracy: 0.4387 - f1_metric: 0.28 - ETA: 0s - loss: 0.9039 - accuracy: 0.4427 - f1_metric: 0.29 - ETA: 0s - loss: 0.9036 - accuracy: 0.4439 - f1_metric: 0.30 - ETA: 0s - loss: 0.9162 - accuracy: 0.4391 - f1_metric: 0.30 - ETA: 0s - loss: 0.9306 - accuracy: 0.4359 - f1_metric: 0.30 - ETA: 0s - loss: 0.9237 - accuracy: 0.4308 - f1_metric: 0.29 - ETA: 0s - loss: 0.9157 - accuracy: 0.4205 - f1_metric: 0.28 - ETA: 0s - loss: 0.9270 - accuracy: 0.4160 - f1_metric: 0.28 - ETA: 0s - loss: 0.9190 - accuracy: 0.3987 - f1_metric: 0.26 - ETA: 0s - loss: 0.9219 - accuracy: 0.3973 - f1_metric: 0.26 - ETA: 0s - loss: 0.9179 - accuracy: 0.3885 - f1_metric: 0.26 - ETA: 0s - loss: 0.9160 - accuracy: 0.3856 - f1_metric: 0.26 - ETA: 0s - loss: 0.9119 - accuracy: 0.3916 - f1_metric: 0.26 - ETA: 0s - loss: 0.8966 - accuracy: 0.3964 - f1_metric: 0.27 - ETA: 0s - loss: 0.8920 - accuracy: 0.4084 - f1_metric: 0.28 - ETA: 0s - loss: 0.8896 - accuracy: 0.4309 - f1_metric: 0.30 - ETA: 0s - loss: 0.8842 - accuracy: 0.4286 - f1_metric: 0.3079\n",
      "Epoch 00004: val_f1_metric did not improve from 0.20576\n",
      "44/44 [==============================] - 2s 37ms/step - loss: 0.8829 - accuracy: 0.4256 - f1_metric: 0.3053 - val_loss: 1.3033 - val_accuracy: 0.2814 - val_f1_metric: 0.1784 - lr: 0.0010\n",
      "Epoch 5/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.7529 - accuracy: 0.3281 - f1_metric: 0.23 - ETA: 0s - loss: 0.8606 - accuracy: 0.2891 - f1_metric: 0.23 - ETA: 0s - loss: 0.8678 - accuracy: 0.3594 - f1_metric: 0.29 - ETA: 0s - loss: 0.8179 - accuracy: 0.4125 - f1_metric: 0.33 - ETA: 0s - loss: 0.8019 - accuracy: 0.4579 - f1_metric: 0.38 - ETA: 0s - loss: 0.8248 - accuracy: 0.4961 - f1_metric: 0.42 - ETA: 0s - loss: 0.8252 - accuracy: 0.4967 - f1_metric: 0.42 - ETA: 0s - loss: 0.8219 - accuracy: 0.4893 - f1_metric: 0.41 - ETA: 0s - loss: 0.8083 - accuracy: 0.4837 - f1_metric: 0.40 - ETA: 0s - loss: 0.8077 - accuracy: 0.4805 - f1_metric: 0.40 - ETA: 0s - loss: 0.8039 - accuracy: 0.4884 - f1_metric: 0.41 - ETA: 0s - loss: 0.8060 - accuracy: 0.4922 - f1_metric: 0.41 - ETA: 0s - loss: 0.8078 - accuracy: 0.4920 - f1_metric: 0.42 - ETA: 0s - loss: 0.8030 - accuracy: 0.4875 - f1_metric: 0.41 - ETA: 0s - loss: 0.8054 - accuracy: 0.4786 - f1_metric: 0.4088\n",
      "Epoch 00005: val_f1_metric improved from 0.20576 to 0.38014, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 112ms/step - loss: 0.8042 - accuracy: 0.4759 - f1_metric: 0.4036 - val_loss: 1.0721 - val_accuracy: 0.4347 - val_f1_metric: 0.3801 - lr: 0.0010\n",
      "Epoch 6/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.6340 - accuracy: 0.4688 - f1_metric: 0.35 - ETA: 0s - loss: 0.6335 - accuracy: 0.5469 - f1_metric: 0.48 - ETA: 0s - loss: 0.7743 - accuracy: 0.5938 - f1_metric: 0.54 - ETA: 0s - loss: 0.7727 - accuracy: 0.5868 - f1_metric: 0.53 - ETA: 0s - loss: 0.7596 - accuracy: 0.5597 - f1_metric: 0.50 - ETA: 0s - loss: 0.7889 - accuracy: 0.5264 - f1_metric: 0.46 - ETA: 0s - loss: 0.7671 - accuracy: 0.5104 - f1_metric: 0.45 - ETA: 0s - loss: 0.7539 - accuracy: 0.5074 - f1_metric: 0.44 - ETA: 0s - loss: 0.7582 - accuracy: 0.5156 - f1_metric: 0.45 - ETA: 0s - loss: 0.7445 - accuracy: 0.5258 - f1_metric: 0.46 - ETA: 0s - loss: 0.7350 - accuracy: 0.5455 - f1_metric: 0.48 - ETA: 0s - loss: 0.7298 - accuracy: 0.5550 - f1_metric: 0.49 - ETA: 0s - loss: 0.7265 - accuracy: 0.5638 - f1_metric: 0.50 - ETA: 0s - loss: 0.7237 - accuracy: 0.5757 - f1_metric: 0.52 - ETA: 0s - loss: 0.7320 - accuracy: 0.5770 - f1_metric: 0.52 - ETA: 0s - loss: 0.7415 - accuracy: 0.5734 - f1_metric: 0.51 - ETA: 0s - loss: 0.7381 - accuracy: 0.5635 - f1_metric: 0.50 - ETA: 0s - loss: 0.7448 - accuracy: 0.5483 - f1_metric: 0.49 - ETA: 0s - loss: 0.7444 - accuracy: 0.5352 - f1_metric: 0.47 - ETA: 0s - loss: 0.7498 - accuracy: 0.5300 - f1_metric: 0.47 - ETA: 0s - loss: 0.7563 - accuracy: 0.5270 - f1_metric: 0.47 - ETA: 0s - loss: 0.7605 - accuracy: 0.5253 - f1_metric: 0.4715\n",
      "Epoch 00006: val_f1_metric improved from 0.38014 to 0.40553, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 128ms/step - loss: 0.7650 - accuracy: 0.5241 - f1_metric: 0.4706 - val_loss: 1.0799 - val_accuracy: 0.4564 - val_f1_metric: 0.4055 - lr: 0.0010\n",
      "Epoch 7/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.8455 - accuracy: 0.4531 - f1_metric: 0.40 - ETA: 0s - loss: 0.6893 - accuracy: 0.4479 - f1_metric: 0.38 - ETA: 0s - loss: 0.6692 - accuracy: 0.4766 - f1_metric: 0.41 - ETA: 0s - loss: 0.6760 - accuracy: 0.5078 - f1_metric: 0.44 - ETA: 0s - loss: 0.6944 - accuracy: 0.5328 - f1_metric: 0.47 - ETA: 0s - loss: 0.7259 - accuracy: 0.5276 - f1_metric: 0.47 - ETA: 0s - loss: 0.7147 - accuracy: 0.5177 - f1_metric: 0.47 - ETA: 0s - loss: 0.7128 - accuracy: 0.5147 - f1_metric: 0.47 - ETA: 0s - loss: 0.7278 - accuracy: 0.5008 - f1_metric: 0.46 - ETA: 0s - loss: 0.7177 - accuracy: 0.4973 - f1_metric: 0.46 - ETA: 0s - loss: 0.7232 - accuracy: 0.4993 - f1_metric: 0.46 - ETA: 0s - loss: 0.7134 - accuracy: 0.5048 - f1_metric: 0.46 - ETA: 0s - loss: 0.7189 - accuracy: 0.5106 - f1_metric: 0.47 - ETA: 0s - loss: 0.7117 - accuracy: 0.5250 - f1_metric: 0.48 - ETA: 0s - loss: 0.7041 - accuracy: 0.5288 - f1_metric: 0.49 - ETA: 0s - loss: 0.7139 - accuracy: 0.5330 - f1_metric: 0.49 - ETA: 0s - loss: 0.7216 - accuracy: 0.5359 - f1_metric: 0.50 - ETA: 0s - loss: 0.7213 - accuracy: 0.5333 - f1_metric: 0.49 - ETA: 0s - loss: 0.7219 - accuracy: 0.5232 - f1_metric: 0.48 - ETA: 0s - loss: 0.7218 - accuracy: 0.5142 - f1_metric: 0.4756\n",
      "Epoch 00007: val_f1_metric did not improve from 0.40553\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.7210 - accuracy: 0.5126 - f1_metric: 0.4733 - val_loss: 1.0950 - val_accuracy: 0.4447 - val_f1_metric: 0.4047 - lr: 0.0010\n",
      "Epoch 8/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.6522 - accuracy: 0.4531 - f1_metric: 0.38 - ETA: 0s - loss: 0.6539 - accuracy: 0.5104 - f1_metric: 0.45 - ETA: 0s - loss: 0.6386 - accuracy: 0.5594 - f1_metric: 0.51 - ETA: 0s - loss: 0.6855 - accuracy: 0.5996 - f1_metric: 0.56 - ETA: 0s - loss: 0.7064 - accuracy: 0.6078 - f1_metric: 0.58 - ETA: 0s - loss: 0.7206 - accuracy: 0.5951 - f1_metric: 0.57 - ETA: 0s - loss: 0.7137 - accuracy: 0.5815 - f1_metric: 0.55 - ETA: 0s - loss: 0.7096 - accuracy: 0.5625 - f1_metric: 0.53 - ETA: 0s - loss: 0.6961 - accuracy: 0.5354 - f1_metric: 0.50 - ETA: 0s - loss: 0.6976 - accuracy: 0.5376 - f1_metric: 0.50 - ETA: 0s - loss: 0.6964 - accuracy: 0.5475 - f1_metric: 0.51 - ETA: 0s - loss: 0.6900 - accuracy: 0.5552 - f1_metric: 0.52 - ETA: 0s - loss: 0.7019 - accuracy: 0.5549 - f1_metric: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.5496 - f1_metric: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.5507 - f1_metric: 0.51 - ETA: 0s - loss: 0.6955 - accuracy: 0.5501 - f1_metric: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.5532 - f1_metric: 0.5158\n",
      "Epoch 00008: val_f1_metric improved from 0.40553 to 0.52814, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 0.6885 - accuracy: 0.5510 - f1_metric: 0.5116 - val_loss: 0.9411 - val_accuracy: 0.5662 - val_f1_metric: 0.5281 - lr: 0.0010\n",
      "Epoch 9/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5805 - accuracy: 0.5781 - f1_metric: 0.56 - ETA: 0s - loss: 0.6168 - accuracy: 0.6250 - f1_metric: 0.59 - ETA: 0s - loss: 0.6068 - accuracy: 0.6302 - f1_metric: 0.60 - ETA: 0s - loss: 0.6086 - accuracy: 0.6523 - f1_metric: 0.62 - ETA: 0s - loss: 0.6340 - accuracy: 0.6469 - f1_metric: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.6367 - f1_metric: 0.60 - ETA: 0s - loss: 0.6433 - accuracy: 0.6306 - f1_metric: 0.59 - ETA: 0s - loss: 0.6329 - accuracy: 0.6289 - f1_metric: 0.59 - ETA: 0s - loss: 0.6402 - accuracy: 0.6189 - f1_metric: 0.58 - ETA: 0s - loss: 0.6364 - accuracy: 0.6172 - f1_metric: 0.58 - ETA: 0s - loss: 0.6556 - accuracy: 0.6087 - f1_metric: 0.57 - ETA: 0s - loss: 0.6463 - accuracy: 0.6101 - f1_metric: 0.57 - ETA: 0s - loss: 0.6521 - accuracy: 0.6061 - f1_metric: 0.57 - ETA: 0s - loss: 0.6555 - accuracy: 0.5980 - f1_metric: 0.56 - ETA: 0s - loss: 0.6548 - accuracy: 0.5920 - f1_metric: 0.56 - ETA: 0s - loss: 0.6488 - accuracy: 0.5911 - f1_metric: 0.56 - ETA: 0s - loss: 0.6463 - accuracy: 0.5902 - f1_metric: 0.55 - ETA: 0s - loss: 0.6567 - accuracy: 0.5919 - f1_metric: 0.55 - ETA: 0s - loss: 0.6619 - accuracy: 0.5951 - f1_metric: 0.56 - ETA: 0s - loss: 0.6581 - accuracy: 0.5976 - f1_metric: 0.56 - ETA: 0s - loss: 0.6620 - accuracy: 0.5942 - f1_metric: 0.56 - ETA: 0s - loss: 0.6608 - accuracy: 0.5930 - f1_metric: 0.56 - ETA: 0s - loss: 0.6566 - accuracy: 0.5868 - f1_metric: 0.5561\n",
      "Epoch 00009: val_f1_metric did not improve from 0.52814\n",
      "44/44 [==============================] - 2s 38ms/step - loss: 0.6594 - accuracy: 0.5851 - f1_metric: 0.5524 - val_loss: 1.0970 - val_accuracy: 0.4548 - val_f1_metric: 0.4172 - lr: 0.0010\n",
      "Epoch 10/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.3906 - f1_metric: 0.38 - ETA: 0s - loss: 0.6499 - accuracy: 0.5104 - f1_metric: 0.48 - ETA: 0s - loss: 0.6119 - accuracy: 0.5437 - f1_metric: 0.50 - ETA: 0s - loss: 0.5882 - accuracy: 0.5781 - f1_metric: 0.53 - ETA: 0s - loss: 0.6342 - accuracy: 0.6094 - f1_metric: 0.56 - ETA: 0s - loss: 0.6217 - accuracy: 0.6322 - f1_metric: 0.59 - ETA: 0s - loss: 0.6262 - accuracy: 0.6479 - f1_metric: 0.60 - ETA: 0s - loss: 0.6239 - accuracy: 0.6471 - f1_metric: 0.60 - ETA: 0s - loss: 0.6314 - accuracy: 0.6211 - f1_metric: 0.57 - ETA: 0s - loss: 0.6340 - accuracy: 0.6080 - f1_metric: 0.56 - ETA: 0s - loss: 0.6271 - accuracy: 0.5980 - f1_metric: 0.56 - ETA: 0s - loss: 0.6152 - accuracy: 0.5986 - f1_metric: 0.56 - ETA: 0s - loss: 0.6338 - accuracy: 0.6069 - f1_metric: 0.56 - ETA: 0s - loss: 0.6313 - accuracy: 0.6080 - f1_metric: 0.56 - ETA: 0s - loss: 0.6438 - accuracy: 0.6020 - f1_metric: 0.56 - ETA: 0s - loss: 0.6461 - accuracy: 0.5972 - f1_metric: 0.56 - ETA: 0s - loss: 0.6455 - accuracy: 0.5959 - f1_metric: 0.5617\n",
      "Epoch 00010: val_f1_metric improved from 0.52814 to 0.54285, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 113ms/step - loss: 0.6455 - accuracy: 0.5959 - f1_metric: 0.5617 - val_loss: 0.8973 - val_accuracy: 0.5779 - val_f1_metric: 0.5428 - lr: 0.0010\n",
      "Epoch 11/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5177 - accuracy: 0.5781 - f1_metric: 0.47 - ETA: 0s - loss: 0.5838 - accuracy: 0.5859 - f1_metric: 0.53 - ETA: 0s - loss: 0.5999 - accuracy: 0.6250 - f1_metric: 0.58 - ETA: 0s - loss: 0.6154 - accuracy: 0.6313 - f1_metric: 0.59 - ETA: 0s - loss: 0.6049 - accuracy: 0.6082 - f1_metric: 0.57 - ETA: 0s - loss: 0.5958 - accuracy: 0.6062 - f1_metric: 0.57 - ETA: 0s - loss: 0.6158 - accuracy: 0.5956 - f1_metric: 0.56 - ETA: 0s - loss: 0.6257 - accuracy: 0.5929 - f1_metric: 0.56 - ETA: 0s - loss: 0.6220 - accuracy: 0.5908 - f1_metric: 0.55 - ETA: 0s - loss: 0.6440 - accuracy: 0.5892 - f1_metric: 0.55 - ETA: 0s - loss: 0.6541 - accuracy: 0.5883 - f1_metric: 0.55 - ETA: 0s - loss: 0.6473 - accuracy: 0.5932 - f1_metric: 0.56 - ETA: 0s - loss: 0.6384 - accuracy: 0.5970 - f1_metric: 0.56 - ETA: 0s - loss: 0.6361 - accuracy: 0.5957 - f1_metric: 0.56 - ETA: 0s - loss: 0.6365 - accuracy: 0.5960 - f1_metric: 0.56 - ETA: 0s - loss: 0.6412 - accuracy: 0.5920 - f1_metric: 0.56 - ETA: 0s - loss: 0.6419 - accuracy: 0.5883 - f1_metric: 0.55 - ETA: 0s - loss: 0.6380 - accuracy: 0.5861 - f1_metric: 0.55 - ETA: 0s - loss: 0.6355 - accuracy: 0.5854 - f1_metric: 0.55 - ETA: 0s - loss: 0.6371 - accuracy: 0.5850 - f1_metric: 0.5529\n",
      "Epoch 00011: val_f1_metric improved from 0.54285 to 0.66603, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 127ms/step - loss: 0.6367 - accuracy: 0.5837 - f1_metric: 0.5513 - val_loss: 0.7207 - val_accuracy: 0.6809 - val_f1_metric: 0.6660 - lr: 0.0010\n",
      "Epoch 12/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5854 - accuracy: 0.7969 - f1_metric: 0.77 - ETA: 0s - loss: 0.6058 - accuracy: 0.7500 - f1_metric: 0.72 - ETA: 0s - loss: 0.6318 - accuracy: 0.6797 - f1_metric: 0.64 - ETA: 0s - loss: 0.6202 - accuracy: 0.6250 - f1_metric: 0.58 - ETA: 0s - loss: 0.6105 - accuracy: 0.6108 - f1_metric: 0.56 - ETA: 0s - loss: 0.5990 - accuracy: 0.6083 - f1_metric: 0.56 - ETA: 0s - loss: 0.6053 - accuracy: 0.6152 - f1_metric: 0.57 - ETA: 0s - loss: 0.6203 - accuracy: 0.6172 - f1_metric: 0.57 - ETA: 0s - loss: 0.6003 - accuracy: 0.6243 - f1_metric: 0.58 - ETA: 0s - loss: 0.6041 - accuracy: 0.6204 - f1_metric: 0.58 - ETA: 0s - loss: 0.6017 - accuracy: 0.6134 - f1_metric: 0.57 - ETA: 0s - loss: 0.6008 - accuracy: 0.6138 - f1_metric: 0.57 - ETA: 0s - loss: 0.5987 - accuracy: 0.6083 - f1_metric: 0.57 - ETA: 0s - loss: 0.5951 - accuracy: 0.6099 - f1_metric: 0.57 - ETA: 0s - loss: 0.5876 - accuracy: 0.6117 - f1_metric: 0.57 - ETA: 0s - loss: 0.5979 - accuracy: 0.6140 - f1_metric: 0.57 - ETA: 0s - loss: 0.5929 - accuracy: 0.6158 - f1_metric: 0.58 - ETA: 0s - loss: 0.5952 - accuracy: 0.6174 - f1_metric: 0.58 - ETA: 0s - loss: 0.5931 - accuracy: 0.6157 - f1_metric: 0.5838\n",
      "Epoch 00012: val_f1_metric did not improve from 0.66603\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.5931 - accuracy: 0.6164 - f1_metric: 0.5849 - val_loss: 0.8757 - val_accuracy: 0.5888 - val_f1_metric: 0.5546 - lr: 0.0010\n",
      "Epoch 13/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5193 - accuracy: 0.5469 - f1_metric: 0.53 - ETA: 0s - loss: 0.5340 - accuracy: 0.5781 - f1_metric: 0.53 - ETA: 0s - loss: 0.5362 - accuracy: 0.5875 - f1_metric: 0.54 - ETA: 0s - loss: 0.5683 - accuracy: 0.6071 - f1_metric: 0.56 - ETA: 0s - loss: 0.5659 - accuracy: 0.6146 - f1_metric: 0.57 - ETA: 0s - loss: 0.5626 - accuracy: 0.6307 - f1_metric: 0.59 - ETA: 0s - loss: 0.5578 - accuracy: 0.6346 - f1_metric: 0.59 - ETA: 0s - loss: 0.5606 - accuracy: 0.6365 - f1_metric: 0.60 - ETA: 0s - loss: 0.5574 - accuracy: 0.6397 - f1_metric: 0.60 - ETA: 0s - loss: 0.5711 - accuracy: 0.6340 - f1_metric: 0.59 - ETA: 0s - loss: 0.5579 - accuracy: 0.6391 - f1_metric: 0.60 - ETA: 0s - loss: 0.5553 - accuracy: 0.6478 - f1_metric: 0.61 - ETA: 0s - loss: 0.5523 - accuracy: 0.6464 - f1_metric: 0.61 - ETA: 0s - loss: 0.5464 - accuracy: 0.6464 - f1_metric: 0.61 - ETA: 0s - loss: 0.5544 - accuracy: 0.6397 - f1_metric: 0.61 - ETA: 0s - loss: 0.5495 - accuracy: 0.6393 - f1_metric: 0.61 - ETA: 0s - loss: 0.5482 - accuracy: 0.6398 - f1_metric: 0.61 - ETA: 0s - loss: 0.5449 - accuracy: 0.6388 - f1_metric: 0.6134\n",
      "Epoch 00013: val_f1_metric did not improve from 0.66603\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.5469 - accuracy: 0.6376 - f1_metric: 0.6126 - val_loss: 0.7717 - val_accuracy: 0.6558 - val_f1_metric: 0.6388 - lr: 0.0010\n",
      "Epoch 14/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5628 - accuracy: 0.7031 - f1_metric: 0.65 - ETA: 0s - loss: 0.4678 - accuracy: 0.6797 - f1_metric: 0.66 - ETA: 0s - loss: 0.5044 - accuracy: 0.7299 - f1_metric: 0.71 - ETA: 0s - loss: 0.5541 - accuracy: 0.7156 - f1_metric: 0.69 - ETA: 0s - loss: 0.5624 - accuracy: 0.6851 - f1_metric: 0.66 - ETA: 0s - loss: 0.5630 - accuracy: 0.6377 - f1_metric: 0.61 - ETA: 0s - loss: 0.5753 - accuracy: 0.6224 - f1_metric: 0.59 - ETA: 0s - loss: 0.5697 - accuracy: 0.6195 - f1_metric: 0.59 - ETA: 0s - loss: 0.5688 - accuracy: 0.6250 - f1_metric: 0.60 - ETA: 0s - loss: 0.5747 - accuracy: 0.6283 - f1_metric: 0.60 - ETA: 0s - loss: 0.5746 - accuracy: 0.6337 - f1_metric: 0.61 - ETA: 0s - loss: 0.5712 - accuracy: 0.6352 - f1_metric: 0.61 - ETA: 0s - loss: 0.5751 - accuracy: 0.6274 - f1_metric: 0.60 - ETA: 0s - loss: 0.5694 - accuracy: 0.6250 - f1_metric: 0.60 - ETA: 0s - loss: 0.5589 - accuracy: 0.6296 - f1_metric: 0.60 - ETA: 0s - loss: 0.5571 - accuracy: 0.6348 - f1_metric: 0.61 - ETA: 0s - loss: 0.5551 - accuracy: 0.6381 - f1_metric: 0.6196\n",
      "Epoch 00014: val_f1_metric improved from 0.66603 to 0.73592, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 122ms/step - loss: 0.5548 - accuracy: 0.6386 - f1_metric: 0.6206 - val_loss: 0.6334 - val_accuracy: 0.7437 - val_f1_metric: 0.7359 - lr: 0.0010\n",
      "Epoch 15/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.8684 - accuracy: 0.7188 - f1_metric: 0.72 - ETA: 0s - loss: 0.5865 - accuracy: 0.7695 - f1_metric: 0.75 - ETA: 0s - loss: 0.5558 - accuracy: 0.6920 - f1_metric: 0.67 - ETA: 0s - loss: 0.5435 - accuracy: 0.6597 - f1_metric: 0.64 - ETA: 0s - loss: 0.5357 - accuracy: 0.6562 - f1_metric: 0.64 - ETA: 0s - loss: 0.5262 - accuracy: 0.6550 - f1_metric: 0.63 - ETA: 0s - loss: 0.5434 - accuracy: 0.6660 - f1_metric: 0.65 - ETA: 0s - loss: 0.5338 - accuracy: 0.6710 - f1_metric: 0.65 - ETA: 0s - loss: 0.5371 - accuracy: 0.6766 - f1_metric: 0.66 - ETA: 0s - loss: 0.5362 - accuracy: 0.6768 - f1_metric: 0.66 - ETA: 0s - loss: 0.5405 - accuracy: 0.6732 - f1_metric: 0.65 - ETA: 0s - loss: 0.5374 - accuracy: 0.6634 - f1_metric: 0.64 - ETA: 0s - loss: 0.5401 - accuracy: 0.6516 - f1_metric: 0.63 - ETA: 0s - loss: 0.5349 - accuracy: 0.6487 - f1_metric: 0.63 - ETA: 0s - loss: 0.5430 - accuracy: 0.6527 - f1_metric: 0.63 - ETA: 0s - loss: 0.5435 - accuracy: 0.6567 - f1_metric: 0.64 - ETA: 0s - loss: 0.5396 - accuracy: 0.6603 - f1_metric: 0.64 - ETA: 0s - loss: 0.5383 - accuracy: 0.6550 - f1_metric: 0.64 - ETA: 0s - loss: 0.5373 - accuracy: 0.6558 - f1_metric: 0.64 - ETA: 0s - loss: 0.5415 - accuracy: 0.6540 - f1_metric: 0.64 - ETA: 0s - loss: 0.5369 - accuracy: 0.6537 - f1_metric: 0.6422\n",
      "Epoch 00015: val_f1_metric did not improve from 0.73592\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.5369 - accuracy: 0.6537 - f1_metric: 0.6422 - val_loss: 0.8364 - val_accuracy: 0.6323 - val_f1_metric: 0.6049 - lr: 0.0010\n",
      "Epoch 16/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.5938 - f1_metric: 0.57 - ETA: 0s - loss: 0.5815 - accuracy: 0.6055 - f1_metric: 0.58 - ETA: 0s - loss: 0.5641 - accuracy: 0.6406 - f1_metric: 0.62 - ETA: 0s - loss: 0.5554 - accuracy: 0.6649 - f1_metric: 0.65 - ETA: 0s - loss: 0.5276 - accuracy: 0.6733 - f1_metric: 0.65 - ETA: 0s - loss: 0.5114 - accuracy: 0.6719 - f1_metric: 0.65 - ETA: 0s - loss: 0.5020 - accuracy: 0.6844 - f1_metric: 0.67 - ETA: 0s - loss: 0.5022 - accuracy: 0.6866 - f1_metric: 0.67 - ETA: 0s - loss: 0.5300 - accuracy: 0.6817 - f1_metric: 0.67 - ETA: 0s - loss: 0.5338 - accuracy: 0.6749 - f1_metric: 0.66 - ETA: 0s - loss: 0.5333 - accuracy: 0.6617 - f1_metric: 0.64 - ETA: 0s - loss: 0.5356 - accuracy: 0.6600 - f1_metric: 0.64 - ETA: 0s - loss: 0.5467 - accuracy: 0.6574 - f1_metric: 0.64 - ETA: 0s - loss: 0.5432 - accuracy: 0.6464 - f1_metric: 0.62 - ETA: 0s - loss: 0.5412 - accuracy: 0.6411 - f1_metric: 0.62 - ETA: 0s - loss: 0.5441 - accuracy: 0.6441 - f1_metric: 0.63 - ETA: 0s - loss: 0.5435 - accuracy: 0.6509 - f1_metric: 0.63 - ETA: 0s - loss: 0.5479 - accuracy: 0.6574 - f1_metric: 0.64 - ETA: 0s - loss: 0.5507 - accuracy: 0.6598 - f1_metric: 0.6455\n",
      "Epoch 00016: val_f1_metric did not improve from 0.73592\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.5507 - accuracy: 0.6598 - f1_metric: 0.6455 - val_loss: 0.9072 - val_accuracy: 0.5637 - val_f1_metric: 0.5420 - lr: 0.0010\n",
      "Epoch 17/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.5938 - f1_metric: 0.50 - ETA: 0s - loss: 0.5756 - accuracy: 0.5273 - f1_metric: 0.47 - ETA: 0s - loss: 0.5826 - accuracy: 0.5402 - f1_metric: 0.50 - ETA: 0s - loss: 0.5692 - accuracy: 0.5562 - f1_metric: 0.52 - ETA: 0s - loss: 0.5535 - accuracy: 0.5697 - f1_metric: 0.53 - ETA: 0s - loss: 0.5498 - accuracy: 0.5885 - f1_metric: 0.55 - ETA: 0s - loss: 0.5317 - accuracy: 0.6111 - f1_metric: 0.58 - ETA: 0s - loss: 0.5253 - accuracy: 0.6273 - f1_metric: 0.60 - ETA: 0s - loss: 0.5217 - accuracy: 0.6433 - f1_metric: 0.61 - ETA: 0s - loss: 0.5110 - accuracy: 0.6556 - f1_metric: 0.63 - ETA: 0s - loss: 0.5130 - accuracy: 0.6568 - f1_metric: 0.63 - ETA: 0s - loss: 0.5048 - accuracy: 0.6592 - f1_metric: 0.64 - ETA: 0s - loss: 0.4991 - accuracy: 0.6616 - f1_metric: 0.64 - ETA: 0s - loss: 0.5030 - accuracy: 0.6628 - f1_metric: 0.64 - ETA: 0s - loss: 0.5077 - accuracy: 0.6696 - f1_metric: 0.65 - ETA: 0s - loss: 0.5100 - accuracy: 0.6685 - f1_metric: 0.6503\n",
      "Epoch 00017: val_f1_metric did not improve from 0.73592\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.5100 - accuracy: 0.6685 - f1_metric: 0.6503 - val_loss: 0.9034 - val_accuracy: 0.5737 - val_f1_metric: 0.5565 - lr: 0.0010\n",
      "Epoch 18/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.6094 - f1_metric: 0.56 - ETA: 0s - loss: 0.5074 - accuracy: 0.5781 - f1_metric: 0.53 - ETA: 0s - loss: 0.5564 - accuracy: 0.5893 - f1_metric: 0.54 - ETA: 0s - loss: 0.5203 - accuracy: 0.6078 - f1_metric: 0.57 - ETA: 0s - loss: 0.5361 - accuracy: 0.6238 - f1_metric: 0.59 - ETA: 0s - loss: 0.5153 - accuracy: 0.6406 - f1_metric: 0.61 - ETA: 0s - loss: 0.4958 - accuracy: 0.6538 - f1_metric: 0.63 - ETA: 0s - loss: 0.5020 - accuracy: 0.6662 - f1_metric: 0.64 - ETA: 0s - loss: 0.5012 - accuracy: 0.6744 - f1_metric: 0.65 - ETA: 0s - loss: 0.5053 - accuracy: 0.6635 - f1_metric: 0.64 - ETA: 0s - loss: 0.5108 - accuracy: 0.6583 - f1_metric: 0.63 - ETA: 0s - loss: 0.5155 - accuracy: 0.6548 - f1_metric: 0.63 - ETA: 0s - loss: 0.5126 - accuracy: 0.6500 - f1_metric: 0.63 - ETA: 0s - loss: 0.5140 - accuracy: 0.6546 - f1_metric: 0.63 - ETA: 0s - loss: 0.5067 - accuracy: 0.6593 - f1_metric: 0.64 - ETA: 0s - loss: 0.5031 - accuracy: 0.6588 - f1_metric: 0.6425\n",
      "Epoch 00018: val_f1_metric improved from 0.73592 to 0.75183, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 116ms/step - loss: 0.5031 - accuracy: 0.6588 - f1_metric: 0.6425 - val_loss: 0.5978 - val_accuracy: 0.7546 - val_f1_metric: 0.7518 - lr: 0.0010\n",
      "Epoch 19/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4173 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.4811 - accuracy: 0.7773 - f1_metric: 0.77 - ETA: 0s - loss: 0.4525 - accuracy: 0.7746 - f1_metric: 0.77 - ETA: 0s - loss: 0.4800 - accuracy: 0.7625 - f1_metric: 0.76 - ETA: 0s - loss: 0.4695 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.4721 - accuracy: 0.7377 - f1_metric: 0.73 - ETA: 0s - loss: 0.4832 - accuracy: 0.7169 - f1_metric: 0.70 - ETA: 0s - loss: 0.4843 - accuracy: 0.7048 - f1_metric: 0.69 - ETA: 0s - loss: 0.4804 - accuracy: 0.7061 - f1_metric: 0.69 - ETA: 0s - loss: 0.4831 - accuracy: 0.7074 - f1_metric: 0.69 - ETA: 0s - loss: 0.4822 - accuracy: 0.7058 - f1_metric: 0.69 - ETA: 0s - loss: 0.4880 - accuracy: 0.7044 - f1_metric: 0.69 - ETA: 0s - loss: 0.4822 - accuracy: 0.6997 - f1_metric: 0.68 - ETA: 0s - loss: 0.4818 - accuracy: 0.6983 - f1_metric: 0.68 - ETA: 0s - loss: 0.4770 - accuracy: 0.7001 - f1_metric: 0.68 - ETA: 0s - loss: 0.4764 - accuracy: 0.7017 - f1_metric: 0.69 - ETA: 0s - loss: 0.4822 - accuracy: 0.7013 - f1_metric: 0.69 - ETA: 0s - loss: 0.4892 - accuracy: 0.7010 - f1_metric: 0.69 - ETA: 0s - loss: 0.4875 - accuracy: 0.6978 - f1_metric: 0.68 - ETA: 0s - loss: 0.4871 - accuracy: 0.6973 - f1_metric: 0.68 - ETA: 0s - loss: 0.4862 - accuracy: 0.6922 - f1_metric: 0.6827\n",
      "Epoch 00019: val_f1_metric did not improve from 0.75183\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.4859 - accuracy: 0.6918 - f1_metric: 0.6816 - val_loss: 0.7371 - val_accuracy: 0.6633 - val_f1_metric: 0.6454 - lr: 0.0010\n",
      "Epoch 20/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4685 - accuracy: 0.6094 - f1_metric: 0.59 - ETA: 0s - loss: 0.4184 - accuracy: 0.7135 - f1_metric: 0.69 - ETA: 0s - loss: 0.4177 - accuracy: 0.7375 - f1_metric: 0.72 - ETA: 0s - loss: 0.4811 - accuracy: 0.7165 - f1_metric: 0.70 - ETA: 0s - loss: 0.4859 - accuracy: 0.6997 - f1_metric: 0.68 - ETA: 0s - loss: 0.4626 - accuracy: 0.7116 - f1_metric: 0.69 - ETA: 0s - loss: 0.4661 - accuracy: 0.7103 - f1_metric: 0.69 - ETA: 0s - loss: 0.4408 - accuracy: 0.7236 - f1_metric: 0.71 - ETA: 0s - loss: 0.4557 - accuracy: 0.7319 - f1_metric: 0.72 - ETA: 0s - loss: 0.4635 - accuracy: 0.7329 - f1_metric: 0.72 - ETA: 0s - loss: 0.4584 - accuracy: 0.7303 - f1_metric: 0.72 - ETA: 0s - loss: 0.4573 - accuracy: 0.7319 - f1_metric: 0.72 - ETA: 0s - loss: 0.4726 - accuracy: 0.7216 - f1_metric: 0.71 - ETA: 0s - loss: 0.4857 - accuracy: 0.7139 - f1_metric: 0.70 - ETA: 0s - loss: 0.4816 - accuracy: 0.7142 - f1_metric: 0.70 - ETA: 0s - loss: 0.4805 - accuracy: 0.7088 - f1_metric: 0.69 - ETA: 0s - loss: 0.4839 - accuracy: 0.7080 - f1_metric: 0.69 - ETA: 0s - loss: 0.4791 - accuracy: 0.7082 - f1_metric: 0.69 - ETA: 0s - loss: 0.4767 - accuracy: 0.7083 - f1_metric: 0.69 - ETA: 0s - loss: 0.4722 - accuracy: 0.7124 - f1_metric: 0.7017\n",
      "Epoch 00020: val_f1_metric did not improve from 0.75183\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.4710 - accuracy: 0.7123 - f1_metric: 0.7033 - val_loss: 0.5963 - val_accuracy: 0.7546 - val_f1_metric: 0.7506 - lr: 0.0010\n",
      "Epoch 21/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5127 - accuracy: 0.7031 - f1_metric: 0.70 - ETA: 0s - loss: 0.4801 - accuracy: 0.7461 - f1_metric: 0.74 - ETA: 0s - loss: 0.4835 - accuracy: 0.7578 - f1_metric: 0.75 - ETA: 0s - loss: 0.4616 - accuracy: 0.7461 - f1_metric: 0.74 - ETA: 0s - loss: 0.4534 - accuracy: 0.7312 - f1_metric: 0.72 - ETA: 0s - loss: 0.4523 - accuracy: 0.7175 - f1_metric: 0.71 - ETA: 0s - loss: 0.4547 - accuracy: 0.7083 - f1_metric: 0.70 - ETA: 0s - loss: 0.4643 - accuracy: 0.7105 - f1_metric: 0.70 - ETA: 0s - loss: 0.4692 - accuracy: 0.7113 - f1_metric: 0.70 - ETA: 0s - loss: 0.4687 - accuracy: 0.7091 - f1_metric: 0.70 - ETA: 0s - loss: 0.4609 - accuracy: 0.7120 - f1_metric: 0.70 - ETA: 0s - loss: 0.4614 - accuracy: 0.7085 - f1_metric: 0.70 - ETA: 0s - loss: 0.4548 - accuracy: 0.7143 - f1_metric: 0.71 - ETA: 0s - loss: 0.4493 - accuracy: 0.7224 - f1_metric: 0.71 - ETA: 0s - loss: 0.4577 - accuracy: 0.7259 - f1_metric: 0.72 - ETA: 0s - loss: 0.4586 - accuracy: 0.7232 - f1_metric: 0.71 - ETA: 0s - loss: 0.4568 - accuracy: 0.7175 - f1_metric: 0.71 - ETA: 0s - loss: 0.4562 - accuracy: 0.7119 - f1_metric: 0.70 - ETA: 0s - loss: 0.4538 - accuracy: 0.7115 - f1_metric: 0.7061\n",
      "Epoch 00021: val_f1_metric improved from 0.75183 to 0.75579, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 6s 126ms/step - loss: 0.4526 - accuracy: 0.7126 - f1_metric: 0.7085 - val_loss: 0.5962 - val_accuracy: 0.7563 - val_f1_metric: 0.7558 - lr: 0.0010\n",
      "Epoch 22/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4967 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.4676 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.4812 - accuracy: 0.7552 - f1_metric: 0.75 - ETA: 0s - loss: 0.4668 - accuracy: 0.7326 - f1_metric: 0.72 - ETA: 0s - loss: 0.4425 - accuracy: 0.7301 - f1_metric: 0.72 - ETA: 0s - loss: 0.4458 - accuracy: 0.7377 - f1_metric: 0.73 - ETA: 0s - loss: 0.4517 - accuracy: 0.7365 - f1_metric: 0.73 - ETA: 0s - loss: 0.4574 - accuracy: 0.7363 - f1_metric: 0.73 - ETA: 0s - loss: 0.4575 - accuracy: 0.7371 - f1_metric: 0.73 - ETA: 0s - loss: 0.4685 - accuracy: 0.7319 - f1_metric: 0.72 - ETA: 0s - loss: 0.4679 - accuracy: 0.7289 - f1_metric: 0.72 - ETA: 0s - loss: 0.4688 - accuracy: 0.7266 - f1_metric: 0.72 - ETA: 0s - loss: 0.4664 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4682 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4611 - accuracy: 0.7154 - f1_metric: 0.70 - ETA: 0s - loss: 0.4627 - accuracy: 0.7177 - f1_metric: 0.70 - ETA: 0s - loss: 0.4567 - accuracy: 0.7178 - f1_metric: 0.70 - ETA: 0s - loss: 0.4566 - accuracy: 0.7174 - f1_metric: 0.70 - ETA: 0s - loss: 0.4555 - accuracy: 0.7227 - f1_metric: 0.71 - ETA: 0s - loss: 0.4626 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4618 - accuracy: 0.7207 - f1_metric: 0.71 - ETA: 0s - loss: 0.4644 - accuracy: 0.7165 - f1_metric: 0.70 - ETA: 0s - loss: 0.4651 - accuracy: 0.7151 - f1_metric: 0.7067\n",
      "Epoch 00022: val_f1_metric did not improve from 0.75579\n",
      "44/44 [==============================] - 2s 37ms/step - loss: 0.4649 - accuracy: 0.7159 - f1_metric: 0.7084 - val_loss: 0.8790 - val_accuracy: 0.5997 - val_f1_metric: 0.5859 - lr: 0.0010\n",
      "Epoch 23/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5539 - accuracy: 0.5938 - f1_metric: 0.58 - ETA: 0s - loss: 0.4837 - accuracy: 0.5990 - f1_metric: 0.59 - ETA: 0s - loss: 0.4397 - accuracy: 0.6469 - f1_metric: 0.63 - ETA: 0s - loss: 0.4259 - accuracy: 0.6808 - f1_metric: 0.67 - ETA: 0s - loss: 0.4294 - accuracy: 0.7047 - f1_metric: 0.69 - ETA: 0s - loss: 0.4691 - accuracy: 0.7161 - f1_metric: 0.71 - ETA: 0s - loss: 0.4578 - accuracy: 0.7154 - f1_metric: 0.70 - ETA: 0s - loss: 0.4554 - accuracy: 0.7004 - f1_metric: 0.69 - ETA: 0s - loss: 0.4625 - accuracy: 0.6941 - f1_metric: 0.68 - ETA: 0s - loss: 0.4554 - accuracy: 0.6964 - f1_metric: 0.68 - ETA: 0s - loss: 0.4542 - accuracy: 0.6950 - f1_metric: 0.68 - ETA: 0s - loss: 0.4598 - accuracy: 0.6905 - f1_metric: 0.68 - ETA: 0s - loss: 0.4605 - accuracy: 0.6897 - f1_metric: 0.68 - ETA: 0s - loss: 0.4638 - accuracy: 0.6906 - f1_metric: 0.68 - ETA: 0s - loss: 0.4690 - accuracy: 0.6895 - f1_metric: 0.68 - ETA: 0s - loss: 0.4659 - accuracy: 0.6866 - f1_metric: 0.67 - ETA: 0s - loss: 0.4669 - accuracy: 0.6871 - f1_metric: 0.67 - ETA: 0s - loss: 0.4640 - accuracy: 0.6871 - f1_metric: 0.67 - ETA: 0s - loss: 0.4638 - accuracy: 0.6930 - f1_metric: 0.6836\n",
      "Epoch 00023: val_f1_metric did not improve from 0.75579\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.4633 - accuracy: 0.6929 - f1_metric: 0.6832 - val_loss: 0.6373 - val_accuracy: 0.7312 - val_f1_metric: 0.7248 - lr: 0.0010\n",
      "Epoch 24/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.6174 - accuracy: 0.6562 - f1_metric: 0.66 - ETA: 0s - loss: 0.4769 - accuracy: 0.6510 - f1_metric: 0.64 - ETA: 0s - loss: 0.4372 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4424 - accuracy: 0.7305 - f1_metric: 0.72 - ETA: 0s - loss: 0.4419 - accuracy: 0.7297 - f1_metric: 0.72 - ETA: 0s - loss: 0.4546 - accuracy: 0.7214 - f1_metric: 0.71 - ETA: 0s - loss: 0.4424 - accuracy: 0.7292 - f1_metric: 0.72 - ETA: 0s - loss: 0.4443 - accuracy: 0.7151 - f1_metric: 0.71 - ETA: 0s - loss: 0.4401 - accuracy: 0.7234 - f1_metric: 0.71 - ETA: 0s - loss: 0.4386 - accuracy: 0.7255 - f1_metric: 0.72 - ETA: 0s - loss: 0.4367 - accuracy: 0.7374 - f1_metric: 0.73 - ETA: 0s - loss: 0.4337 - accuracy: 0.7328 - f1_metric: 0.73 - ETA: 0s - loss: 0.4267 - accuracy: 0.7368 - f1_metric: 0.73 - ETA: 0s - loss: 0.4378 - accuracy: 0.7335 - f1_metric: 0.73 - ETA: 0s - loss: 0.4359 - accuracy: 0.7331 - f1_metric: 0.73 - ETA: 0s - loss: 0.4389 - accuracy: 0.7315 - f1_metric: 0.72 - ETA: 0s - loss: 0.4377 - accuracy: 0.7270 - f1_metric: 0.72 - ETA: 0s - loss: 0.4375 - accuracy: 0.7209 - f1_metric: 0.7177\n",
      "Epoch 00024: val_f1_metric improved from 0.75579 to 0.77635, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 0.4354 - accuracy: 0.7220 - f1_metric: 0.7198 - val_loss: 0.5647 - val_accuracy: 0.7772 - val_f1_metric: 0.7764 - lr: 0.0010\n",
      "Epoch 25/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.3867 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.3961 - accuracy: 0.7695 - f1_metric: 0.77 - ETA: 0s - loss: 0.4337 - accuracy: 0.7902 - f1_metric: 0.78 - ETA: 0s - loss: 0.4264 - accuracy: 0.7727 - f1_metric: 0.77 - ETA: 0s - loss: 0.4389 - accuracy: 0.7545 - f1_metric: 0.75 - ETA: 0s - loss: 0.4404 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.4399 - accuracy: 0.7387 - f1_metric: 0.73 - ETA: 0s - loss: 0.4474 - accuracy: 0.7312 - f1_metric: 0.72 - ETA: 0s - loss: 0.4479 - accuracy: 0.7223 - f1_metric: 0.71 - ETA: 0s - loss: 0.4483 - accuracy: 0.7161 - f1_metric: 0.71 - ETA: 0s - loss: 0.4466 - accuracy: 0.7175 - f1_metric: 0.71 - ETA: 0s - loss: 0.4491 - accuracy: 0.7247 - f1_metric: 0.72 - ETA: 0s - loss: 0.4462 - accuracy: 0.7271 - f1_metric: 0.72 - ETA: 0s - loss: 0.4380 - accuracy: 0.7305 - f1_metric: 0.72 - ETA: 0s - loss: 0.4356 - accuracy: 0.7292 - f1_metric: 0.72 - ETA: 0s - loss: 0.4371 - accuracy: 0.7299 - f1_metric: 0.72 - ETA: 0s - loss: 0.4402 - accuracy: 0.7289 - f1_metric: 0.72 - ETA: 0s - loss: 0.4385 - accuracy: 0.7309 - f1_metric: 0.72 - ETA: 0s - loss: 0.4339 - accuracy: 0.7315 - f1_metric: 0.72 - ETA: 0s - loss: 0.4335 - accuracy: 0.7313 - f1_metric: 0.7278\n",
      "Epoch 00025: val_f1_metric did not improve from 0.77635\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.4335 - accuracy: 0.7313 - f1_metric: 0.7278 - val_loss: 0.6343 - val_accuracy: 0.7404 - val_f1_metric: 0.7375 - lr: 0.0010\n",
      "Epoch 26/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3426 - accuracy: 0.7500 - f1_metric: 0.73 - ETA: 0s - loss: 0.4558 - accuracy: 0.7344 - f1_metric: 0.72 - ETA: 0s - loss: 0.4558 - accuracy: 0.6875 - f1_metric: 0.68 - ETA: 0s - loss: 0.4499 - accuracy: 0.7012 - f1_metric: 0.70 - ETA: 0s - loss: 0.4449 - accuracy: 0.7131 - f1_metric: 0.71 - ETA: 0s - loss: 0.4503 - accuracy: 0.7221 - f1_metric: 0.72 - ETA: 0s - loss: 0.4447 - accuracy: 0.7197 - f1_metric: 0.71 - ETA: 0s - loss: 0.4359 - accuracy: 0.7211 - f1_metric: 0.72 - ETA: 0s - loss: 0.4460 - accuracy: 0.7249 - f1_metric: 0.72 - ETA: 0s - loss: 0.4439 - accuracy: 0.7260 - f1_metric: 0.72 - ETA: 0s - loss: 0.4398 - accuracy: 0.7241 - f1_metric: 0.72 - ETA: 0s - loss: 0.4368 - accuracy: 0.7212 - f1_metric: 0.71 - ETA: 0s - loss: 0.4307 - accuracy: 0.7224 - f1_metric: 0.72 - ETA: 0s - loss: 0.4366 - accuracy: 0.7306 - f1_metric: 0.72 - ETA: 0s - loss: 0.4352 - accuracy: 0.7328 - f1_metric: 0.73 - ETA: 0s - loss: 0.4372 - accuracy: 0.7311 - f1_metric: 0.7293\n",
      "Epoch 00026: val_f1_metric did not improve from 0.77635\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.4368 - accuracy: 0.7292 - f1_metric: 0.7260 - val_loss: 0.8231 - val_accuracy: 0.6290 - val_f1_metric: 0.6196 - lr: 0.0010\n",
      "Epoch 27/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4301 - accuracy: 0.5781 - f1_metric: 0.58 - ETA: 0s - loss: 0.4360 - accuracy: 0.6602 - f1_metric: 0.65 - ETA: 0s - loss: 0.4383 - accuracy: 0.7165 - f1_metric: 0.71 - ETA: 0s - loss: 0.4279 - accuracy: 0.7281 - f1_metric: 0.72 - ETA: 0s - loss: 0.4318 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4576 - accuracy: 0.7105 - f1_metric: 0.70 - ETA: 0s - loss: 0.4472 - accuracy: 0.7141 - f1_metric: 0.71 - ETA: 0s - loss: 0.4466 - accuracy: 0.7081 - f1_metric: 0.70 - ETA: 0s - loss: 0.4434 - accuracy: 0.7096 - f1_metric: 0.70 - ETA: 0s - loss: 0.4407 - accuracy: 0.7131 - f1_metric: 0.70 - ETA: 0s - loss: 0.4360 - accuracy: 0.7170 - f1_metric: 0.71 - ETA: 0s - loss: 0.4358 - accuracy: 0.7284 - f1_metric: 0.72 - ETA: 0s - loss: 0.4422 - accuracy: 0.7359 - f1_metric: 0.73 - ETA: 0s - loss: 0.4366 - accuracy: 0.7391 - f1_metric: 0.73 - ETA: 0s - loss: 0.4345 - accuracy: 0.7362 - f1_metric: 0.73 - ETA: 0s - loss: 0.4339 - accuracy: 0.7289 - f1_metric: 0.72 - ETA: 0s - loss: 0.4312 - accuracy: 0.7273 - f1_metric: 0.72 - ETA: 0s - loss: 0.4283 - accuracy: 0.7269 - f1_metric: 0.7219\n",
      "Epoch 00027: val_f1_metric improved from 0.77635 to 0.83508, saving model to .\\best_model_keras_petrobras\n",
      "INFO:tensorflow:Assets written to: .\\best_model_keras_petrobras\\assets\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.4271 - accuracy: 0.7288 - f1_metric: 0.7253 - val_loss: 0.4560 - val_accuracy: 0.8350 - val_f1_metric: 0.8351 - lr: 0.0010\n",
      "Epoch 28/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4316 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.4682 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.4366 - accuracy: 0.7768 - f1_metric: 0.77 - ETA: 0s - loss: 0.4196 - accuracy: 0.7587 - f1_metric: 0.75 - ETA: 0s - loss: 0.4129 - accuracy: 0.7486 - f1_metric: 0.74 - ETA: 0s - loss: 0.4141 - accuracy: 0.7512 - f1_metric: 0.74 - ETA: 0s - loss: 0.4099 - accuracy: 0.7531 - f1_metric: 0.74 - ETA: 0s - loss: 0.4174 - accuracy: 0.7610 - f1_metric: 0.75 - ETA: 0s - loss: 0.4150 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.4171 - accuracy: 0.7671 - f1_metric: 0.76 - ETA: 0s - loss: 0.4133 - accuracy: 0.7602 - f1_metric: 0.75 - ETA: 0s - loss: 0.4139 - accuracy: 0.7488 - f1_metric: 0.74 - ETA: 0s - loss: 0.4159 - accuracy: 0.7477 - f1_metric: 0.74 - ETA: 0s - loss: 0.4178 - accuracy: 0.7484 - f1_metric: 0.74 - ETA: 0s - loss: 0.4158 - accuracy: 0.7460 - f1_metric: 0.74 - ETA: 0s - loss: 0.4187 - accuracy: 0.7528 - f1_metric: 0.74 - ETA: 0s - loss: 0.4243 - accuracy: 0.7526 - f1_metric: 0.74 - ETA: 0s - loss: 0.4226 - accuracy: 0.7516 - f1_metric: 0.74 - ETA: 0s - loss: 0.4272 - accuracy: 0.7410 - f1_metric: 0.73 - ETA: 0s - loss: 0.4289 - accuracy: 0.7359 - f1_metric: 0.7323\n",
      "Epoch 00028: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.4270 - accuracy: 0.7331 - f1_metric: 0.7292 - val_loss: 0.7306 - val_accuracy: 0.6750 - val_f1_metric: 0.6768 - lr: 0.0010\n",
      "Epoch 29/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3839 - accuracy: 0.6562 - f1_metric: 0.66 - ETA: 0s - loss: 0.4706 - accuracy: 0.7083 - f1_metric: 0.71 - ETA: 0s - loss: 0.4148 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.4078 - accuracy: 0.7634 - f1_metric: 0.76 - ETA: 0s - loss: 0.4097 - accuracy: 0.7778 - f1_metric: 0.77 - ETA: 0s - loss: 0.4050 - accuracy: 0.7784 - f1_metric: 0.77 - ETA: 0s - loss: 0.4269 - accuracy: 0.7723 - f1_metric: 0.77 - ETA: 0s - loss: 0.4231 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.4188 - accuracy: 0.7604 - f1_metric: 0.76 - ETA: 0s - loss: 0.4263 - accuracy: 0.7508 - f1_metric: 0.75 - ETA: 0s - loss: 0.4294 - accuracy: 0.7436 - f1_metric: 0.74 - ETA: 0s - loss: 0.4257 - accuracy: 0.7448 - f1_metric: 0.74 - ETA: 0s - loss: 0.4176 - accuracy: 0.7494 - f1_metric: 0.74 - ETA: 0s - loss: 0.4124 - accuracy: 0.7615 - f1_metric: 0.76 - ETA: 0s - loss: 0.4172 - accuracy: 0.7614 - f1_metric: 0.76 - ETA: 0s - loss: 0.4252 - accuracy: 0.7549 - f1_metric: 0.75 - ETA: 0s - loss: 0.4269 - accuracy: 0.7537 - f1_metric: 0.75 - ETA: 0s - loss: 0.4290 - accuracy: 0.7473 - f1_metric: 0.74 - ETA: 0s - loss: 0.4285 - accuracy: 0.7425 - f1_metric: 0.7384\n",
      "Epoch 00029: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.4285 - accuracy: 0.7425 - f1_metric: 0.7384 - val_loss: 0.7426 - val_accuracy: 0.6750 - val_f1_metric: 0.6642 - lr: 0.0010\n",
      "Epoch 30/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4947 - accuracy: 0.6719 - f1_metric: 0.65 - ETA: 0s - loss: 0.4388 - accuracy: 0.6758 - f1_metric: 0.66 - ETA: 0s - loss: 0.4356 - accuracy: 0.7370 - f1_metric: 0.73 - ETA: 0s - loss: 0.4393 - accuracy: 0.7480 - f1_metric: 0.74 - ETA: 0s - loss: 0.4200 - accuracy: 0.7571 - f1_metric: 0.75 - ETA: 0s - loss: 0.4371 - accuracy: 0.7488 - f1_metric: 0.74 - ETA: 0s - loss: 0.4420 - accuracy: 0.7469 - f1_metric: 0.74 - ETA: 0s - loss: 0.4408 - accuracy: 0.7390 - f1_metric: 0.73 - ETA: 0s - loss: 0.4440 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.4357 - accuracy: 0.7372 - f1_metric: 0.73 - ETA: 0s - loss: 0.4256 - accuracy: 0.7415 - f1_metric: 0.73 - ETA: 0s - loss: 0.4247 - accuracy: 0.7452 - f1_metric: 0.74 - ETA: 0s - loss: 0.4187 - accuracy: 0.7461 - f1_metric: 0.74 - ETA: 0s - loss: 0.4135 - accuracy: 0.7535 - f1_metric: 0.75 - ETA: 0s - loss: 0.4265 - accuracy: 0.7555 - f1_metric: 0.75 - ETA: 0s - loss: 0.4262 - accuracy: 0.7516 - f1_metric: 0.74 - ETA: 0s - loss: 0.4265 - accuracy: 0.7437 - f1_metric: 0.7403\n",
      "Epoch 00030: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.4275 - accuracy: 0.7414 - f1_metric: 0.7375 - val_loss: 0.6669 - val_accuracy: 0.7320 - val_f1_metric: 0.7322 - lr: 0.0010\n",
      "Epoch 31/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3781 - accuracy: 0.6719 - f1_metric: 0.67 - ETA: 0s - loss: 0.4980 - accuracy: 0.7292 - f1_metric: 0.72 - ETA: 0s - loss: 0.4387 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.4335 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.4437 - accuracy: 0.7516 - f1_metric: 0.75 - ETA: 0s - loss: 0.4544 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.4502 - accuracy: 0.7355 - f1_metric: 0.73 - ETA: 0s - loss: 0.4516 - accuracy: 0.7285 - f1_metric: 0.72 - ETA: 0s - loss: 0.4558 - accuracy: 0.7270 - f1_metric: 0.72 - ETA: 0s - loss: 0.4571 - accuracy: 0.7113 - f1_metric: 0.70 - ETA: 0s - loss: 0.4492 - accuracy: 0.7116 - f1_metric: 0.70 - ETA: 0s - loss: 0.4435 - accuracy: 0.7148 - f1_metric: 0.70 - ETA: 0s - loss: 0.4428 - accuracy: 0.7205 - f1_metric: 0.71 - ETA: 0s - loss: 0.4345 - accuracy: 0.7240 - f1_metric: 0.71 - ETA: 0s - loss: 0.4376 - accuracy: 0.7207 - f1_metric: 0.71 - ETA: 0s - loss: 0.4353 - accuracy: 0.7233 - f1_metric: 0.71 - ETA: 0s - loss: 0.4297 - accuracy: 0.7276 - f1_metric: 0.72 - ETA: 0s - loss: 0.4249 - accuracy: 0.7309 - f1_metric: 0.72 - ETA: 0s - loss: 0.4245 - accuracy: 0.7318 - f1_metric: 0.7282\n",
      "Epoch 00031: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.4225 - accuracy: 0.7320 - f1_metric: 0.7287 - val_loss: 0.5507 - val_accuracy: 0.7789 - val_f1_metric: 0.7796 - lr: 0.0010\n",
      "Epoch 32/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3202 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.3699 - accuracy: 0.7760 - f1_metric: 0.77 - ETA: 0s - loss: 0.3831 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3858 - accuracy: 0.7930 - f1_metric: 0.79 - ETA: 0s - loss: 0.3881 - accuracy: 0.7891 - f1_metric: 0.78 - ETA: 0s - loss: 0.4426 - accuracy: 0.7620 - f1_metric: 0.76 - ETA: 0s - loss: 0.4425 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.4408 - accuracy: 0.7352 - f1_metric: 0.73 - ETA: 0s - loss: 0.4482 - accuracy: 0.7227 - f1_metric: 0.71 - ETA: 0s - loss: 0.4432 - accuracy: 0.7223 - f1_metric: 0.71 - ETA: 0s - loss: 0.4436 - accuracy: 0.7244 - f1_metric: 0.71 - ETA: 0s - loss: 0.4490 - accuracy: 0.7245 - f1_metric: 0.71 - ETA: 0s - loss: 0.4409 - accuracy: 0.7328 - f1_metric: 0.72 - ETA: 0s - loss: 0.4402 - accuracy: 0.7334 - f1_metric: 0.72 - ETA: 0s - loss: 0.4336 - accuracy: 0.7371 - f1_metric: 0.73 - ETA: 0s - loss: 0.4288 - accuracy: 0.7420 - f1_metric: 0.73 - ETA: 0s - loss: 0.4249 - accuracy: 0.7453 - f1_metric: 0.74 - ETA: 0s - loss: 0.4273 - accuracy: 0.7459 - f1_metric: 0.7415\n",
      "Epoch 00032: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.4264 - accuracy: 0.7443 - f1_metric: 0.7414 - val_loss: 0.5977 - val_accuracy: 0.7571 - val_f1_metric: 0.7542 - lr: 0.0010\n",
      "Epoch 33/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3286 - accuracy: 0.7865 - f1_metric: 0.78 - ETA: 0s - loss: 0.4286 - accuracy: 0.7474 - f1_metric: 0.74 - ETA: 0s - loss: 0.4236 - accuracy: 0.7285 - f1_metric: 0.72 - ETA: 0s - loss: 0.4313 - accuracy: 0.7045 - f1_metric: 0.70 - ETA: 0s - loss: 0.4187 - accuracy: 0.7020 - f1_metric: 0.69 - ETA: 0s - loss: 0.4081 - accuracy: 0.7206 - f1_metric: 0.71 - ETA: 0s - loss: 0.4114 - accuracy: 0.7422 - f1_metric: 0.73 - ETA: 0s - loss: 0.4028 - accuracy: 0.7521 - f1_metric: 0.75 - ETA: 0s - loss: 0.4225 - accuracy: 0.7533 - f1_metric: 0.75 - ETA: 0s - loss: 0.4161 - accuracy: 0.7483 - f1_metric: 0.74 - ETA: 0s - loss: 0.4140 - accuracy: 0.7398 - f1_metric: 0.73 - ETA: 0s - loss: 0.4226 - accuracy: 0.7308 - f1_metric: 0.72 - ETA: 0s - loss: 0.4214 - accuracy: 0.7302 - f1_metric: 0.72 - ETA: 0s - loss: 0.4247 - accuracy: 0.7287 - f1_metric: 0.72 - ETA: 0s - loss: 0.4278 - accuracy: 0.7303 - f1_metric: 0.72 - ETA: 0s - loss: 0.4218 - accuracy: 0.7305 - f1_metric: 0.72 - ETA: 0s - loss: 0.4279 - accuracy: 0.7306 - f1_metric: 0.72 - ETA: 0s - loss: 0.4259 - accuracy: 0.7329 - f1_metric: 0.7293\n",
      "Epoch 00033: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.4270 - accuracy: 0.7317 - f1_metric: 0.7259 - val_loss: 0.6671 - val_accuracy: 0.7111 - val_f1_metric: 0.7111 - lr: 0.0010\n",
      "Epoch 34/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4322 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.4228 - accuracy: 0.7552 - f1_metric: 0.75 - ETA: 0s - loss: 0.3796 - accuracy: 0.7563 - f1_metric: 0.75 - ETA: 0s - loss: 0.3944 - accuracy: 0.7746 - f1_metric: 0.77 - ETA: 0s - loss: 0.3901 - accuracy: 0.7437 - f1_metric: 0.74 - ETA: 0s - loss: 0.3798 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.3874 - accuracy: 0.7625 - f1_metric: 0.76 - ETA: 0s - loss: 0.3718 - accuracy: 0.7760 - f1_metric: 0.77 - ETA: 0s - loss: 0.3806 - accuracy: 0.7742 - f1_metric: 0.77 - ETA: 0s - loss: 0.3804 - accuracy: 0.7791 - f1_metric: 0.77 - ETA: 0s - loss: 0.3770 - accuracy: 0.7832 - f1_metric: 0.78 - ETA: 0s - loss: 0.3804 - accuracy: 0.7760 - f1_metric: 0.77 - ETA: 0s - loss: 0.3799 - accuracy: 0.7716 - f1_metric: 0.76 - ETA: 0s - loss: 0.3805 - accuracy: 0.7692 - f1_metric: 0.76 - ETA: 0s - loss: 0.3837 - accuracy: 0.7661 - f1_metric: 0.76 - ETA: 0s - loss: 0.3914 - accuracy: 0.7669 - f1_metric: 0.76 - ETA: 0s - loss: 0.3988 - accuracy: 0.7648 - f1_metric: 0.76 - ETA: 0s - loss: 0.3983 - accuracy: 0.7604 - f1_metric: 0.7585\n",
      "Epoch 00034: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.3985 - accuracy: 0.7572 - f1_metric: 0.7548 - val_loss: 0.7506 - val_accuracy: 0.6692 - val_f1_metric: 0.6682 - lr: 0.0010\n",
      "Epoch 35/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4152 - accuracy: 0.7031 - f1_metric: 0.69 - ETA: 0s - loss: 0.4730 - accuracy: 0.6445 - f1_metric: 0.63 - ETA: 0s - loss: 0.4109 - accuracy: 0.7121 - f1_metric: 0.70 - ETA: 0s - loss: 0.4093 - accuracy: 0.7375 - f1_metric: 0.73 - ETA: 0s - loss: 0.3994 - accuracy: 0.7474 - f1_metric: 0.74 - ETA: 0s - loss: 0.3938 - accuracy: 0.7567 - f1_metric: 0.75 - ETA: 0s - loss: 0.3930 - accuracy: 0.7684 - f1_metric: 0.76 - ETA: 0s - loss: 0.3858 - accuracy: 0.7680 - f1_metric: 0.76 - ETA: 0s - loss: 0.3822 - accuracy: 0.7649 - f1_metric: 0.76 - ETA: 0s - loss: 0.3804 - accuracy: 0.7644 - f1_metric: 0.76 - ETA: 0s - loss: 0.3852 - accuracy: 0.7639 - f1_metric: 0.76 - ETA: 0s - loss: 0.3842 - accuracy: 0.7667 - f1_metric: 0.76 - ETA: 0s - loss: 0.3860 - accuracy: 0.7690 - f1_metric: 0.76 - ETA: 0s - loss: 0.3846 - accuracy: 0.7710 - f1_metric: 0.76 - ETA: 0s - loss: 0.3869 - accuracy: 0.7734 - f1_metric: 0.77 - ETA: 0s - loss: 0.3875 - accuracy: 0.7691 - f1_metric: 0.76 - ETA: 0s - loss: 0.3914 - accuracy: 0.7638 - f1_metric: 0.7615\n",
      "Epoch 00035: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3912 - accuracy: 0.7626 - f1_metric: 0.7611 - val_loss: 0.7511 - val_accuracy: 0.6859 - val_f1_metric: 0.6799 - lr: 0.0010\n",
      "Epoch 36/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.5144 - accuracy: 0.5156 - f1_metric: 0.51 - ETA: 0s - loss: 0.4575 - accuracy: 0.5729 - f1_metric: 0.57 - ETA: 0s - loss: 0.4541 - accuracy: 0.6432 - f1_metric: 0.64 - ETA: 0s - loss: 0.4499 - accuracy: 0.6816 - f1_metric: 0.67 - ETA: 0s - loss: 0.4317 - accuracy: 0.7109 - f1_metric: 0.70 - ETA: 0s - loss: 0.4424 - accuracy: 0.7148 - f1_metric: 0.71 - ETA: 0s - loss: 0.4356 - accuracy: 0.7199 - f1_metric: 0.71 - ETA: 0s - loss: 0.4344 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4198 - accuracy: 0.7220 - f1_metric: 0.71 - ETA: 0s - loss: 0.4123 - accuracy: 0.7299 - f1_metric: 0.72 - ETA: 0s - loss: 0.4001 - accuracy: 0.7389 - f1_metric: 0.73 - ETA: 0s - loss: 0.4020 - accuracy: 0.7440 - f1_metric: 0.74 - ETA: 0s - loss: 0.3995 - accuracy: 0.7484 - f1_metric: 0.74 - ETA: 0s - loss: 0.3945 - accuracy: 0.7465 - f1_metric: 0.74 - ETA: 0s - loss: 0.3890 - accuracy: 0.7486 - f1_metric: 0.74 - ETA: 0s - loss: 0.3855 - accuracy: 0.7483 - f1_metric: 0.74 - ETA: 0s - loss: 0.3837 - accuracy: 0.7488 - f1_metric: 0.74 - ETA: 0s - loss: 0.3866 - accuracy: 0.7511 - f1_metric: 0.74 - ETA: 0s - loss: 0.3863 - accuracy: 0.7573 - f1_metric: 0.7544\n",
      "Epoch 00036: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3844 - accuracy: 0.7575 - f1_metric: 0.7550 - val_loss: 0.5296 - val_accuracy: 0.7873 - val_f1_metric: 0.7853 - lr: 0.0010\n",
      "Epoch 37/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5736 - accuracy: 0.6562 - f1_metric: 0.65 - ETA: 0s - loss: 0.4142 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.3958 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.3588 - accuracy: 0.7701 - f1_metric: 0.77 - ETA: 0s - loss: 0.3565 - accuracy: 0.7797 - f1_metric: 0.78 - ETA: 0s - loss: 0.3654 - accuracy: 0.7885 - f1_metric: 0.78 - ETA: 0s - loss: 0.3628 - accuracy: 0.7896 - f1_metric: 0.79 - ETA: 0s - loss: 0.3664 - accuracy: 0.7923 - f1_metric: 0.79 - ETA: 0s - loss: 0.3639 - accuracy: 0.7914 - f1_metric: 0.79 - ETA: 0s - loss: 0.3692 - accuracy: 0.7869 - f1_metric: 0.78 - ETA: 0s - loss: 0.3687 - accuracy: 0.7881 - f1_metric: 0.78 - ETA: 0s - loss: 0.3643 - accuracy: 0.7870 - f1_metric: 0.78 - ETA: 0s - loss: 0.3661 - accuracy: 0.7807 - f1_metric: 0.78 - ETA: 0s - loss: 0.3711 - accuracy: 0.7886 - f1_metric: 0.78 - ETA: 0s - loss: 0.3720 - accuracy: 0.7915 - f1_metric: 0.79 - ETA: 0s - loss: 0.3757 - accuracy: 0.7884 - f1_metric: 0.78 - ETA: 0s - loss: 0.3772 - accuracy: 0.7800 - f1_metric: 0.77 - ETA: 0s - loss: 0.3774 - accuracy: 0.7775 - f1_metric: 0.7763\n",
      "Epoch 00037: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3747 - accuracy: 0.7784 - f1_metric: 0.7781 - val_loss: 0.5882 - val_accuracy: 0.7630 - val_f1_metric: 0.7616 - lr: 0.0010\n",
      "Epoch 38/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3390 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.3642 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3462 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3411 - accuracy: 0.8214 - f1_metric: 0.82 - ETA: 0s - loss: 0.3665 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3793 - accuracy: 0.8054 - f1_metric: 0.80 - ETA: 0s - loss: 0.3810 - accuracy: 0.7768 - f1_metric: 0.77 - ETA: 0s - loss: 0.3819 - accuracy: 0.7705 - f1_metric: 0.77 - ETA: 0s - loss: 0.3860 - accuracy: 0.7648 - f1_metric: 0.76 - ETA: 0s - loss: 0.3830 - accuracy: 0.7609 - f1_metric: 0.75 - ETA: 0s - loss: 0.3863 - accuracy: 0.7599 - f1_metric: 0.75 - ETA: 0s - loss: 0.3840 - accuracy: 0.7611 - f1_metric: 0.75 - ETA: 0s - loss: 0.3803 - accuracy: 0.7668 - f1_metric: 0.76 - ETA: 0s - loss: 0.3833 - accuracy: 0.7667 - f1_metric: 0.76 - ETA: 0s - loss: 0.3851 - accuracy: 0.7631 - f1_metric: 0.76 - ETA: 0s - loss: 0.3849 - accuracy: 0.7685 - f1_metric: 0.76 - ETA: 0s - loss: 0.3878 - accuracy: 0.7688 - f1_metric: 0.76 - ETA: 0s - loss: 0.3899 - accuracy: 0.7660 - f1_metric: 0.76 - ETA: 0s - loss: 0.3903 - accuracy: 0.7620 - f1_metric: 0.76 - ETA: 0s - loss: 0.3880 - accuracy: 0.7622 - f1_metric: 0.76 - ETA: 0s - loss: 0.3878 - accuracy: 0.7613 - f1_metric: 0.7602\n",
      "Epoch 00038: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.3866 - accuracy: 0.7622 - f1_metric: 0.7621 - val_loss: 0.4955 - val_accuracy: 0.8166 - val_f1_metric: 0.8141 - lr: 0.0010\n",
      "Epoch 39/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2258 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.4096 - accuracy: 0.8320 - f1_metric: 0.82 - ETA: 0s - loss: 0.4121 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3940 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.4062 - accuracy: 0.7604 - f1_metric: 0.75 - ETA: 0s - loss: 0.4071 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.4005 - accuracy: 0.7436 - f1_metric: 0.73 - ETA: 0s - loss: 0.3979 - accuracy: 0.7492 - f1_metric: 0.74 - ETA: 0s - loss: 0.3914 - accuracy: 0.7585 - f1_metric: 0.75 - ETA: 0s - loss: 0.3950 - accuracy: 0.7598 - f1_metric: 0.75 - ETA: 0s - loss: 0.3981 - accuracy: 0.7630 - f1_metric: 0.75 - ETA: 0s - loss: 0.3938 - accuracy: 0.7637 - f1_metric: 0.75 - ETA: 0s - loss: 0.3928 - accuracy: 0.7626 - f1_metric: 0.75 - ETA: 0s - loss: 0.3920 - accuracy: 0.7640 - f1_metric: 0.76 - ETA: 0s - loss: 0.3928 - accuracy: 0.7633 - f1_metric: 0.75 - ETA: 0s - loss: 0.3927 - accuracy: 0.7598 - f1_metric: 0.7564\n",
      "Epoch 00039: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3924 - accuracy: 0.7601 - f1_metric: 0.7570 - val_loss: 0.5262 - val_accuracy: 0.7923 - val_f1_metric: 0.7934 - lr: 0.0010\n",
      "Epoch 40/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2594 - accuracy: 0.8672 - f1_metric: 0.86 - ETA: 0s - loss: 0.3279 - accuracy: 0.8638 - f1_metric: 0.86 - ETA: 0s - loss: 0.3356 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3547 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3727 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.3872 - accuracy: 0.7837 - f1_metric: 0.78 - ETA: 0s - loss: 0.3985 - accuracy: 0.7607 - f1_metric: 0.76 - ETA: 0s - loss: 0.4017 - accuracy: 0.7525 - f1_metric: 0.75 - ETA: 0s - loss: 0.4017 - accuracy: 0.7556 - f1_metric: 0.75 - ETA: 0s - loss: 0.4022 - accuracy: 0.7606 - f1_metric: 0.76 - ETA: 0s - loss: 0.4058 - accuracy: 0.7576 - f1_metric: 0.75 - ETA: 0s - loss: 0.4002 - accuracy: 0.7561 - f1_metric: 0.75 - ETA: 0s - loss: 0.3979 - accuracy: 0.7545 - f1_metric: 0.75 - ETA: 0s - loss: 0.3985 - accuracy: 0.7530 - f1_metric: 0.75 - ETA: 0s - loss: 0.3981 - accuracy: 0.7518 - f1_metric: 0.7516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00040: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3974 - accuracy: 0.7518 - f1_metric: 0.7515 - val_loss: 0.5825 - val_accuracy: 0.7521 - val_f1_metric: 0.7541 - lr: 0.0010\n",
      "Epoch 41/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.6875 - f1_metric: 0.68 - ETA: 0s - loss: 0.3456 - accuracy: 0.7708 - f1_metric: 0.77 - ETA: 0s - loss: 0.4023 - accuracy: 0.7688 - f1_metric: 0.76 - ETA: 0s - loss: 0.4397 - accuracy: 0.7441 - f1_metric: 0.74 - ETA: 0s - loss: 0.4380 - accuracy: 0.7358 - f1_metric: 0.73 - ETA: 0s - loss: 0.4388 - accuracy: 0.7355 - f1_metric: 0.73 - ETA: 0s - loss: 0.4340 - accuracy: 0.7275 - f1_metric: 0.72 - ETA: 0s - loss: 0.4183 - accuracy: 0.7294 - f1_metric: 0.72 - ETA: 0s - loss: 0.4034 - accuracy: 0.7436 - f1_metric: 0.74 - ETA: 0s - loss: 0.4183 - accuracy: 0.7467 - f1_metric: 0.74 - ETA: 0s - loss: 0.4203 - accuracy: 0.7518 - f1_metric: 0.75 - ETA: 0s - loss: 0.4137 - accuracy: 0.7527 - f1_metric: 0.75 - ETA: 0s - loss: 0.4130 - accuracy: 0.7495 - f1_metric: 0.74 - ETA: 0s - loss: 0.4106 - accuracy: 0.7509 - f1_metric: 0.74 - ETA: 0s - loss: 0.4114 - accuracy: 0.7482 - f1_metric: 0.74 - ETA: 0s - loss: 0.4086 - accuracy: 0.7463 - f1_metric: 0.74 - ETA: 0s - loss: 0.4106 - accuracy: 0.7443 - f1_metric: 0.74 - ETA: 0s - loss: 0.4078 - accuracy: 0.7478 - f1_metric: 0.7464\n",
      "Epoch 00041: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.4083 - accuracy: 0.7471 - f1_metric: 0.7453 - val_loss: 0.5238 - val_accuracy: 0.7864 - val_f1_metric: 0.7884 - lr: 0.0010\n",
      "Epoch 42/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.3887 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3686 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.3622 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.3736 - accuracy: 0.7832 - f1_metric: 0.78 - ETA: 0s - loss: 0.3877 - accuracy: 0.7869 - f1_metric: 0.78 - ETA: 0s - loss: 0.3754 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3744 - accuracy: 0.7719 - f1_metric: 0.77 - ETA: 0s - loss: 0.3751 - accuracy: 0.7629 - f1_metric: 0.76 - ETA: 0s - loss: 0.3781 - accuracy: 0.7599 - f1_metric: 0.75 - ETA: 0s - loss: 0.3769 - accuracy: 0.7664 - f1_metric: 0.76 - ETA: 0s - loss: 0.3711 - accuracy: 0.7717 - f1_metric: 0.77 - ETA: 0s - loss: 0.3788 - accuracy: 0.7750 - f1_metric: 0.77 - ETA: 0s - loss: 0.3783 - accuracy: 0.7749 - f1_metric: 0.77 - ETA: 0s - loss: 0.3844 - accuracy: 0.7693 - f1_metric: 0.76 - ETA: 0s - loss: 0.3830 - accuracy: 0.7681 - f1_metric: 0.76 - ETA: 0s - loss: 0.3803 - accuracy: 0.7661 - f1_metric: 0.76 - ETA: 0s - loss: 0.3773 - accuracy: 0.7664 - f1_metric: 0.76 - ETA: 0s - loss: 0.3767 - accuracy: 0.7679 - f1_metric: 0.7674\n",
      "Epoch 00042: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3752 - accuracy: 0.7705 - f1_metric: 0.7709 - val_loss: 0.4675 - val_accuracy: 0.8233 - val_f1_metric: 0.8222 - lr: 0.0010\n",
      "Epoch 43/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3244 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3347 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3410 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3456 - accuracy: 0.8203 - f1_metric: 0.81 - ETA: 0s - loss: 0.3359 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3509 - accuracy: 0.8146 - f1_metric: 0.81 - ETA: 0s - loss: 0.3462 - accuracy: 0.8134 - f1_metric: 0.81 - ETA: 0s - loss: 0.3456 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3442 - accuracy: 0.8050 - f1_metric: 0.80 - ETA: 0s - loss: 0.3439 - accuracy: 0.8012 - f1_metric: 0.79 - ETA: 0s - loss: 0.3492 - accuracy: 0.7919 - f1_metric: 0.79 - ETA: 0s - loss: 0.3501 - accuracy: 0.7953 - f1_metric: 0.79 - ETA: 0s - loss: 0.3497 - accuracy: 0.7939 - f1_metric: 0.79 - ETA: 0s - loss: 0.3492 - accuracy: 0.7920 - f1_metric: 0.79 - ETA: 0s - loss: 0.3488 - accuracy: 0.7936 - f1_metric: 0.79 - ETA: 0s - loss: 0.3517 - accuracy: 0.7923 - f1_metric: 0.79 - ETA: 0s - loss: 0.3554 - accuracy: 0.7877 - f1_metric: 0.7838\n",
      "Epoch 00043: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3554 - accuracy: 0.7877 - f1_metric: 0.7838 - val_loss: 0.6290 - val_accuracy: 0.7404 - val_f1_metric: 0.7386 - lr: 0.0010\n",
      "Epoch 44/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2714 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.3181 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3247 - accuracy: 0.7844 - f1_metric: 0.78 - ETA: 0s - loss: 0.3440 - accuracy: 0.7793 - f1_metric: 0.77 - ETA: 0s - loss: 0.3625 - accuracy: 0.7869 - f1_metric: 0.78 - ETA: 0s - loss: 0.3453 - accuracy: 0.7924 - f1_metric: 0.79 - ETA: 0s - loss: 0.3458 - accuracy: 0.7941 - f1_metric: 0.79 - ETA: 0s - loss: 0.3472 - accuracy: 0.7977 - f1_metric: 0.79 - ETA: 0s - loss: 0.3475 - accuracy: 0.7990 - f1_metric: 0.79 - ETA: 0s - loss: 0.3423 - accuracy: 0.8034 - f1_metric: 0.80 - ETA: 0s - loss: 0.3519 - accuracy: 0.7992 - f1_metric: 0.79 - ETA: 0s - loss: 0.3532 - accuracy: 0.7979 - f1_metric: 0.79 - ETA: 0s - loss: 0.3538 - accuracy: 0.7935 - f1_metric: 0.79 - ETA: 0s - loss: 0.3539 - accuracy: 0.7875 - f1_metric: 0.78 - ETA: 0s - loss: 0.3544 - accuracy: 0.7833 - f1_metric: 0.78 - ETA: 0s - loss: 0.3530 - accuracy: 0.7863 - f1_metric: 0.78 - ETA: 0s - loss: 0.3597 - accuracy: 0.7885 - f1_metric: 0.7866\n",
      "Epoch 00044: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3599 - accuracy: 0.7884 - f1_metric: 0.7864 - val_loss: 0.4661 - val_accuracy: 0.8250 - val_f1_metric: 0.8246 - lr: 0.0010\n",
      "Epoch 45/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.4062 - accuracy: 0.7708 - f1_metric: 0.77 - ETA: 0s - loss: 0.3955 - accuracy: 0.7688 - f1_metric: 0.76 - ETA: 0s - loss: 0.4010 - accuracy: 0.7207 - f1_metric: 0.72 - ETA: 0s - loss: 0.3877 - accuracy: 0.7301 - f1_metric: 0.72 - ETA: 0s - loss: 0.3818 - accuracy: 0.7224 - f1_metric: 0.72 - ETA: 0s - loss: 0.3862 - accuracy: 0.7363 - f1_metric: 0.73 - ETA: 0s - loss: 0.3911 - accuracy: 0.7467 - f1_metric: 0.74 - ETA: 0s - loss: 0.3977 - accuracy: 0.7436 - f1_metric: 0.74 - ETA: 0s - loss: 0.3939 - accuracy: 0.7467 - f1_metric: 0.74 - ETA: 0s - loss: 0.3889 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.3836 - accuracy: 0.7484 - f1_metric: 0.74 - ETA: 0s - loss: 0.3842 - accuracy: 0.7505 - f1_metric: 0.74 - ETA: 0s - loss: 0.3776 - accuracy: 0.7569 - f1_metric: 0.75 - ETA: 0s - loss: 0.3738 - accuracy: 0.7615 - f1_metric: 0.76 - ETA: 0s - loss: 0.3760 - accuracy: 0.7687 - f1_metric: 0.76 - ETA: 0s - loss: 0.3748 - accuracy: 0.7669 - f1_metric: 0.7641\n",
      "Epoch 00045: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3748 - accuracy: 0.7669 - f1_metric: 0.7641 - val_loss: 0.5710 - val_accuracy: 0.7705 - val_f1_metric: 0.7713 - lr: 0.0010\n",
      "Epoch 46/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.4955 - accuracy: 0.7188 - f1_metric: 0.71 - ETA: 0s - loss: 0.4061 - accuracy: 0.7227 - f1_metric: 0.72 - ETA: 0s - loss: 0.3765 - accuracy: 0.7433 - f1_metric: 0.74 - ETA: 0s - loss: 0.3633 - accuracy: 0.7517 - f1_metric: 0.75 - ETA: 0s - loss: 0.3734 - accuracy: 0.7708 - f1_metric: 0.77 - ETA: 0s - loss: 0.3820 - accuracy: 0.7779 - f1_metric: 0.77 - ETA: 0s - loss: 0.3721 - accuracy: 0.7793 - f1_metric: 0.77 - ETA: 0s - loss: 0.3784 - accuracy: 0.7714 - f1_metric: 0.77 - ETA: 0s - loss: 0.3780 - accuracy: 0.7671 - f1_metric: 0.76 - ETA: 0s - loss: 0.3694 - accuracy: 0.7715 - f1_metric: 0.77 - ETA: 0s - loss: 0.3720 - accuracy: 0.7772 - f1_metric: 0.77 - ETA: 0s - loss: 0.3742 - accuracy: 0.7766 - f1_metric: 0.77 - ETA: 0s - loss: 0.3732 - accuracy: 0.7741 - f1_metric: 0.77 - ETA: 0s - loss: 0.3693 - accuracy: 0.7741 - f1_metric: 0.77 - ETA: 0s - loss: 0.3697 - accuracy: 0.7714 - f1_metric: 0.77 - ETA: 0s - loss: 0.3684 - accuracy: 0.7736 - f1_metric: 0.77 - ETA: 0s - loss: 0.3643 - accuracy: 0.7773 - f1_metric: 0.7789\n",
      "Epoch 00046: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3643 - accuracy: 0.7773 - f1_metric: 0.7789 - val_loss: 0.4653 - val_accuracy: 0.8250 - val_f1_metric: 0.8242 - lr: 0.0010\n",
      "Epoch 47/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.4807 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.7679 - f1_metric: 0.77 - ETA: 0s - loss: 0.4214 - accuracy: 0.7431 - f1_metric: 0.74 - ETA: 0s - loss: 0.3992 - accuracy: 0.7422 - f1_metric: 0.74 - ETA: 0s - loss: 0.3893 - accuracy: 0.7467 - f1_metric: 0.74 - ETA: 0s - loss: 0.3924 - accuracy: 0.7509 - f1_metric: 0.75 - ETA: 0s - loss: 0.4026 - accuracy: 0.7563 - f1_metric: 0.75 - ETA: 0s - loss: 0.4079 - accuracy: 0.7609 - f1_metric: 0.76 - ETA: 0s - loss: 0.4070 - accuracy: 0.7554 - f1_metric: 0.75 - ETA: 0s - loss: 0.4106 - accuracy: 0.7473 - f1_metric: 0.74 - ETA: 0s - loss: 0.4054 - accuracy: 0.7402 - f1_metric: 0.73 - ETA: 0s - loss: 0.4005 - accuracy: 0.7451 - f1_metric: 0.74 - ETA: 0s - loss: 0.3947 - accuracy: 0.7541 - f1_metric: 0.75 - ETA: 0s - loss: 0.3913 - accuracy: 0.7599 - f1_metric: 0.75 - ETA: 0s - loss: 0.3907 - accuracy: 0.7654 - f1_metric: 0.7656\n",
      "Epoch 00047: val_f1_metric did not improve from 0.83508\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3907 - accuracy: 0.7654 - f1_metric: 0.7656 - val_loss: 0.5801 - val_accuracy: 0.7504 - val_f1_metric: 0.7512 - lr: 0.0010\n",
      "Epoch 48/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4205 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3465 - accuracy: 0.7695 - f1_metric: 0.76 - ETA: 0s - loss: 0.3804 - accuracy: 0.7760 - f1_metric: 0.77 - ETA: 0s - loss: 0.3676 - accuracy: 0.7715 - f1_metric: 0.77 - ETA: 0s - loss: 0.3550 - accuracy: 0.7699 - f1_metric: 0.76 - ETA: 0s - loss: 0.3582 - accuracy: 0.7716 - f1_metric: 0.77 - ETA: 0s - loss: 0.3718 - accuracy: 0.7688 - f1_metric: 0.76 - ETA: 0s - loss: 0.3637 - accuracy: 0.7757 - f1_metric: 0.77 - ETA: 0s - loss: 0.3591 - accuracy: 0.7789 - f1_metric: 0.77 - ETA: 0s - loss: 0.3559 - accuracy: 0.7784 - f1_metric: 0.77 - ETA: 0s - loss: 0.3532 - accuracy: 0.7775 - f1_metric: 0.77 - ETA: 0s - loss: 0.3494 - accuracy: 0.7789 - f1_metric: 0.77 - ETA: 0s - loss: 0.3480 - accuracy: 0.7823 - f1_metric: 0.78 - ETA: 0s - loss: 0.3456 - accuracy: 0.7843 - f1_metric: 0.78 - ETA: 0s - loss: 0.3510 - accuracy: 0.7826 - f1_metric: 0.78 - ETA: 0s - loss: 0.3509 - accuracy: 0.7852 - f1_metric: 0.78 - ETA: 0s - loss: 0.3497 - accuracy: 0.7882 - f1_metric: 0.78 - ETA: 0s - loss: 0.3519 - accuracy: 0.7898 - f1_metric: 0.78 - ETA: 0s - loss: 0.3491 - accuracy: 0.7935 - f1_metric: 0.7935\n",
      "Epoch 00048: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3498 - accuracy: 0.7938 - f1_metric: 0.7943 - val_loss: 0.5407 - val_accuracy: 0.7856 - val_f1_metric: 0.7863 - lr: 1.0000e-04\n",
      "Epoch 49/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.2605 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3029 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.3090 - accuracy: 0.8000 - f1_metric: 0.80 - ETA: 0s - loss: 0.3051 - accuracy: 0.7879 - f1_metric: 0.78 - ETA: 0s - loss: 0.3075 - accuracy: 0.7875 - f1_metric: 0.78 - ETA: 0s - loss: 0.3302 - accuracy: 0.7897 - f1_metric: 0.78 - ETA: 0s - loss: 0.3310 - accuracy: 0.7881 - f1_metric: 0.78 - ETA: 0s - loss: 0.3266 - accuracy: 0.7925 - f1_metric: 0.79 - ETA: 0s - loss: 0.3209 - accuracy: 0.7939 - f1_metric: 0.79 - ETA: 0s - loss: 0.3194 - accuracy: 0.7901 - f1_metric: 0.79 - ETA: 0s - loss: 0.3381 - accuracy: 0.7909 - f1_metric: 0.79 - ETA: 0s - loss: 0.3376 - accuracy: 0.7932 - f1_metric: 0.79 - ETA: 0s - loss: 0.3399 - accuracy: 0.7945 - f1_metric: 0.79 - ETA: 0s - loss: 0.3367 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3343 - accuracy: 0.7933 - f1_metric: 0.79 - ETA: 0s - loss: 0.3395 - accuracy: 0.7950 - f1_metric: 0.7946\n",
      "Epoch 00049: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3375 - accuracy: 0.7942 - f1_metric: 0.7930 - val_loss: 0.5362 - val_accuracy: 0.7923 - val_f1_metric: 0.7900 - lr: 1.0000e-04\n",
      "Epoch 50/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4559 - accuracy: 0.7500 - f1_metric: 0.74 - ETA: 0s - loss: 0.3686 - accuracy: 0.7617 - f1_metric: 0.75 - ETA: 0s - loss: 0.3546 - accuracy: 0.7790 - f1_metric: 0.77 - ETA: 0s - loss: 0.3539 - accuracy: 0.7984 - f1_metric: 0.79 - ETA: 0s - loss: 0.3524 - accuracy: 0.8008 - f1_metric: 0.79 - ETA: 0s - loss: 0.3497 - accuracy: 0.8031 - f1_metric: 0.80 - ETA: 0s - loss: 0.3491 - accuracy: 0.7987 - f1_metric: 0.79 - ETA: 0s - loss: 0.3460 - accuracy: 0.7953 - f1_metric: 0.79 - ETA: 0s - loss: 0.3467 - accuracy: 0.7976 - f1_metric: 0.79 - ETA: 0s - loss: 0.3490 - accuracy: 0.7937 - f1_metric: 0.79 - ETA: 0s - loss: 0.3432 - accuracy: 0.7980 - f1_metric: 0.79 - ETA: 0s - loss: 0.3406 - accuracy: 0.7958 - f1_metric: 0.79 - ETA: 0s - loss: 0.3430 - accuracy: 0.7955 - f1_metric: 0.79 - ETA: 0s - loss: 0.3427 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3377 - accuracy: 0.7993 - f1_metric: 0.79 - ETA: 0s - loss: 0.3329 - accuracy: 0.8036 - f1_metric: 0.8022\n",
      "Epoch 00050: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3401 - accuracy: 0.8042 - f1_metric: 0.8031 - val_loss: 0.5059 - val_accuracy: 0.8090 - val_f1_metric: 0.8082 - lr: 1.0000e-04\n",
      "Epoch 51/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3284 - accuracy: 0.8073 - f1_metric: 0.81 - ETA: 0s - loss: 0.3303 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3321 - accuracy: 0.7899 - f1_metric: 0.79 - ETA: 0s - loss: 0.3177 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3184 - accuracy: 0.7896 - f1_metric: 0.79 - ETA: 0s - loss: 0.3186 - accuracy: 0.7865 - f1_metric: 0.78 - ETA: 0s - loss: 0.3240 - accuracy: 0.7887 - f1_metric: 0.78 - ETA: 0s - loss: 0.3205 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.3259 - accuracy: 0.7928 - f1_metric: 0.79 - ETA: 0s - loss: 0.3256 - accuracy: 0.7942 - f1_metric: 0.79 - ETA: 0s - loss: 0.3224 - accuracy: 0.7964 - f1_metric: 0.79 - ETA: 0s - loss: 0.3239 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3267 - accuracy: 0.7960 - f1_metric: 0.79 - ETA: 0s - loss: 0.3239 - accuracy: 0.7965 - f1_metric: 0.79 - ETA: 0s - loss: 0.3277 - accuracy: 0.7983 - f1_metric: 0.7988\n",
      "Epoch 00051: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3284 - accuracy: 0.7989 - f1_metric: 0.7998 - val_loss: 0.5191 - val_accuracy: 0.7982 - val_f1_metric: 0.7983 - lr: 1.0000e-04\n",
      "Epoch 52/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5744 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.3758 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3235 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3373 - accuracy: 0.7934 - f1_metric: 0.79 - ETA: 0s - loss: 0.3344 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3308 - accuracy: 0.7949 - f1_metric: 0.79 - ETA: 0s - loss: 0.3290 - accuracy: 0.7895 - f1_metric: 0.78 - ETA: 0s - loss: 0.3271 - accuracy: 0.7905 - f1_metric: 0.79 - ETA: 0s - loss: 0.3292 - accuracy: 0.7923 - f1_metric: 0.79 - ETA: 0s - loss: 0.3294 - accuracy: 0.7957 - f1_metric: 0.79 - ETA: 0s - loss: 0.3281 - accuracy: 0.8001 - f1_metric: 0.79 - ETA: 0s - loss: 0.3318 - accuracy: 0.7999 - f1_metric: 0.79 - ETA: 0s - loss: 0.3302 - accuracy: 0.7988 - f1_metric: 0.79 - ETA: 0s - loss: 0.3291 - accuracy: 0.8000 - f1_metric: 0.79 - ETA: 0s - loss: 0.3254 - accuracy: 0.8015 - f1_metric: 0.80 - ETA: 0s - loss: 0.3257 - accuracy: 0.8012 - f1_metric: 0.80 - ETA: 0s - loss: 0.3260 - accuracy: 0.8043 - f1_metric: 0.8039\n",
      "Epoch 00052: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3250 - accuracy: 0.8057 - f1_metric: 0.8054 - val_loss: 0.5118 - val_accuracy: 0.8082 - val_f1_metric: 0.8066 - lr: 1.0000e-04\n",
      "Epoch 53/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3656 - accuracy: 0.7656 - f1_metric: 0.75 - ETA: 0s - loss: 0.3203 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.3227 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3085 - accuracy: 0.8264 - f1_metric: 0.82 - ETA: 0s - loss: 0.3062 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3059 - accuracy: 0.8198 - f1_metric: 0.81 - ETA: 0s - loss: 0.3071 - accuracy: 0.8217 - f1_metric: 0.82 - ETA: 0s - loss: 0.3150 - accuracy: 0.8133 - f1_metric: 0.81 - ETA: 0s - loss: 0.3148 - accuracy: 0.8118 - f1_metric: 0.81 - ETA: 0s - loss: 0.3153 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3149 - accuracy: 0.8114 - f1_metric: 0.81 - ETA: 0s - loss: 0.3121 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.3173 - accuracy: 0.8134 - f1_metric: 0.81 - ETA: 0s - loss: 0.3134 - accuracy: 0.8129 - f1_metric: 0.81 - ETA: 0s - loss: 0.3220 - accuracy: 0.8113 - f1_metric: 0.81 - ETA: 0s - loss: 0.3259 - accuracy: 0.8103 - f1_metric: 0.8099\n",
      "Epoch 00053: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3266 - accuracy: 0.8121 - f1_metric: 0.8132 - val_loss: 0.5047 - val_accuracy: 0.8074 - val_f1_metric: 0.8074 - lr: 1.0000e-04\n",
      "Epoch 54/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3314 - accuracy: 0.7917 - f1_metric: 0.78 - ETA: 0s - loss: 0.3195 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.3157 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.3344 - accuracy: 0.7926 - f1_metric: 0.79 - ETA: 0s - loss: 0.3314 - accuracy: 0.7946 - f1_metric: 0.79 - ETA: 0s - loss: 0.3339 - accuracy: 0.7895 - f1_metric: 0.78 - ETA: 0s - loss: 0.3265 - accuracy: 0.7945 - f1_metric: 0.79 - ETA: 0s - loss: 0.3248 - accuracy: 0.7983 - f1_metric: 0.79 - ETA: 0s - loss: 0.3254 - accuracy: 0.7988 - f1_metric: 0.79 - ETA: 0s - loss: 0.3271 - accuracy: 0.7981 - f1_metric: 0.79 - ETA: 0s - loss: 0.3303 - accuracy: 0.8001 - f1_metric: 0.79 - ETA: 0s - loss: 0.3252 - accuracy: 0.8018 - f1_metric: 0.80 - ETA: 0s - loss: 0.3308 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3316 - accuracy: 0.8024 - f1_metric: 0.80 - ETA: 0s - loss: 0.3302 - accuracy: 0.8037 - f1_metric: 0.80 - ETA: 0s - loss: 0.3301 - accuracy: 0.8022 - f1_metric: 0.80 - ETA: 0s - loss: 0.3330 - accuracy: 0.8010 - f1_metric: 0.8002\n",
      "Epoch 00054: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3330 - accuracy: 0.8010 - f1_metric: 0.8002 - val_loss: 0.5335 - val_accuracy: 0.7915 - val_f1_metric: 0.7899 - lr: 1.0000e-04\n",
      "Epoch 55/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2867 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3243 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3070 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3135 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3147 - accuracy: 0.8185 - f1_metric: 0.81 - ETA: 0s - loss: 0.3312 - accuracy: 0.8051 - f1_metric: 0.80 - ETA: 0s - loss: 0.3335 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3286 - accuracy: 0.8092 - f1_metric: 0.80 - ETA: 0s - loss: 0.3237 - accuracy: 0.8119 - f1_metric: 0.81 - ETA: 0s - loss: 0.3297 - accuracy: 0.8103 - f1_metric: 0.80 - ETA: 0s - loss: 0.3310 - accuracy: 0.8054 - f1_metric: 0.80 - ETA: 0s - loss: 0.3295 - accuracy: 0.8045 - f1_metric: 0.80 - ETA: 0s - loss: 0.3302 - accuracy: 0.8013 - f1_metric: 0.80 - ETA: 0s - loss: 0.3275 - accuracy: 0.8015 - f1_metric: 0.80 - ETA: 0s - loss: 0.3290 - accuracy: 0.8012 - f1_metric: 0.80 - ETA: 0s - loss: 0.3274 - accuracy: 0.8006 - f1_metric: 0.7996\n",
      "Epoch 00055: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3251 - accuracy: 0.8035 - f1_metric: 0.8027 - val_loss: 0.5097 - val_accuracy: 0.8049 - val_f1_metric: 0.8036 - lr: 1.0000e-04\n",
      "Epoch 56/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3429 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3303 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3116 - accuracy: 0.8056 - f1_metric: 0.80 - ETA: 0s - loss: 0.3098 - accuracy: 0.8216 - f1_metric: 0.81 - ETA: 0s - loss: 0.3168 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3175 - accuracy: 0.8215 - f1_metric: 0.82 - ETA: 0s - loss: 0.3156 - accuracy: 0.8251 - f1_metric: 0.82 - ETA: 0s - loss: 0.3255 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3265 - accuracy: 0.8245 - f1_metric: 0.82 - ETA: 0s - loss: 0.3276 - accuracy: 0.8209 - f1_metric: 0.82 - ETA: 0s - loss: 0.3253 - accuracy: 0.8201 - f1_metric: 0.81 - ETA: 0s - loss: 0.3254 - accuracy: 0.8196 - f1_metric: 0.81 - ETA: 0s - loss: 0.3254 - accuracy: 0.8174 - f1_metric: 0.81 - ETA: 0s - loss: 0.3217 - accuracy: 0.8195 - f1_metric: 0.81 - ETA: 0s - loss: 0.3200 - accuracy: 0.8199 - f1_metric: 0.81 - ETA: 0s - loss: 0.3213 - accuracy: 0.8180 - f1_metric: 0.8176\n",
      "Epoch 00056: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3201 - accuracy: 0.8179 - f1_metric: 0.8175 - val_loss: 0.5128 - val_accuracy: 0.8040 - val_f1_metric: 0.8046 - lr: 1.0000e-04\n",
      "Epoch 57/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3209 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.3227 - accuracy: 0.8125 - f1_metric: 0.80 - ETA: 0s - loss: 0.3679 - accuracy: 0.7879 - f1_metric: 0.78 - ETA: 0s - loss: 0.3538 - accuracy: 0.8038 - f1_metric: 0.80 - ETA: 0s - loss: 0.3547 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3610 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3594 - accuracy: 0.7943 - f1_metric: 0.79 - ETA: 0s - loss: 0.3550 - accuracy: 0.7961 - f1_metric: 0.79 - ETA: 0s - loss: 0.3477 - accuracy: 0.7935 - f1_metric: 0.79 - ETA: 0s - loss: 0.3445 - accuracy: 0.7912 - f1_metric: 0.79 - ETA: 0s - loss: 0.3410 - accuracy: 0.7902 - f1_metric: 0.78 - ETA: 0s - loss: 0.3438 - accuracy: 0.7906 - f1_metric: 0.79 - ETA: 0s - loss: 0.3481 - accuracy: 0.7926 - f1_metric: 0.79 - ETA: 0s - loss: 0.3449 - accuracy: 0.7960 - f1_metric: 0.79 - ETA: 0s - loss: 0.3399 - accuracy: 0.7960 - f1_metric: 0.79 - ETA: 0s - loss: 0.3351 - accuracy: 0.7980 - f1_metric: 0.79 - ETA: 0s - loss: 0.3350 - accuracy: 0.7984 - f1_metric: 0.7976\n",
      "Epoch 00057: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3342 - accuracy: 0.8003 - f1_metric: 0.7997 - val_loss: 0.4994 - val_accuracy: 0.8090 - val_f1_metric: 0.8094 - lr: 1.0000e-04\n",
      "Epoch 58/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3871 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3712 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3401 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3335 - accuracy: 0.8247 - f1_metric: 0.82 - ETA: 0s - loss: 0.3197 - accuracy: 0.8216 - f1_metric: 0.82 - ETA: 0s - loss: 0.3162 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3191 - accuracy: 0.8240 - f1_metric: 0.82 - ETA: 0s - loss: 0.3198 - accuracy: 0.8224 - f1_metric: 0.82 - ETA: 0s - loss: 0.3110 - accuracy: 0.8275 - f1_metric: 0.82 - ETA: 0s - loss: 0.3108 - accuracy: 0.8258 - f1_metric: 0.82 - ETA: 0s - loss: 0.3180 - accuracy: 0.8208 - f1_metric: 0.82 - ETA: 0s - loss: 0.3240 - accuracy: 0.8205 - f1_metric: 0.82 - ETA: 0s - loss: 0.3313 - accuracy: 0.8199 - f1_metric: 0.81 - ETA: 0s - loss: 0.3316 - accuracy: 0.8225 - f1_metric: 0.82 - ETA: 0s - loss: 0.3294 - accuracy: 0.8225 - f1_metric: 0.8226\n",
      "Epoch 00058: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3298 - accuracy: 0.8175 - f1_metric: 0.8154 - val_loss: 0.5480 - val_accuracy: 0.7873 - val_f1_metric: 0.7862 - lr: 1.0000e-04\n",
      "Epoch 59/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3261 - accuracy: 0.8008 - f1_metric: 0.80 - ETA: 0s - loss: 0.3397 - accuracy: 0.8013 - f1_metric: 0.80 - ETA: 0s - loss: 0.3268 - accuracy: 0.8094 - f1_metric: 0.80 - ETA: 0s - loss: 0.3257 - accuracy: 0.8137 - f1_metric: 0.81 - ETA: 0s - loss: 0.3255 - accuracy: 0.8154 - f1_metric: 0.81 - ETA: 0s - loss: 0.3253 - accuracy: 0.8084 - f1_metric: 0.80 - ETA: 0s - loss: 0.3172 - accuracy: 0.8089 - f1_metric: 0.80 - ETA: 0s - loss: 0.3193 - accuracy: 0.8081 - f1_metric: 0.80 - ETA: 0s - loss: 0.3210 - accuracy: 0.8050 - f1_metric: 0.80 - ETA: 0s - loss: 0.3226 - accuracy: 0.8028 - f1_metric: 0.80 - ETA: 0s - loss: 0.3224 - accuracy: 0.8034 - f1_metric: 0.80 - ETA: 0s - loss: 0.3214 - accuracy: 0.8024 - f1_metric: 0.80 - ETA: 0s - loss: 0.3202 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3192 - accuracy: 0.8030 - f1_metric: 0.80 - ETA: 0s - loss: 0.3230 - accuracy: 0.8014 - f1_metric: 0.80 - ETA: 0s - loss: 0.3272 - accuracy: 0.8009 - f1_metric: 0.8001\n",
      "Epoch 00059: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3255 - accuracy: 0.8014 - f1_metric: 0.8011 - val_loss: 0.4910 - val_accuracy: 0.8141 - val_f1_metric: 0.8128 - lr: 1.0000e-04\n",
      "Epoch 60/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.2031 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2328 - accuracy: 0.8828 - f1_metric: 0.88 - ETA: 0s - loss: 0.2747 - accuracy: 0.8683 - f1_metric: 0.86 - ETA: 0s - loss: 0.2887 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2935 - accuracy: 0.8398 - f1_metric: 0.83 - ETA: 0s - loss: 0.3002 - accuracy: 0.8292 - f1_metric: 0.82 - ETA: 0s - loss: 0.3026 - accuracy: 0.8290 - f1_metric: 0.82 - ETA: 0s - loss: 0.3086 - accuracy: 0.8180 - f1_metric: 0.81 - ETA: 0s - loss: 0.3142 - accuracy: 0.8153 - f1_metric: 0.81 - ETA: 0s - loss: 0.3137 - accuracy: 0.8132 - f1_metric: 0.81 - ETA: 0s - loss: 0.3169 - accuracy: 0.8137 - f1_metric: 0.81 - ETA: 0s - loss: 0.3182 - accuracy: 0.8167 - f1_metric: 0.81 - ETA: 0s - loss: 0.3190 - accuracy: 0.8163 - f1_metric: 0.81 - ETA: 0s - loss: 0.3209 - accuracy: 0.8168 - f1_metric: 0.81 - ETA: 0s - loss: 0.3234 - accuracy: 0.8169 - f1_metric: 0.81 - ETA: 0s - loss: 0.3228 - accuracy: 0.8171 - f1_metric: 0.8169\n",
      "Epoch 00060: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3208 - accuracy: 0.8143 - f1_metric: 0.8127 - val_loss: 0.5320 - val_accuracy: 0.7906 - val_f1_metric: 0.7902 - lr: 1.0000e-04\n",
      "Epoch 61/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3738 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.3774 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3519 - accuracy: 0.7857 - f1_metric: 0.78 - ETA: 0s - loss: 0.3322 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3253 - accuracy: 0.7957 - f1_metric: 0.79 - ETA: 0s - loss: 0.3182 - accuracy: 0.7979 - f1_metric: 0.79 - ETA: 0s - loss: 0.3191 - accuracy: 0.8067 - f1_metric: 0.80 - ETA: 0s - loss: 0.3200 - accuracy: 0.8036 - f1_metric: 0.80 - ETA: 0s - loss: 0.3206 - accuracy: 0.8053 - f1_metric: 0.80 - ETA: 0s - loss: 0.3185 - accuracy: 0.8059 - f1_metric: 0.80 - ETA: 0s - loss: 0.3204 - accuracy: 0.8028 - f1_metric: 0.80 - ETA: 0s - loss: 0.3163 - accuracy: 0.8057 - f1_metric: 0.80 - ETA: 0s - loss: 0.3146 - accuracy: 0.8067 - f1_metric: 0.80 - ETA: 0s - loss: 0.3143 - accuracy: 0.8092 - f1_metric: 0.80 - ETA: 0s - loss: 0.3176 - accuracy: 0.8109 - f1_metric: 0.81 - ETA: 0s - loss: 0.3133 - accuracy: 0.8110 - f1_metric: 0.8110\n",
      "Epoch 00061: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3151 - accuracy: 0.8111 - f1_metric: 0.8110 - val_loss: 0.4871 - val_accuracy: 0.8199 - val_f1_metric: 0.8180 - lr: 1.0000e-04\n",
      "Epoch 62/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4898 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3486 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.3152 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3025 - accuracy: 0.8212 - f1_metric: 0.82 - ETA: 0s - loss: 0.2985 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3099 - accuracy: 0.8104 - f1_metric: 0.80 - ETA: 0s - loss: 0.3221 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3231 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3214 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3277 - accuracy: 0.8119 - f1_metric: 0.81 - ETA: 0s - loss: 0.3308 - accuracy: 0.8069 - f1_metric: 0.80 - ETA: 0s - loss: 0.3302 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.3290 - accuracy: 0.8116 - f1_metric: 0.81 - ETA: 0s - loss: 0.3266 - accuracy: 0.8129 - f1_metric: 0.81 - ETA: 0s - loss: 0.3256 - accuracy: 0.8121 - f1_metric: 0.81 - ETA: 0s - loss: 0.3225 - accuracy: 0.8147 - f1_metric: 0.8144\n",
      "Epoch 00062: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3235 - accuracy: 0.8150 - f1_metric: 0.8151 - val_loss: 0.5288 - val_accuracy: 0.7956 - val_f1_metric: 0.7951 - lr: 1.0000e-04\n",
      "Epoch 63/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3428 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3227 - accuracy: 0.8203 - f1_metric: 0.81 - ETA: 0s - loss: 0.3136 - accuracy: 0.8164 - f1_metric: 0.81 - ETA: 0s - loss: 0.3214 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3239 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.3121 - accuracy: 0.8082 - f1_metric: 0.80 - ETA: 0s - loss: 0.3117 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3097 - accuracy: 0.8066 - f1_metric: 0.80 - ETA: 0s - loss: 0.3066 - accuracy: 0.8053 - f1_metric: 0.80 - ETA: 0s - loss: 0.3081 - accuracy: 0.8097 - f1_metric: 0.80 - ETA: 0s - loss: 0.3115 - accuracy: 0.8120 - f1_metric: 0.81 - ETA: 0s - loss: 0.3162 - accuracy: 0.8111 - f1_metric: 0.81 - ETA: 0s - loss: 0.3134 - accuracy: 0.8116 - f1_metric: 0.81 - ETA: 0s - loss: 0.3158 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3147 - accuracy: 0.8129 - f1_metric: 0.81 - ETA: 0s - loss: 0.3153 - accuracy: 0.8150 - f1_metric: 0.8149\n",
      "Epoch 00063: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3147 - accuracy: 0.8161 - f1_metric: 0.8170 - val_loss: 0.4904 - val_accuracy: 0.8183 - val_f1_metric: 0.8183 - lr: 1.0000e-04\n",
      "Epoch 64/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3020 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.3122 - accuracy: 0.8500 - f1_metric: 0.85 - ETA: 0s - loss: 0.2989 - accuracy: 0.8457 - f1_metric: 0.84 - ETA: 0s - loss: 0.3013 - accuracy: 0.8328 - f1_metric: 0.83 - ETA: 0s - loss: 0.3120 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3076 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3105 - accuracy: 0.8263 - f1_metric: 0.82 - ETA: 0s - loss: 0.3107 - accuracy: 0.8207 - f1_metric: 0.82 - ETA: 0s - loss: 0.3291 - accuracy: 0.8155 - f1_metric: 0.81 - ETA: 0s - loss: 0.3255 - accuracy: 0.8139 - f1_metric: 0.81 - ETA: 0s - loss: 0.3271 - accuracy: 0.8089 - f1_metric: 0.80 - ETA: 0s - loss: 0.3270 - accuracy: 0.8119 - f1_metric: 0.81 - ETA: 0s - loss: 0.3263 - accuracy: 0.8100 - f1_metric: 0.81 - ETA: 0s - loss: 0.3265 - accuracy: 0.8111 - f1_metric: 0.81 - ETA: 0s - loss: 0.3256 - accuracy: 0.8108 - f1_metric: 0.81 - ETA: 0s - loss: 0.3262 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.3224 - accuracy: 0.8164 - f1_metric: 0.81 - ETA: 0s - loss: 0.3222 - accuracy: 0.8165 - f1_metric: 0.8164\n",
      "Epoch 00064: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3220 - accuracy: 0.8168 - f1_metric: 0.8170 - val_loss: 0.5131 - val_accuracy: 0.8049 - val_f1_metric: 0.8039 - lr: 1.0000e-04\n",
      "Epoch 65/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3144 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3270 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3256 - accuracy: 0.7930 - f1_metric: 0.79 - ETA: 0s - loss: 0.3305 - accuracy: 0.7884 - f1_metric: 0.78 - ETA: 0s - loss: 0.3295 - accuracy: 0.7991 - f1_metric: 0.79 - ETA: 0s - loss: 0.3222 - accuracy: 0.8037 - f1_metric: 0.80 - ETA: 0s - loss: 0.3224 - accuracy: 0.8082 - f1_metric: 0.80 - ETA: 0s - loss: 0.3218 - accuracy: 0.8103 - f1_metric: 0.81 - ETA: 0s - loss: 0.3206 - accuracy: 0.8118 - f1_metric: 0.81 - ETA: 0s - loss: 0.3188 - accuracy: 0.8142 - f1_metric: 0.81 - ETA: 0s - loss: 0.3201 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.3243 - accuracy: 0.8120 - f1_metric: 0.81 - ETA: 0s - loss: 0.3249 - accuracy: 0.8138 - f1_metric: 0.81 - ETA: 0s - loss: 0.3218 - accuracy: 0.8148 - f1_metric: 0.81 - ETA: 0s - loss: 0.3210 - accuracy: 0.8147 - f1_metric: 0.8143\n",
      "Epoch 00065: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3205 - accuracy: 0.8147 - f1_metric: 0.8143 - val_loss: 0.5108 - val_accuracy: 0.8032 - val_f1_metric: 0.8034 - lr: 1.0000e-04\n",
      "Epoch 66/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4237 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3317 - accuracy: 0.8242 - f1_metric: 0.81 - ETA: 0s - loss: 0.3161 - accuracy: 0.8326 - f1_metric: 0.83 - ETA: 0s - loss: 0.2998 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3248 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.3191 - accuracy: 0.8104 - f1_metric: 0.80 - ETA: 0s - loss: 0.3194 - accuracy: 0.8079 - f1_metric: 0.80 - ETA: 0s - loss: 0.3142 - accuracy: 0.8156 - f1_metric: 0.81 - ETA: 0s - loss: 0.3238 - accuracy: 0.8179 - f1_metric: 0.81 - ETA: 0s - loss: 0.3244 - accuracy: 0.8179 - f1_metric: 0.81 - ETA: 0s - loss: 0.3302 - accuracy: 0.8142 - f1_metric: 0.81 - ETA: 0s - loss: 0.3253 - accuracy: 0.8193 - f1_metric: 0.81 - ETA: 0s - loss: 0.3243 - accuracy: 0.8191 - f1_metric: 0.81 - ETA: 0s - loss: 0.3231 - accuracy: 0.8186 - f1_metric: 0.81 - ETA: 0s - loss: 0.3289 - accuracy: 0.8137 - f1_metric: 0.81 - ETA: 0s - loss: 0.3245 - accuracy: 0.8147 - f1_metric: 0.8141\n",
      "Epoch 00066: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3286 - accuracy: 0.8136 - f1_metric: 0.8140 - val_loss: 0.5428 - val_accuracy: 0.7848 - val_f1_metric: 0.7862 - lr: 1.0000e-04\n",
      "Epoch 67/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4973 - accuracy: 0.6719 - f1_metric: 0.66 - ETA: 0s - loss: 0.3415 - accuracy: 0.7760 - f1_metric: 0.77 - ETA: 0s - loss: 0.3125 - accuracy: 0.8099 - f1_metric: 0.80 - ETA: 0s - loss: 0.3054 - accuracy: 0.8160 - f1_metric: 0.81 - ETA: 0s - loss: 0.3224 - accuracy: 0.8008 - f1_metric: 0.79 - ETA: 0s - loss: 0.3236 - accuracy: 0.8031 - f1_metric: 0.80 - ETA: 0s - loss: 0.3303 - accuracy: 0.8024 - f1_metric: 0.80 - ETA: 0s - loss: 0.3282 - accuracy: 0.8023 - f1_metric: 0.80 - ETA: 0s - loss: 0.3275 - accuracy: 0.8018 - f1_metric: 0.80 - ETA: 0s - loss: 0.3261 - accuracy: 0.8040 - f1_metric: 0.80 - ETA: 0s - loss: 0.3261 - accuracy: 0.8017 - f1_metric: 0.80 - ETA: 0s - loss: 0.3255 - accuracy: 0.8052 - f1_metric: 0.80 - ETA: 0s - loss: 0.3301 - accuracy: 0.8065 - f1_metric: 0.80 - ETA: 0s - loss: 0.3322 - accuracy: 0.8087 - f1_metric: 0.80 - ETA: 0s - loss: 0.3284 - accuracy: 0.8112 - f1_metric: 0.81 - ETA: 0s - loss: 0.3277 - accuracy: 0.8079 - f1_metric: 0.80 - ETA: 0s - loss: 0.3264 - accuracy: 0.8094 - f1_metric: 0.80 - ETA: 0s - loss: 0.3279 - accuracy: 0.8103 - f1_metric: 0.8100\n",
      "Epoch 00067: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3272 - accuracy: 0.8100 - f1_metric: 0.8093 - val_loss: 0.5269 - val_accuracy: 0.7931 - val_f1_metric: 0.7942 - lr: 1.0000e-04\n",
      "Epoch 68/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3160 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.2833 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3377 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3184 - accuracy: 0.8224 - f1_metric: 0.82 - ETA: 0s - loss: 0.3201 - accuracy: 0.8245 - f1_metric: 0.82 - ETA: 0s - loss: 0.3185 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3205 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3224 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.3302 - accuracy: 0.8075 - f1_metric: 0.80 - ETA: 0s - loss: 0.3281 - accuracy: 0.8112 - f1_metric: 0.81 - ETA: 0s - loss: 0.3250 - accuracy: 0.8084 - f1_metric: 0.80 - ETA: 0s - loss: 0.3206 - accuracy: 0.8068 - f1_metric: 0.80 - ETA: 0s - loss: 0.3175 - accuracy: 0.8054 - f1_metric: 0.80 - ETA: 0s - loss: 0.3222 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3258 - accuracy: 0.8025 - f1_metric: 0.80 - ETA: 0s - loss: 0.3269 - accuracy: 0.8043 - f1_metric: 0.8044\n",
      "Epoch 00068: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3273 - accuracy: 0.8035 - f1_metric: 0.8030 - val_loss: 0.5089 - val_accuracy: 0.8099 - val_f1_metric: 0.8085 - lr: 1.0000e-04\n",
      "Epoch 69/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3097 - accuracy: 0.8164 - f1_metric: 0.81 - ETA: 0s - loss: 0.3231 - accuracy: 0.8156 - f1_metric: 0.81 - ETA: 0s - loss: 0.3138 - accuracy: 0.8184 - f1_metric: 0.81 - ETA: 0s - loss: 0.3055 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3230 - accuracy: 0.8000 - f1_metric: 0.80 - ETA: 0s - loss: 0.3264 - accuracy: 0.8038 - f1_metric: 0.80 - ETA: 0s - loss: 0.3234 - accuracy: 0.8078 - f1_metric: 0.80 - ETA: 0s - loss: 0.3134 - accuracy: 0.8118 - f1_metric: 0.81 - ETA: 0s - loss: 0.3131 - accuracy: 0.8137 - f1_metric: 0.81 - ETA: 0s - loss: 0.3178 - accuracy: 0.8152 - f1_metric: 0.81 - ETA: 0s - loss: 0.3200 - accuracy: 0.8174 - f1_metric: 0.81 - ETA: 0s - loss: 0.3229 - accuracy: 0.8148 - f1_metric: 0.81 - ETA: 0s - loss: 0.3209 - accuracy: 0.8142 - f1_metric: 0.81 - ETA: 0s - loss: 0.3209 - accuracy: 0.8145 - f1_metric: 0.81 - ETA: 0s - loss: 0.3218 - accuracy: 0.8117 - f1_metric: 0.81 - ETA: 0s - loss: 0.3241 - accuracy: 0.8114 - f1_metric: 0.8119\n",
      "Epoch 00069: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3241 - accuracy: 0.8114 - f1_metric: 0.8119 - val_loss: 0.5232 - val_accuracy: 0.8007 - val_f1_metric: 0.7997 - lr: 1.0000e-04\n",
      "Epoch 70/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.3346 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3176 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.3267 - accuracy: 0.7995 - f1_metric: 0.79 - ETA: 0s - loss: 0.3378 - accuracy: 0.8038 - f1_metric: 0.80 - ETA: 0s - loss: 0.3226 - accuracy: 0.8099 - f1_metric: 0.80 - ETA: 0s - loss: 0.3163 - accuracy: 0.8042 - f1_metric: 0.80 - ETA: 0s - loss: 0.3171 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3070 - accuracy: 0.8088 - f1_metric: 0.80 - ETA: 0s - loss: 0.3002 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.3003 - accuracy: 0.8100 - f1_metric: 0.81 - ETA: 0s - loss: 0.3052 - accuracy: 0.8079 - f1_metric: 0.80 - ETA: 0s - loss: 0.3037 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3039 - accuracy: 0.8140 - f1_metric: 0.81 - ETA: 0s - loss: 0.3123 - accuracy: 0.8157 - f1_metric: 0.81 - ETA: 0s - loss: 0.3153 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3130 - accuracy: 0.8183 - f1_metric: 0.81 - ETA: 0s - loss: 0.3148 - accuracy: 0.8178 - f1_metric: 0.81 - ETA: 0s - loss: 0.3168 - accuracy: 0.8161 - f1_metric: 0.8155\n",
      "Epoch 00070: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3175 - accuracy: 0.8157 - f1_metric: 0.8147 - val_loss: 0.5054 - val_accuracy: 0.8099 - val_f1_metric: 0.8102 - lr: 1.0000e-04\n",
      "Epoch 71/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.2789 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.2907 - accuracy: 0.7875 - f1_metric: 0.78 - ETA: 0s - loss: 0.2912 - accuracy: 0.7924 - f1_metric: 0.79 - ETA: 0s - loss: 0.3211 - accuracy: 0.7951 - f1_metric: 0.79 - ETA: 0s - loss: 0.3216 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3412 - accuracy: 0.7990 - f1_metric: 0.79 - ETA: 0s - loss: 0.3384 - accuracy: 0.8003 - f1_metric: 0.80 - ETA: 0s - loss: 0.3308 - accuracy: 0.8036 - f1_metric: 0.80 - ETA: 0s - loss: 0.3255 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3235 - accuracy: 0.8061 - f1_metric: 0.80 - ETA: 0s - loss: 0.3181 - accuracy: 0.8104 - f1_metric: 0.81 - ETA: 0s - loss: 0.3202 - accuracy: 0.8087 - f1_metric: 0.80 - ETA: 0s - loss: 0.3240 - accuracy: 0.8066 - f1_metric: 0.80 - ETA: 0s - loss: 0.3188 - accuracy: 0.8101 - f1_metric: 0.81 - ETA: 0s - loss: 0.3177 - accuracy: 0.8117 - f1_metric: 0.81 - ETA: 0s - loss: 0.3180 - accuracy: 0.8136 - f1_metric: 0.8144\n",
      "Epoch 00071: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3180 - accuracy: 0.8136 - f1_metric: 0.8144 - val_loss: 0.4978 - val_accuracy: 0.8124 - val_f1_metric: 0.8111 - lr: 1.0000e-04\n",
      "Epoch 72/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3604 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2743 - accuracy: 0.8698 - f1_metric: 0.87 - ETA: 0s - loss: 0.2962 - accuracy: 0.8464 - f1_metric: 0.84 - ETA: 0s - loss: 0.2909 - accuracy: 0.8496 - f1_metric: 0.85 - ETA: 0s - loss: 0.2909 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2965 - accuracy: 0.8451 - f1_metric: 0.84 - ETA: 0s - loss: 0.3035 - accuracy: 0.8365 - f1_metric: 0.83 - ETA: 0s - loss: 0.3071 - accuracy: 0.8368 - f1_metric: 0.83 - ETA: 0s - loss: 0.3202 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3212 - accuracy: 0.8255 - f1_metric: 0.82 - ETA: 0s - loss: 0.3187 - accuracy: 0.8241 - f1_metric: 0.82 - ETA: 0s - loss: 0.3233 - accuracy: 0.8198 - f1_metric: 0.81 - ETA: 0s - loss: 0.3227 - accuracy: 0.8205 - f1_metric: 0.82 - ETA: 0s - loss: 0.3181 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3191 - accuracy: 0.8225 - f1_metric: 0.82 - ETA: 0s - loss: 0.3200 - accuracy: 0.8203 - f1_metric: 0.8200\n",
      "Epoch 00072: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3205 - accuracy: 0.8204 - f1_metric: 0.8198 - val_loss: 0.5207 - val_accuracy: 0.7965 - val_f1_metric: 0.7957 - lr: 1.0000e-04\n",
      "Epoch 73/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.4054 - accuracy: 0.7500 - f1_metric: 0.75 - ETA: 0s - loss: 0.3348 - accuracy: 0.7865 - f1_metric: 0.78 - ETA: 0s - loss: 0.3356 - accuracy: 0.7937 - f1_metric: 0.79 - ETA: 0s - loss: 0.3451 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3357 - accuracy: 0.7997 - f1_metric: 0.80 - ETA: 0s - loss: 0.3263 - accuracy: 0.8065 - f1_metric: 0.80 - ETA: 0s - loss: 0.3225 - accuracy: 0.8076 - f1_metric: 0.80 - ETA: 0s - loss: 0.3173 - accuracy: 0.8142 - f1_metric: 0.81 - ETA: 0s - loss: 0.3199 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3253 - accuracy: 0.8082 - f1_metric: 0.80 - ETA: 0s - loss: 0.3295 - accuracy: 0.8069 - f1_metric: 0.80 - ETA: 0s - loss: 0.3323 - accuracy: 0.8036 - f1_metric: 0.80 - ETA: 0s - loss: 0.3287 - accuracy: 0.8034 - f1_metric: 0.80 - ETA: 0s - loss: 0.3232 - accuracy: 0.8079 - f1_metric: 0.80 - ETA: 0s - loss: 0.3212 - accuracy: 0.8100 - f1_metric: 0.80 - ETA: 0s - loss: 0.3192 - accuracy: 0.8117 - f1_metric: 0.81 - ETA: 0s - loss: 0.3159 - accuracy: 0.8147 - f1_metric: 0.8135\n",
      "Epoch 00073: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3161 - accuracy: 0.8150 - f1_metric: 0.8142 - val_loss: 0.5001 - val_accuracy: 0.8124 - val_f1_metric: 0.8118 - lr: 1.0000e-04\n",
      "Epoch 74/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.8906 - f1_metric: 0.89 - ETA: 0s - loss: 0.2843 - accuracy: 0.8594 - f1_metric: 0.86 - ETA: 0s - loss: 0.2929 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3034 - accuracy: 0.8247 - f1_metric: 0.82 - ETA: 0s - loss: 0.3003 - accuracy: 0.8294 - f1_metric: 0.83 - ETA: 0s - loss: 0.3003 - accuracy: 0.8248 - f1_metric: 0.82 - ETA: 0s - loss: 0.3001 - accuracy: 0.8244 - f1_metric: 0.82 - ETA: 0s - loss: 0.3026 - accuracy: 0.8273 - f1_metric: 0.82 - ETA: 0s - loss: 0.3069 - accuracy: 0.8217 - f1_metric: 0.82 - ETA: 0s - loss: 0.3037 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3077 - accuracy: 0.8227 - f1_metric: 0.82 - ETA: 0s - loss: 0.3092 - accuracy: 0.8206 - f1_metric: 0.82 - ETA: 0s - loss: 0.3128 - accuracy: 0.8213 - f1_metric: 0.82 - ETA: 0s - loss: 0.3142 - accuracy: 0.8208 - f1_metric: 0.82 - ETA: 0s - loss: 0.3164 - accuracy: 0.8190 - f1_metric: 0.81 - ETA: 0s - loss: 0.3156 - accuracy: 0.8187 - f1_metric: 0.81 - ETA: 0s - loss: 0.3171 - accuracy: 0.8201 - f1_metric: 0.81 - ETA: 0s - loss: 0.3169 - accuracy: 0.8198 - f1_metric: 0.8193\n",
      "Epoch 00074: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3181 - accuracy: 0.8197 - f1_metric: 0.8192 - val_loss: 0.5129 - val_accuracy: 0.8065 - val_f1_metric: 0.8067 - lr: 1.0000e-04\n",
      "Epoch 75/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2504 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2895 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.3137 - accuracy: 0.8000 - f1_metric: 0.80 - ETA: 0s - loss: 0.3069 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3099 - accuracy: 0.7997 - f1_metric: 0.79 - ETA: 0s - loss: 0.3043 - accuracy: 0.8077 - f1_metric: 0.80 - ETA: 0s - loss: 0.3048 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3087 - accuracy: 0.8038 - f1_metric: 0.80 - ETA: 0s - loss: 0.3091 - accuracy: 0.8078 - f1_metric: 0.80 - ETA: 0s - loss: 0.3067 - accuracy: 0.8139 - f1_metric: 0.81 - ETA: 0s - loss: 0.3050 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.3011 - accuracy: 0.8194 - f1_metric: 0.81 - ETA: 0s - loss: 0.3043 - accuracy: 0.8217 - f1_metric: 0.82 - ETA: 0s - loss: 0.3096 - accuracy: 0.8203 - f1_metric: 0.82 - ETA: 0s - loss: 0.3160 - accuracy: 0.8162 - f1_metric: 0.81 - ETA: 0s - loss: 0.3102 - accuracy: 0.8205 - f1_metric: 0.82 - ETA: 0s - loss: 0.3131 - accuracy: 0.8197 - f1_metric: 0.81 - ETA: 0s - loss: 0.3121 - accuracy: 0.8188 - f1_metric: 0.8186\n",
      "Epoch 00075: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3133 - accuracy: 0.8193 - f1_metric: 0.8186 - val_loss: 0.5212 - val_accuracy: 0.7948 - val_f1_metric: 0.7949 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.3057 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.2992 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3008 - accuracy: 0.8264 - f1_metric: 0.82 - ETA: 0s - loss: 0.2908 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.3132 - accuracy: 0.8167 - f1_metric: 0.81 - ETA: 0s - loss: 0.3156 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3113 - accuracy: 0.8207 - f1_metric: 0.81 - ETA: 0s - loss: 0.3147 - accuracy: 0.8171 - f1_metric: 0.81 - ETA: 0s - loss: 0.3177 - accuracy: 0.8143 - f1_metric: 0.81 - ETA: 0s - loss: 0.3237 - accuracy: 0.8168 - f1_metric: 0.81 - ETA: 0s - loss: 0.3196 - accuracy: 0.8180 - f1_metric: 0.81 - ETA: 0s - loss: 0.3179 - accuracy: 0.8157 - f1_metric: 0.81 - ETA: 0s - loss: 0.3160 - accuracy: 0.8167 - f1_metric: 0.81 - ETA: 0s - loss: 0.3190 - accuracy: 0.8168 - f1_metric: 0.81 - ETA: 0s - loss: 0.3196 - accuracy: 0.8150 - f1_metric: 0.8144\n",
      "Epoch 00076: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3197 - accuracy: 0.8154 - f1_metric: 0.8151 - val_loss: 0.5197 - val_accuracy: 0.8015 - val_f1_metric: 0.8012 - lr: 1.0000e-04\n",
      "Epoch 77/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4680 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.2811 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2977 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.3080 - accuracy: 0.8294 - f1_metric: 0.82 - ETA: 0s - loss: 0.3064 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3111 - accuracy: 0.8238 - f1_metric: 0.82 - ETA: 0s - loss: 0.3096 - accuracy: 0.8266 - f1_metric: 0.82 - ETA: 0s - loss: 0.3151 - accuracy: 0.8236 - f1_metric: 0.82 - ETA: 0s - loss: 0.3105 - accuracy: 0.8212 - f1_metric: 0.82 - ETA: 0s - loss: 0.3127 - accuracy: 0.8167 - f1_metric: 0.81 - ETA: 0s - loss: 0.3133 - accuracy: 0.8187 - f1_metric: 0.81 - ETA: 0s - loss: 0.3184 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3176 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.3154 - accuracy: 0.8151 - f1_metric: 0.8146\n",
      "Epoch 00077: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.3151 - accuracy: 0.8147 - f1_metric: 0.8145 - val_loss: 0.5010 - val_accuracy: 0.8007 - val_f1_metric: 0.8005 - lr: 1.0000e-04\n",
      "Epoch 78/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3013 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3289 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.3328 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.3315 - accuracy: 0.8239 - f1_metric: 0.82 - ETA: 0s - loss: 0.3220 - accuracy: 0.8245 - f1_metric: 0.82 - ETA: 0s - loss: 0.3193 - accuracy: 0.8262 - f1_metric: 0.82 - ETA: 0s - loss: 0.3232 - accuracy: 0.8183 - f1_metric: 0.81 - ETA: 0s - loss: 0.3434 - accuracy: 0.8189 - f1_metric: 0.81 - ETA: 0s - loss: 0.3389 - accuracy: 0.8150 - f1_metric: 0.81 - ETA: 0s - loss: 0.3371 - accuracy: 0.8108 - f1_metric: 0.81 - ETA: 0s - loss: 0.3376 - accuracy: 0.8095 - f1_metric: 0.80 - ETA: 0s - loss: 0.3331 - accuracy: 0.8111 - f1_metric: 0.81 - ETA: 0s - loss: 0.3278 - accuracy: 0.8112 - f1_metric: 0.81 - ETA: 0s - loss: 0.3264 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3258 - accuracy: 0.8118 - f1_metric: 0.8111\n",
      "Epoch 00078: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3253 - accuracy: 0.8129 - f1_metric: 0.8133 - val_loss: 0.5034 - val_accuracy: 0.8099 - val_f1_metric: 0.8086 - lr: 1.0000e-04\n",
      "Epoch 79/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3609 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3526 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3835 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.3673 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3626 - accuracy: 0.7955 - f1_metric: 0.79 - ETA: 0s - loss: 0.3470 - accuracy: 0.8077 - f1_metric: 0.80 - ETA: 0s - loss: 0.3454 - accuracy: 0.7998 - f1_metric: 0.80 - ETA: 0s - loss: 0.3447 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3400 - accuracy: 0.7996 - f1_metric: 0.80 - ETA: 0s - loss: 0.3286 - accuracy: 0.8065 - f1_metric: 0.80 - ETA: 0s - loss: 0.3214 - accuracy: 0.8055 - f1_metric: 0.80 - ETA: 0s - loss: 0.3230 - accuracy: 0.8052 - f1_metric: 0.80 - ETA: 0s - loss: 0.3218 - accuracy: 0.8061 - f1_metric: 0.80 - ETA: 0s - loss: 0.3200 - accuracy: 0.8069 - f1_metric: 0.80 - ETA: 0s - loss: 0.3201 - accuracy: 0.8109 - f1_metric: 0.81 - ETA: 0s - loss: 0.3191 - accuracy: 0.8121 - f1_metric: 0.81 - ETA: 0s - loss: 0.3181 - accuracy: 0.8158 - f1_metric: 0.8159\n",
      "Epoch 00079: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3200 - accuracy: 0.8154 - f1_metric: 0.8153 - val_loss: 0.4868 - val_accuracy: 0.8116 - val_f1_metric: 0.8115 - lr: 1.0000e-04\n",
      "Epoch 80/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.5123 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.3412 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.3405 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.3922 - accuracy: 0.8013 - f1_metric: 0.80 - ETA: 0s - loss: 0.3597 - accuracy: 0.8078 - f1_metric: 0.80 - ETA: 0s - loss: 0.3426 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.3357 - accuracy: 0.8192 - f1_metric: 0.81 - ETA: 0s - loss: 0.3355 - accuracy: 0.8184 - f1_metric: 0.81 - ETA: 0s - loss: 0.3390 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.3352 - accuracy: 0.8162 - f1_metric: 0.81 - ETA: 0s - loss: 0.3285 - accuracy: 0.8092 - f1_metric: 0.80 - ETA: 0s - loss: 0.3270 - accuracy: 0.8079 - f1_metric: 0.80 - ETA: 0s - loss: 0.3267 - accuracy: 0.8083 - f1_metric: 0.80 - ETA: 0s - loss: 0.3209 - accuracy: 0.8087 - f1_metric: 0.80 - ETA: 0s - loss: 0.3202 - accuracy: 0.8043 - f1_metric: 0.80 - ETA: 0s - loss: 0.3181 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.3168 - accuracy: 0.8043 - f1_metric: 0.80 - ETA: 0s - loss: 0.3181 - accuracy: 0.8062 - f1_metric: 0.8058\n",
      "Epoch 00080: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3158 - accuracy: 0.8082 - f1_metric: 0.8083 - val_loss: 0.4582 - val_accuracy: 0.8291 - val_f1_metric: 0.8300 - lr: 1.0000e-04\n",
      "Epoch 81/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2623 - accuracy: 0.8633 - f1_metric: 0.86 - ETA: 0s - loss: 0.2739 - accuracy: 0.8482 - f1_metric: 0.84 - ETA: 0s - loss: 0.2904 - accuracy: 0.8531 - f1_metric: 0.85 - ETA: 0s - loss: 0.2968 - accuracy: 0.8451 - f1_metric: 0.84 - ETA: 0s - loss: 0.3002 - accuracy: 0.8404 - f1_metric: 0.84 - ETA: 0s - loss: 0.3065 - accuracy: 0.8410 - f1_metric: 0.84 - ETA: 0s - loss: 0.3125 - accuracy: 0.8445 - f1_metric: 0.84 - ETA: 0s - loss: 0.3104 - accuracy: 0.8411 - f1_metric: 0.84 - ETA: 0s - loss: 0.3123 - accuracy: 0.8426 - f1_metric: 0.84 - ETA: 0s - loss: 0.3104 - accuracy: 0.8421 - f1_metric: 0.84 - ETA: 0s - loss: 0.3092 - accuracy: 0.8443 - f1_metric: 0.84 - ETA: 0s - loss: 0.3114 - accuracy: 0.8396 - f1_metric: 0.83 - ETA: 0s - loss: 0.3113 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3126 - accuracy: 0.8351 - f1_metric: 0.83 - ETA: 0s - loss: 0.3136 - accuracy: 0.8293 - f1_metric: 0.82 - ETA: 0s - loss: 0.3136 - accuracy: 0.8270 - f1_metric: 0.8265\n",
      "Epoch 00081: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3142 - accuracy: 0.8258 - f1_metric: 0.8240 - val_loss: 0.5406 - val_accuracy: 0.7898 - val_f1_metric: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 82/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2871 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3154 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3223 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.2947 - accuracy: 0.8164 - f1_metric: 0.81 - ETA: 0s - loss: 0.3115 - accuracy: 0.8156 - f1_metric: 0.81 - ETA: 0s - loss: 0.3118 - accuracy: 0.8221 - f1_metric: 0.82 - ETA: 0s - loss: 0.3244 - accuracy: 0.8184 - f1_metric: 0.81 - ETA: 0s - loss: 0.3209 - accuracy: 0.8174 - f1_metric: 0.81 - ETA: 0s - loss: 0.3178 - accuracy: 0.8170 - f1_metric: 0.81 - ETA: 0s - loss: 0.3324 - accuracy: 0.8171 - f1_metric: 0.81 - ETA: 0s - loss: 0.3292 - accuracy: 0.8143 - f1_metric: 0.81 - ETA: 0s - loss: 0.3295 - accuracy: 0.8109 - f1_metric: 0.81 - ETA: 0s - loss: 0.3251 - accuracy: 0.8100 - f1_metric: 0.80 - ETA: 0s - loss: 0.3273 - accuracy: 0.8030 - f1_metric: 0.80 - ETA: 0s - loss: 0.3261 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3229 - accuracy: 0.8045 - f1_metric: 0.80 - ETA: 0s - loss: 0.3269 - accuracy: 0.8027 - f1_metric: 0.80 - ETA: 0s - loss: 0.3261 - accuracy: 0.8036 - f1_metric: 0.8033\n",
      "Epoch 00082: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3262 - accuracy: 0.8060 - f1_metric: 0.8066 - val_loss: 0.5119 - val_accuracy: 0.7940 - val_f1_metric: 0.7952 - lr: 1.0000e-04\n",
      "Epoch 83/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3200 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.3266 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3381 - accuracy: 0.7946 - f1_metric: 0.79 - ETA: 0s - loss: 0.3356 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3325 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.3259 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.3173 - accuracy: 0.8158 - f1_metric: 0.81 - ETA: 0s - loss: 0.3134 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3151 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.3122 - accuracy: 0.8112 - f1_metric: 0.81 - ETA: 0s - loss: 0.3098 - accuracy: 0.8108 - f1_metric: 0.81 - ETA: 0s - loss: 0.3095 - accuracy: 0.8115 - f1_metric: 0.81 - ETA: 0s - loss: 0.3141 - accuracy: 0.8144 - f1_metric: 0.81 - ETA: 0s - loss: 0.3139 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3134 - accuracy: 0.8149 - f1_metric: 0.81 - ETA: 0s - loss: 0.3145 - accuracy: 0.8118 - f1_metric: 0.8112\n",
      "Epoch 00083: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3155 - accuracy: 0.8107 - f1_metric: 0.8095 - val_loss: 0.5140 - val_accuracy: 0.7931 - val_f1_metric: 0.7947 - lr: 1.0000e-04\n",
      "Epoch 84/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2266 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2761 - accuracy: 0.8490 - f1_metric: 0.84 - ETA: 0s - loss: 0.3120 - accuracy: 0.8094 - f1_metric: 0.80 - ETA: 0s - loss: 0.3119 - accuracy: 0.7988 - f1_metric: 0.79 - ETA: 0s - loss: 0.2978 - accuracy: 0.8168 - f1_metric: 0.81 - ETA: 0s - loss: 0.2989 - accuracy: 0.8173 - f1_metric: 0.81 - ETA: 0s - loss: 0.3013 - accuracy: 0.8252 - f1_metric: 0.82 - ETA: 0s - loss: 0.3010 - accuracy: 0.8232 - f1_metric: 0.82 - ETA: 0s - loss: 0.3062 - accuracy: 0.8267 - f1_metric: 0.82 - ETA: 0s - loss: 0.3055 - accuracy: 0.8244 - f1_metric: 0.82 - ETA: 0s - loss: 0.3036 - accuracy: 0.8212 - f1_metric: 0.82 - ETA: 0s - loss: 0.3027 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3073 - accuracy: 0.8248 - f1_metric: 0.82 - ETA: 0s - loss: 0.3112 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3114 - accuracy: 0.8189 - f1_metric: 0.81 - ETA: 0s - loss: 0.3166 - accuracy: 0.8192 - f1_metric: 0.8189\n",
      "Epoch 00084: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3167 - accuracy: 0.8182 - f1_metric: 0.8169 - val_loss: 0.5193 - val_accuracy: 0.7906 - val_f1_metric: 0.7920 - lr: 1.0000e-04\n",
      "Epoch 85/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.7969 - f1_metric: 0.78 - ETA: 0s - loss: 0.2468 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.2579 - accuracy: 0.8482 - f1_metric: 0.84 - ETA: 0s - loss: 0.2613 - accuracy: 0.8559 - f1_metric: 0.85 - ETA: 0s - loss: 0.2761 - accuracy: 0.8490 - f1_metric: 0.84 - ETA: 0s - loss: 0.2742 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.2832 - accuracy: 0.8418 - f1_metric: 0.84 - ETA: 0s - loss: 0.2865 - accuracy: 0.8420 - f1_metric: 0.84 - ETA: 0s - loss: 0.3018 - accuracy: 0.8288 - f1_metric: 0.82 - ETA: 0s - loss: 0.3122 - accuracy: 0.8244 - f1_metric: 0.82 - ETA: 0s - loss: 0.3088 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3052 - accuracy: 0.8260 - f1_metric: 0.82 - ETA: 0s - loss: 0.3046 - accuracy: 0.8258 - f1_metric: 0.82 - ETA: 0s - loss: 0.3063 - accuracy: 0.8250 - f1_metric: 0.82 - ETA: 0s - loss: 0.3089 - accuracy: 0.8240 - f1_metric: 0.82 - ETA: 0s - loss: 0.3099 - accuracy: 0.8238 - f1_metric: 0.82 - ETA: 0s - loss: 0.3128 - accuracy: 0.8212 - f1_metric: 0.8209\n",
      "Epoch 00085: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3132 - accuracy: 0.8204 - f1_metric: 0.8193 - val_loss: 0.5068 - val_accuracy: 0.8032 - val_f1_metric: 0.8029 - lr: 1.0000e-04\n",
      "Epoch 86/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2682 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3920 - accuracy: 0.7708 - f1_metric: 0.77 - ETA: 0s - loss: 0.3289 - accuracy: 0.7839 - f1_metric: 0.78 - ETA: 0s - loss: 0.3074 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.3153 - accuracy: 0.8099 - f1_metric: 0.80 - ETA: 0s - loss: 0.3169 - accuracy: 0.8062 - f1_metric: 0.80 - ETA: 0s - loss: 0.3109 - accuracy: 0.8056 - f1_metric: 0.80 - ETA: 0s - loss: 0.3089 - accuracy: 0.8110 - f1_metric: 0.81 - ETA: 0s - loss: 0.3184 - accuracy: 0.8084 - f1_metric: 0.80 - ETA: 0s - loss: 0.3189 - accuracy: 0.8077 - f1_metric: 0.80 - ETA: 0s - loss: 0.3199 - accuracy: 0.8114 - f1_metric: 0.81 - ETA: 0s - loss: 0.3185 - accuracy: 0.8115 - f1_metric: 0.81 - ETA: 0s - loss: 0.3192 - accuracy: 0.8102 - f1_metric: 0.80 - ETA: 0s - loss: 0.3234 - accuracy: 0.8074 - f1_metric: 0.80 - ETA: 0s - loss: 0.3209 - accuracy: 0.8098 - f1_metric: 0.80 - ETA: 0s - loss: 0.3198 - accuracy: 0.8107 - f1_metric: 0.8105\n",
      "Epoch 00086: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3188 - accuracy: 0.8107 - f1_metric: 0.8105 - val_loss: 0.4995 - val_accuracy: 0.8124 - val_f1_metric: 0.8129 - lr: 1.0000e-04\n",
      "Epoch 87/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3223 - accuracy: 0.7930 - f1_metric: 0.79 - ETA: 0s - loss: 0.3287 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3217 - accuracy: 0.8156 - f1_metric: 0.81 - ETA: 0s - loss: 0.3148 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3232 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3157 - accuracy: 0.8289 - f1_metric: 0.82 - ETA: 0s - loss: 0.3136 - accuracy: 0.8288 - f1_metric: 0.82 - ETA: 0s - loss: 0.3148 - accuracy: 0.8268 - f1_metric: 0.82 - ETA: 0s - loss: 0.3129 - accuracy: 0.8275 - f1_metric: 0.82 - ETA: 0s - loss: 0.3156 - accuracy: 0.8233 - f1_metric: 0.82 - ETA: 0s - loss: 0.3209 - accuracy: 0.8221 - f1_metric: 0.82 - ETA: 0s - loss: 0.3184 - accuracy: 0.8196 - f1_metric: 0.81 - ETA: 0s - loss: 0.3183 - accuracy: 0.8129 - f1_metric: 0.81 - ETA: 0s - loss: 0.3155 - accuracy: 0.8121 - f1_metric: 0.81 - ETA: 0s - loss: 0.3139 - accuracy: 0.8094 - f1_metric: 0.80 - ETA: 0s - loss: 0.3090 - accuracy: 0.8103 - f1_metric: 0.8101\n",
      "Epoch 00087: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3095 - accuracy: 0.8114 - f1_metric: 0.8123 - val_loss: 0.4710 - val_accuracy: 0.8266 - val_f1_metric: 0.8262 - lr: 1.0000e-04\n",
      "Epoch 88/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.3505 - accuracy: 0.8164 - f1_metric: 0.81 - ETA: 0s - loss: 0.3282 - accuracy: 0.8307 - f1_metric: 0.83 - ETA: 0s - loss: 0.3087 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3064 - accuracy: 0.8381 - f1_metric: 0.83 - ETA: 0s - loss: 0.3159 - accuracy: 0.8365 - f1_metric: 0.83 - ETA: 0s - loss: 0.3138 - accuracy: 0.8375 - f1_metric: 0.83 - ETA: 0s - loss: 0.3148 - accuracy: 0.8325 - f1_metric: 0.83 - ETA: 0s - loss: 0.3118 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.3208 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3249 - accuracy: 0.8251 - f1_metric: 0.82 - ETA: 0s - loss: 0.3200 - accuracy: 0.8238 - f1_metric: 0.82 - ETA: 0s - loss: 0.3207 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.3188 - accuracy: 0.8237 - f1_metric: 0.82 - ETA: 0s - loss: 0.3184 - accuracy: 0.8236 - f1_metric: 0.82 - ETA: 0s - loss: 0.3208 - accuracy: 0.8201 - f1_metric: 0.82 - ETA: 0s - loss: 0.3209 - accuracy: 0.8186 - f1_metric: 0.8180\n",
      "Epoch 00088: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3209 - accuracy: 0.8186 - f1_metric: 0.8180 - val_loss: 0.5123 - val_accuracy: 0.8040 - val_f1_metric: 0.8035 - lr: 1.0000e-04\n",
      "Epoch 89/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.3428 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3414 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3157 - accuracy: 0.8172 - f1_metric: 0.81 - ETA: 0s - loss: 0.3131 - accuracy: 0.8259 - f1_metric: 0.82 - ETA: 0s - loss: 0.3173 - accuracy: 0.8108 - f1_metric: 0.81 - ETA: 0s - loss: 0.3053 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3037 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.3004 - accuracy: 0.8250 - f1_metric: 0.82 - ETA: 0s - loss: 0.2980 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.2973 - accuracy: 0.8250 - f1_metric: 0.82 - ETA: 0s - loss: 0.3052 - accuracy: 0.8262 - f1_metric: 0.82 - ETA: 0s - loss: 0.3041 - accuracy: 0.8273 - f1_metric: 0.82 - ETA: 0s - loss: 0.3021 - accuracy: 0.8252 - f1_metric: 0.82 - ETA: 0s - loss: 0.3040 - accuracy: 0.8232 - f1_metric: 0.82 - ETA: 0s - loss: 0.3028 - accuracy: 0.8236 - f1_metric: 0.8239\n",
      "Epoch 00089: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3028 - accuracy: 0.8236 - f1_metric: 0.8239 - val_loss: 0.4798 - val_accuracy: 0.8208 - val_f1_metric: 0.8208 - lr: 1.0000e-04\n",
      "Epoch 90/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2268 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2887 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3301 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3225 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3168 - accuracy: 0.8295 - f1_metric: 0.82 - ETA: 0s - loss: 0.3061 - accuracy: 0.8317 - f1_metric: 0.83 - ETA: 0s - loss: 0.3146 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3204 - accuracy: 0.8300 - f1_metric: 0.83 - ETA: 0s - loss: 0.3119 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.3083 - accuracy: 0.8308 - f1_metric: 0.83 - ETA: 0s - loss: 0.3082 - accuracy: 0.8294 - f1_metric: 0.82 - ETA: 0s - loss: 0.3148 - accuracy: 0.8310 - f1_metric: 0.83 - ETA: 0s - loss: 0.3109 - accuracy: 0.8302 - f1_metric: 0.83 - ETA: 0s - loss: 0.3105 - accuracy: 0.8271 - f1_metric: 0.82 - ETA: 0s - loss: 0.3110 - accuracy: 0.8263 - f1_metric: 0.82 - ETA: 0s - loss: 0.3095 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.3144 - accuracy: 0.8205 - f1_metric: 0.82 - ETA: 0s - loss: 0.3119 - accuracy: 0.8232 - f1_metric: 0.82 - ETA: 0s - loss: 0.3156 - accuracy: 0.8194 - f1_metric: 0.8192\n",
      "Epoch 00090: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3152 - accuracy: 0.8204 - f1_metric: 0.8212 - val_loss: 0.4993 - val_accuracy: 0.8107 - val_f1_metric: 0.8108 - lr: 1.0000e-04\n",
      "Epoch 91/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3309 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3091 - accuracy: 0.8406 - f1_metric: 0.84 - ETA: 0s - loss: 0.3374 - accuracy: 0.8103 - f1_metric: 0.81 - ETA: 0s - loss: 0.3213 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3243 - accuracy: 0.8182 - f1_metric: 0.81 - ETA: 0s - loss: 0.3159 - accuracy: 0.8161 - f1_metric: 0.81 - ETA: 0s - loss: 0.3230 - accuracy: 0.8104 - f1_metric: 0.81 - ETA: 0s - loss: 0.3280 - accuracy: 0.8116 - f1_metric: 0.81 - ETA: 0s - loss: 0.3272 - accuracy: 0.8133 - f1_metric: 0.81 - ETA: 0s - loss: 0.3277 - accuracy: 0.8159 - f1_metric: 0.81 - ETA: 0s - loss: 0.3272 - accuracy: 0.8161 - f1_metric: 0.81 - ETA: 0s - loss: 0.3273 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3253 - accuracy: 0.8150 - f1_metric: 0.81 - ETA: 0s - loss: 0.3205 - accuracy: 0.8189 - f1_metric: 0.81 - ETA: 0s - loss: 0.3139 - accuracy: 0.8205 - f1_metric: 0.82 - ETA: 0s - loss: 0.3136 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.3135 - accuracy: 0.8198 - f1_metric: 0.8199\n",
      "Epoch 00091: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3127 - accuracy: 0.8208 - f1_metric: 0.8219 - val_loss: 0.4889 - val_accuracy: 0.8183 - val_f1_metric: 0.8191 - lr: 1.0000e-04\n",
      "Epoch 92/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2342 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2971 - accuracy: 0.8490 - f1_metric: 0.84 - ETA: 0s - loss: 0.2855 - accuracy: 0.8568 - f1_metric: 0.85 - ETA: 0s - loss: 0.2918 - accuracy: 0.8484 - f1_metric: 0.84 - ETA: 0s - loss: 0.2915 - accuracy: 0.8353 - f1_metric: 0.83 - ETA: 0s - loss: 0.2919 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.2855 - accuracy: 0.8322 - f1_metric: 0.83 - ETA: 0s - loss: 0.2831 - accuracy: 0.8295 - f1_metric: 0.83 - ETA: 0s - loss: 0.2873 - accuracy: 0.8275 - f1_metric: 0.82 - ETA: 0s - loss: 0.2896 - accuracy: 0.8276 - f1_metric: 0.82 - ETA: 0s - loss: 0.2902 - accuracy: 0.8317 - f1_metric: 0.83 - ETA: 0s - loss: 0.2913 - accuracy: 0.8329 - f1_metric: 0.83 - ETA: 0s - loss: 0.2939 - accuracy: 0.8307 - f1_metric: 0.83 - ETA: 0s - loss: 0.2975 - accuracy: 0.8321 - f1_metric: 0.83 - ETA: 0s - loss: 0.3018 - accuracy: 0.8333 - f1_metric: 0.8335\n",
      "Epoch 00092: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3038 - accuracy: 0.8326 - f1_metric: 0.8322 - val_loss: 0.4989 - val_accuracy: 0.8107 - val_f1_metric: 0.8107 - lr: 1.0000e-04\n",
      "Epoch 93/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2997 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2983 - accuracy: 0.8164 - f1_metric: 0.81 - ETA: 0s - loss: 0.2906 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.2982 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.2983 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.2936 - accuracy: 0.8113 - f1_metric: 0.81 - ETA: 0s - loss: 0.3065 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3049 - accuracy: 0.8171 - f1_metric: 0.81 - ETA: 0s - loss: 0.3087 - accuracy: 0.8117 - f1_metric: 0.81 - ETA: 0s - loss: 0.3070 - accuracy: 0.8118 - f1_metric: 0.81 - ETA: 0s - loss: 0.3016 - accuracy: 0.8175 - f1_metric: 0.81 - ETA: 0s - loss: 0.3057 - accuracy: 0.8166 - f1_metric: 0.81 - ETA: 0s - loss: 0.3049 - accuracy: 0.8190 - f1_metric: 0.81 - ETA: 0s - loss: 0.3026 - accuracy: 0.8216 - f1_metric: 0.82 - ETA: 0s - loss: 0.3023 - accuracy: 0.8224 - f1_metric: 0.82 - ETA: 0s - loss: 0.2992 - accuracy: 0.8228 - f1_metric: 0.82 - ETA: 0s - loss: 0.2994 - accuracy: 0.8231 - f1_metric: 0.82 - ETA: 0s - loss: 0.3056 - accuracy: 0.8207 - f1_metric: 0.82 - ETA: 0s - loss: 0.3110 - accuracy: 0.8219 - f1_metric: 0.8222\n",
      "Epoch 00093: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3113 - accuracy: 0.8218 - f1_metric: 0.8219 - val_loss: 0.4929 - val_accuracy: 0.8199 - val_f1_metric: 0.8195 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.4229 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3401 - accuracy: 0.7760 - f1_metric: 0.77 - ETA: 0s - loss: 0.3242 - accuracy: 0.7786 - f1_metric: 0.77 - ETA: 0s - loss: 0.3272 - accuracy: 0.7832 - f1_metric: 0.78 - ETA: 0s - loss: 0.3272 - accuracy: 0.7750 - f1_metric: 0.77 - ETA: 0s - loss: 0.3306 - accuracy: 0.7839 - f1_metric: 0.78 - ETA: 0s - loss: 0.3359 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3393 - accuracy: 0.7858 - f1_metric: 0.78 - ETA: 0s - loss: 0.3327 - accuracy: 0.7919 - f1_metric: 0.79 - ETA: 0s - loss: 0.3268 - accuracy: 0.7976 - f1_metric: 0.79 - ETA: 0s - loss: 0.3252 - accuracy: 0.8030 - f1_metric: 0.80 - ETA: 0s - loss: 0.3214 - accuracy: 0.8081 - f1_metric: 0.80 - ETA: 0s - loss: 0.3272 - accuracy: 0.8092 - f1_metric: 0.80 - ETA: 0s - loss: 0.3244 - accuracy: 0.8110 - f1_metric: 0.81 - ETA: 0s - loss: 0.3240 - accuracy: 0.8107 - f1_metric: 0.81 - ETA: 0s - loss: 0.3194 - accuracy: 0.8121 - f1_metric: 0.81 - ETA: 0s - loss: 0.3211 - accuracy: 0.8141 - f1_metric: 0.81 - ETA: 0s - loss: 0.3188 - accuracy: 0.8162 - f1_metric: 0.8165\n",
      "Epoch 00094: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3186 - accuracy: 0.8147 - f1_metric: 0.8142 - val_loss: 0.5311 - val_accuracy: 0.7915 - val_f1_metric: 0.7910 - lr: 1.0000e-04\n",
      "Epoch 95/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3276 - accuracy: 0.7773 - f1_metric: 0.77 - ETA: 0s - loss: 0.3173 - accuracy: 0.7839 - f1_metric: 0.78 - ETA: 0s - loss: 0.3269 - accuracy: 0.8000 - f1_metric: 0.80 - ETA: 0s - loss: 0.3184 - accuracy: 0.8029 - f1_metric: 0.80 - ETA: 0s - loss: 0.3279 - accuracy: 0.8076 - f1_metric: 0.80 - ETA: 0s - loss: 0.3275 - accuracy: 0.8035 - f1_metric: 0.80 - ETA: 0s - loss: 0.3246 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.3186 - accuracy: 0.8043 - f1_metric: 0.80 - ETA: 0s - loss: 0.3171 - accuracy: 0.8113 - f1_metric: 0.81 - ETA: 0s - loss: 0.3199 - accuracy: 0.8108 - f1_metric: 0.81 - ETA: 0s - loss: 0.3166 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3166 - accuracy: 0.8115 - f1_metric: 0.81 - ETA: 0s - loss: 0.3129 - accuracy: 0.8130 - f1_metric: 0.81 - ETA: 0s - loss: 0.3092 - accuracy: 0.8159 - f1_metric: 0.81 - ETA: 0s - loss: 0.3074 - accuracy: 0.8153 - f1_metric: 0.81 - ETA: 0s - loss: 0.3033 - accuracy: 0.8216 - f1_metric: 0.8219\n",
      "Epoch 00095: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3083 - accuracy: 0.8211 - f1_metric: 0.8210 - val_loss: 0.4621 - val_accuracy: 0.8308 - val_f1_metric: 0.8317 - lr: 1.0000e-04\n",
      "Epoch 96/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.8750 - f1_metric: 0.87 - ETA: 0s - loss: 0.2759 - accuracy: 0.8646 - f1_metric: 0.86 - ETA: 0s - loss: 0.2680 - accuracy: 0.8469 - f1_metric: 0.84 - ETA: 0s - loss: 0.2804 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.2847 - accuracy: 0.8352 - f1_metric: 0.83 - ETA: 0s - loss: 0.2994 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3047 - accuracy: 0.8291 - f1_metric: 0.82 - ETA: 0s - loss: 0.3007 - accuracy: 0.8298 - f1_metric: 0.83 - ETA: 0s - loss: 0.2992 - accuracy: 0.8311 - f1_metric: 0.83 - ETA: 0s - loss: 0.3021 - accuracy: 0.8255 - f1_metric: 0.82 - ETA: 0s - loss: 0.2975 - accuracy: 0.8264 - f1_metric: 0.82 - ETA: 0s - loss: 0.2992 - accuracy: 0.8297 - f1_metric: 0.83 - ETA: 0s - loss: 0.3007 - accuracy: 0.8291 - f1_metric: 0.82 - ETA: 0s - loss: 0.3009 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2996 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3005 - accuracy: 0.8285 - f1_metric: 0.82 - ETA: 0s - loss: 0.3040 - accuracy: 0.8247 - f1_metric: 0.8247\n",
      "Epoch 00096: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3040 - accuracy: 0.8247 - f1_metric: 0.8247 - val_loss: 0.4992 - val_accuracy: 0.8132 - val_f1_metric: 0.8139 - lr: 1.0000e-04\n",
      "Epoch 97/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2892 - accuracy: 0.7865 - f1_metric: 0.78 - ETA: 0s - loss: 0.2757 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.2988 - accuracy: 0.8066 - f1_metric: 0.80 - ETA: 0s - loss: 0.2937 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.2895 - accuracy: 0.8216 - f1_metric: 0.82 - ETA: 0s - loss: 0.2847 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.2902 - accuracy: 0.8217 - f1_metric: 0.82 - ETA: 0s - loss: 0.3085 - accuracy: 0.8180 - f1_metric: 0.81 - ETA: 0s - loss: 0.3064 - accuracy: 0.8189 - f1_metric: 0.81 - ETA: 0s - loss: 0.3127 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3116 - accuracy: 0.8220 - f1_metric: 0.82 - ETA: 0s - loss: 0.3098 - accuracy: 0.8251 - f1_metric: 0.82 - ETA: 0s - loss: 0.3114 - accuracy: 0.8239 - f1_metric: 0.82 - ETA: 0s - loss: 0.3097 - accuracy: 0.8214 - f1_metric: 0.82 - ETA: 0s - loss: 0.3102 - accuracy: 0.8197 - f1_metric: 0.81 - ETA: 0s - loss: 0.3106 - accuracy: 0.8161 - f1_metric: 0.81 - ETA: 0s - loss: 0.3094 - accuracy: 0.8159 - f1_metric: 0.81 - ETA: 0s - loss: 0.3109 - accuracy: 0.8154 - f1_metric: 0.8152\n",
      "Epoch 00097: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.3120 - accuracy: 0.8157 - f1_metric: 0.8158 - val_loss: 0.5015 - val_accuracy: 0.8116 - val_f1_metric: 0.8111 - lr: 1.0000e-04\n",
      "Epoch 98/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2979 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2459 - accuracy: 0.8542 - f1_metric: 0.85 - ETA: 0s - loss: 0.2564 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2705 - accuracy: 0.8477 - f1_metric: 0.84 - ETA: 0s - loss: 0.2788 - accuracy: 0.8480 - f1_metric: 0.84 - ETA: 0s - loss: 0.2828 - accuracy: 0.8401 - f1_metric: 0.83 - ETA: 0s - loss: 0.2787 - accuracy: 0.8365 - f1_metric: 0.83 - ETA: 0s - loss: 0.2934 - accuracy: 0.8336 - f1_metric: 0.83 - ETA: 0s - loss: 0.2944 - accuracy: 0.8391 - f1_metric: 0.83 - ETA: 0s - loss: 0.3025 - accuracy: 0.8317 - f1_metric: 0.83 - ETA: 0s - loss: 0.3007 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.3107 - accuracy: 0.8257 - f1_metric: 0.82 - ETA: 0s - loss: 0.3071 - accuracy: 0.8265 - f1_metric: 0.82 - ETA: 0s - loss: 0.3065 - accuracy: 0.8255 - f1_metric: 0.82 - ETA: 0s - loss: 0.3071 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.3074 - accuracy: 0.8238 - f1_metric: 0.82 - ETA: 0s - loss: 0.3084 - accuracy: 0.8253 - f1_metric: 0.82 - ETA: 0s - loss: 0.3108 - accuracy: 0.8251 - f1_metric: 0.8251\n",
      "Epoch 00098: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3104 - accuracy: 0.8254 - f1_metric: 0.8256 - val_loss: 0.5085 - val_accuracy: 0.8032 - val_f1_metric: 0.8034 - lr: 1.0000e-04\n",
      "Epoch 99/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2492 - accuracy: 0.7969 - f1_metric: 0.80 - ETA: 0s - loss: 0.3137 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3207 - accuracy: 0.8145 - f1_metric: 0.81 - ETA: 0s - loss: 0.3130 - accuracy: 0.8224 - f1_metric: 0.82 - ETA: 0s - loss: 0.3167 - accuracy: 0.8192 - f1_metric: 0.81 - ETA: 0s - loss: 0.3051 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3113 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.3181 - accuracy: 0.8261 - f1_metric: 0.82 - ETA: 0s - loss: 0.3219 - accuracy: 0.8209 - f1_metric: 0.82 - ETA: 0s - loss: 0.3198 - accuracy: 0.8190 - f1_metric: 0.81 - ETA: 0s - loss: 0.3198 - accuracy: 0.8184 - f1_metric: 0.81 - ETA: 0s - loss: 0.3195 - accuracy: 0.8162 - f1_metric: 0.81 - ETA: 0s - loss: 0.3180 - accuracy: 0.8155 - f1_metric: 0.81 - ETA: 0s - loss: 0.3120 - accuracy: 0.8176 - f1_metric: 0.81 - ETA: 0s - loss: 0.3111 - accuracy: 0.8166 - f1_metric: 0.8168\n",
      "Epoch 00099: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3130 - accuracy: 0.8182 - f1_metric: 0.8191 - val_loss: 0.4983 - val_accuracy: 0.8107 - val_f1_metric: 0.8095 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2869 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.2842 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.2790 - accuracy: 0.8340 - f1_metric: 0.83 - ETA: 0s - loss: 0.2907 - accuracy: 0.8494 - f1_metric: 0.84 - ETA: 0s - loss: 0.2816 - accuracy: 0.8522 - f1_metric: 0.85 - ETA: 0s - loss: 0.2858 - accuracy: 0.8406 - f1_metric: 0.84 - ETA: 0s - loss: 0.3066 - accuracy: 0.8342 - f1_metric: 0.83 - ETA: 0s - loss: 0.3080 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.3035 - accuracy: 0.8314 - f1_metric: 0.83 - ETA: 0s - loss: 0.3026 - accuracy: 0.8339 - f1_metric: 0.83 - ETA: 0s - loss: 0.3060 - accuracy: 0.8324 - f1_metric: 0.83 - ETA: 0s - loss: 0.3054 - accuracy: 0.8306 - f1_metric: 0.83 - ETA: 0s - loss: 0.3063 - accuracy: 0.8300 - f1_metric: 0.83 - ETA: 0s - loss: 0.3101 - accuracy: 0.8290 - f1_metric: 0.82 - ETA: 0s - loss: 0.3118 - accuracy: 0.8266 - f1_metric: 0.82 - ETA: 0s - loss: 0.3081 - accuracy: 0.8263 - f1_metric: 0.8267\n",
      "Epoch 00100: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3118 - accuracy: 0.8254 - f1_metric: 0.8250 - val_loss: 0.5171 - val_accuracy: 0.8023 - val_f1_metric: 0.8025 - lr: 1.0000e-04\n",
      "Epoch 101/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3104 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.3172 - accuracy: 0.7786 - f1_metric: 0.77 - ETA: 0s - loss: 0.3153 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.3210 - accuracy: 0.7956 - f1_metric: 0.79 - ETA: 0s - loss: 0.3133 - accuracy: 0.8062 - f1_metric: 0.80 - ETA: 0s - loss: 0.3152 - accuracy: 0.8073 - f1_metric: 0.80 - ETA: 0s - loss: 0.3132 - accuracy: 0.8016 - f1_metric: 0.80 - ETA: 0s - loss: 0.3109 - accuracy: 0.8030 - f1_metric: 0.80 - ETA: 0s - loss: 0.3126 - accuracy: 0.8059 - f1_metric: 0.80 - ETA: 0s - loss: 0.3073 - accuracy: 0.8109 - f1_metric: 0.81 - ETA: 0s - loss: 0.3080 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3030 - accuracy: 0.8192 - f1_metric: 0.81 - ETA: 0s - loss: 0.3087 - accuracy: 0.8187 - f1_metric: 0.81 - ETA: 0s - loss: 0.3081 - accuracy: 0.8213 - f1_metric: 0.82 - ETA: 0s - loss: 0.3076 - accuracy: 0.8222 - f1_metric: 0.8220\n",
      "Epoch 00101: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3076 - accuracy: 0.8222 - f1_metric: 0.8220 - val_loss: 0.5016 - val_accuracy: 0.8107 - val_f1_metric: 0.8114 - lr: 1.0000e-04\n",
      "Epoch 102/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.2920 - accuracy: 0.7344 - f1_metric: 0.73 - ETA: 0s - loss: 0.2475 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2861 - accuracy: 0.7937 - f1_metric: 0.79 - ETA: 0s - loss: 0.2864 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.2819 - accuracy: 0.8172 - f1_metric: 0.81 - ETA: 0s - loss: 0.2901 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.2914 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3026 - accuracy: 0.8199 - f1_metric: 0.82 - ETA: 0s - loss: 0.3080 - accuracy: 0.8199 - f1_metric: 0.82 - ETA: 0s - loss: 0.3097 - accuracy: 0.8239 - f1_metric: 0.82 - ETA: 0s - loss: 0.3056 - accuracy: 0.8262 - f1_metric: 0.82 - ETA: 0s - loss: 0.3029 - accuracy: 0.8241 - f1_metric: 0.82 - ETA: 0s - loss: 0.3086 - accuracy: 0.8250 - f1_metric: 0.82 - ETA: 0s - loss: 0.3068 - accuracy: 0.8286 - f1_metric: 0.82 - ETA: 0s - loss: 0.3079 - accuracy: 0.8273 - f1_metric: 0.82 - ETA: 0s - loss: 0.3088 - accuracy: 0.8277 - f1_metric: 0.82 - ETA: 0s - loss: 0.3101 - accuracy: 0.8262 - f1_metric: 0.8261\n",
      "Epoch 00102: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3086 - accuracy: 0.8261 - f1_metric: 0.8259 - val_loss: 0.5265 - val_accuracy: 0.7956 - val_f1_metric: 0.7953 - lr: 1.0000e-04\n",
      "Epoch 103/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3350 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3172 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3048 - accuracy: 0.8194 - f1_metric: 0.81 - ETA: 0s - loss: 0.2971 - accuracy: 0.8239 - f1_metric: 0.82 - ETA: 0s - loss: 0.3022 - accuracy: 0.8214 - f1_metric: 0.82 - ETA: 0s - loss: 0.3080 - accuracy: 0.8107 - f1_metric: 0.81 - ETA: 0s - loss: 0.3054 - accuracy: 0.8117 - f1_metric: 0.81 - ETA: 0s - loss: 0.3041 - accuracy: 0.8166 - f1_metric: 0.81 - ETA: 0s - loss: 0.3060 - accuracy: 0.8138 - f1_metric: 0.81 - ETA: 0s - loss: 0.3028 - accuracy: 0.8170 - f1_metric: 0.81 - ETA: 0s - loss: 0.3056 - accuracy: 0.8201 - f1_metric: 0.81 - ETA: 0s - loss: 0.3096 - accuracy: 0.8189 - f1_metric: 0.81 - ETA: 0s - loss: 0.3087 - accuracy: 0.8209 - f1_metric: 0.82 - ETA: 0s - loss: 0.3094 - accuracy: 0.8215 - f1_metric: 0.82 - ETA: 0s - loss: 0.3108 - accuracy: 0.8212 - f1_metric: 0.8210\n",
      "Epoch 00103: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3104 - accuracy: 0.8215 - f1_metric: 0.8215 - val_loss: 0.5051 - val_accuracy: 0.8099 - val_f1_metric: 0.8072 - lr: 1.0000e-04\n",
      "Epoch 104/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3308 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.2908 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.2877 - accuracy: 0.8145 - f1_metric: 0.81 - ETA: 0s - loss: 0.2803 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2862 - accuracy: 0.8221 - f1_metric: 0.82 - ETA: 0s - loss: 0.2993 - accuracy: 0.8262 - f1_metric: 0.82 - ETA: 0s - loss: 0.2936 - accuracy: 0.8240 - f1_metric: 0.82 - ETA: 0s - loss: 0.3014 - accuracy: 0.8217 - f1_metric: 0.82 - ETA: 0s - loss: 0.2992 - accuracy: 0.8275 - f1_metric: 0.82 - ETA: 0s - loss: 0.3006 - accuracy: 0.8299 - f1_metric: 0.82 - ETA: 0s - loss: 0.3021 - accuracy: 0.8297 - f1_metric: 0.82 - ETA: 0s - loss: 0.3033 - accuracy: 0.8306 - f1_metric: 0.83 - ETA: 0s - loss: 0.3032 - accuracy: 0.8277 - f1_metric: 0.82 - ETA: 0s - loss: 0.3028 - accuracy: 0.8264 - f1_metric: 0.82 - ETA: 0s - loss: 0.3030 - accuracy: 0.8277 - f1_metric: 0.82 - ETA: 0s - loss: 0.3016 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3013 - accuracy: 0.8274 - f1_metric: 0.8273\n",
      "Epoch 00104: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3002 - accuracy: 0.8272 - f1_metric: 0.8270 - val_loss: 0.4994 - val_accuracy: 0.8124 - val_f1_metric: 0.8129 - lr: 1.0000e-04\n",
      "Epoch 105/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3166 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3128 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3499 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3478 - accuracy: 0.8212 - f1_metric: 0.82 - ETA: 0s - loss: 0.3442 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3313 - accuracy: 0.8135 - f1_metric: 0.81 - ETA: 0s - loss: 0.3235 - accuracy: 0.8160 - f1_metric: 0.81 - ETA: 0s - loss: 0.3232 - accuracy: 0.8156 - f1_metric: 0.81 - ETA: 0s - loss: 0.3182 - accuracy: 0.8200 - f1_metric: 0.81 - ETA: 0s - loss: 0.3159 - accuracy: 0.8181 - f1_metric: 0.81 - ETA: 0s - loss: 0.3167 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3120 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3105 - accuracy: 0.8182 - f1_metric: 0.81 - ETA: 0s - loss: 0.3089 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.3149 - accuracy: 0.8174 - f1_metric: 0.81 - ETA: 0s - loss: 0.3168 - accuracy: 0.8184 - f1_metric: 0.81 - ETA: 0s - loss: 0.3136 - accuracy: 0.8196 - f1_metric: 0.8192\n",
      "Epoch 00105: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3134 - accuracy: 0.8179 - f1_metric: 0.8175 - val_loss: 0.4885 - val_accuracy: 0.8174 - val_f1_metric: 0.8175 - lr: 1.0000e-04\n",
      "Epoch 106/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2938 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.3002 - accuracy: 0.8237 - f1_metric: 0.82 - ETA: 0s - loss: 0.2963 - accuracy: 0.8203 - f1_metric: 0.81 - ETA: 0s - loss: 0.2961 - accuracy: 0.8329 - f1_metric: 0.83 - ETA: 0s - loss: 0.2961 - accuracy: 0.8292 - f1_metric: 0.82 - ETA: 0s - loss: 0.3004 - accuracy: 0.8316 - f1_metric: 0.83 - ETA: 0s - loss: 0.2959 - accuracy: 0.8378 - f1_metric: 0.83 - ETA: 0s - loss: 0.2997 - accuracy: 0.8372 - f1_metric: 0.83 - ETA: 0s - loss: 0.3021 - accuracy: 0.8351 - f1_metric: 0.83 - ETA: 0s - loss: 0.3063 - accuracy: 0.8307 - f1_metric: 0.83 - ETA: 0s - loss: 0.3045 - accuracy: 0.8324 - f1_metric: 0.83 - ETA: 0s - loss: 0.3046 - accuracy: 0.8295 - f1_metric: 0.82 - ETA: 0s - loss: 0.3015 - accuracy: 0.8261 - f1_metric: 0.82 - ETA: 0s - loss: 0.3038 - accuracy: 0.8238 - f1_metric: 0.82 - ETA: 0s - loss: 0.3024 - accuracy: 0.8230 - f1_metric: 0.8226\n",
      "Epoch 00106: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3010 - accuracy: 0.8240 - f1_metric: 0.8245 - val_loss: 0.4781 - val_accuracy: 0.8241 - val_f1_metric: 0.8238 - lr: 1.0000e-04\n",
      "Epoch 107/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.7812 - f1_metric: 0.78 - ETA: 0s - loss: 0.3418 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3119 - accuracy: 0.8344 - f1_metric: 0.83 - ETA: 0s - loss: 0.3018 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.3192 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3169 - accuracy: 0.8323 - f1_metric: 0.83 - ETA: 0s - loss: 0.3053 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3041 - accuracy: 0.8341 - f1_metric: 0.83 - ETA: 0s - loss: 0.3005 - accuracy: 0.8336 - f1_metric: 0.83 - ETA: 0s - loss: 0.3045 - accuracy: 0.8323 - f1_metric: 0.83 - ETA: 0s - loss: 0.3049 - accuracy: 0.8319 - f1_metric: 0.83 - ETA: 0s - loss: 0.3155 - accuracy: 0.8301 - f1_metric: 0.82 - ETA: 0s - loss: 0.3165 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3106 - accuracy: 0.8313 - f1_metric: 0.83 - ETA: 0s - loss: 0.3101 - accuracy: 0.8299 - f1_metric: 0.8297\n",
      "Epoch 00107: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3114 - accuracy: 0.8287 - f1_metric: 0.8271 - val_loss: 0.5051 - val_accuracy: 0.8049 - val_f1_metric: 0.8048 - lr: 1.0000e-04\n",
      "Epoch 108/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3246 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3095 - accuracy: 0.7917 - f1_metric: 0.79 - ETA: 0s - loss: 0.3356 - accuracy: 0.7937 - f1_metric: 0.79 - ETA: 0s - loss: 0.3236 - accuracy: 0.8058 - f1_metric: 0.80 - ETA: 0s - loss: 0.3135 - accuracy: 0.8108 - f1_metric: 0.81 - ETA: 0s - loss: 0.3110 - accuracy: 0.8097 - f1_metric: 0.81 - ETA: 0s - loss: 0.3068 - accuracy: 0.8259 - f1_metric: 0.82 - ETA: 0s - loss: 0.3016 - accuracy: 0.8244 - f1_metric: 0.82 - ETA: 0s - loss: 0.3142 - accuracy: 0.8266 - f1_metric: 0.82 - ETA: 0s - loss: 0.3034 - accuracy: 0.8295 - f1_metric: 0.82 - ETA: 0s - loss: 0.3062 - accuracy: 0.8294 - f1_metric: 0.82 - ETA: 0s - loss: 0.3011 - accuracy: 0.8293 - f1_metric: 0.82 - ETA: 0s - loss: 0.3026 - accuracy: 0.8250 - f1_metric: 0.82 - ETA: 0s - loss: 0.3034 - accuracy: 0.8239 - f1_metric: 0.82 - ETA: 0s - loss: 0.3073 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3060 - accuracy: 0.8201 - f1_metric: 0.82 - ETA: 0s - loss: 0.3047 - accuracy: 0.8203 - f1_metric: 0.82 - ETA: 0s - loss: 0.3076 - accuracy: 0.8203 - f1_metric: 0.8205\n",
      "Epoch 00108: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3083 - accuracy: 0.8204 - f1_metric: 0.8209 - val_loss: 0.4863 - val_accuracy: 0.8157 - val_f1_metric: 0.8159 - lr: 1.0000e-04\n",
      "Epoch 109/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3489 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2937 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.2859 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.2858 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.2950 - accuracy: 0.8224 - f1_metric: 0.82 - ETA: 0s - loss: 0.2941 - accuracy: 0.8317 - f1_metric: 0.83 - ETA: 0s - loss: 0.2890 - accuracy: 0.8311 - f1_metric: 0.83 - ETA: 0s - loss: 0.3006 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3046 - accuracy: 0.8375 - f1_metric: 0.83 - ETA: 0s - loss: 0.2970 - accuracy: 0.8404 - f1_metric: 0.83 - ETA: 0s - loss: 0.2901 - accuracy: 0.8456 - f1_metric: 0.84 - ETA: 0s - loss: 0.3008 - accuracy: 0.8443 - f1_metric: 0.84 - ETA: 0s - loss: 0.3035 - accuracy: 0.8375 - f1_metric: 0.83 - ETA: 0s - loss: 0.3085 - accuracy: 0.8357 - f1_metric: 0.83 - ETA: 0s - loss: 0.3099 - accuracy: 0.8303 - f1_metric: 0.83 - ETA: 0s - loss: 0.3092 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3108 - accuracy: 0.8305 - f1_metric: 0.83 - ETA: 0s - loss: 0.3090 - accuracy: 0.8304 - f1_metric: 0.8299\n",
      "Epoch 00109: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3086 - accuracy: 0.8297 - f1_metric: 0.8291 - val_loss: 0.5195 - val_accuracy: 0.7990 - val_f1_metric: 0.7988 - lr: 1.0000e-04\n",
      "Epoch 110/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.7969 - f1_metric: 0.80 - ETA: 0s - loss: 0.3250 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3100 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2871 - accuracy: 0.8203 - f1_metric: 0.82 - ETA: 0s - loss: 0.2862 - accuracy: 0.8352 - f1_metric: 0.83 - ETA: 0s - loss: 0.2897 - accuracy: 0.8329 - f1_metric: 0.83 - ETA: 0s - loss: 0.2972 - accuracy: 0.8250 - f1_metric: 0.82 - ETA: 0s - loss: 0.2925 - accuracy: 0.8247 - f1_metric: 0.82 - ETA: 0s - loss: 0.3008 - accuracy: 0.8188 - f1_metric: 0.81 - ETA: 0s - loss: 0.2980 - accuracy: 0.8234 - f1_metric: 0.82 - ETA: 0s - loss: 0.2970 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2952 - accuracy: 0.8315 - f1_metric: 0.83 - ETA: 0s - loss: 0.2933 - accuracy: 0.8344 - f1_metric: 0.83 - ETA: 0s - loss: 0.2990 - accuracy: 0.8330 - f1_metric: 0.83 - ETA: 0s - loss: 0.3052 - accuracy: 0.8308 - f1_metric: 0.83 - ETA: 0s - loss: 0.3050 - accuracy: 0.8318 - f1_metric: 0.83 - ETA: 0s - loss: 0.3061 - accuracy: 0.8304 - f1_metric: 0.83 - ETA: 0s - loss: 0.3032 - accuracy: 0.8287 - f1_metric: 0.8280\n",
      "Epoch 00110: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3032 - accuracy: 0.8287 - f1_metric: 0.8280 - val_loss: 0.4932 - val_accuracy: 0.8191 - val_f1_metric: 0.8208 - lr: 1.0000e-04\n",
      "Epoch 111/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2068 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.3788 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.3417 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3209 - accuracy: 0.8368 - f1_metric: 0.83 - ETA: 0s - loss: 0.3171 - accuracy: 0.8224 - f1_metric: 0.82 - ETA: 0s - loss: 0.3090 - accuracy: 0.8192 - f1_metric: 0.81 - ETA: 0s - loss: 0.3188 - accuracy: 0.8213 - f1_metric: 0.82 - ETA: 0s - loss: 0.3158 - accuracy: 0.8265 - f1_metric: 0.82 - ETA: 0s - loss: 0.3187 - accuracy: 0.8267 - f1_metric: 0.82 - ETA: 0s - loss: 0.3211 - accuracy: 0.8249 - f1_metric: 0.82 - ETA: 0s - loss: 0.3194 - accuracy: 0.8251 - f1_metric: 0.82 - ETA: 0s - loss: 0.3166 - accuracy: 0.8233 - f1_metric: 0.82 - ETA: 0s - loss: 0.3175 - accuracy: 0.8218 - f1_metric: 0.82 - ETA: 0s - loss: 0.3195 - accuracy: 0.8226 - f1_metric: 0.82 - ETA: 0s - loss: 0.3202 - accuracy: 0.8216 - f1_metric: 0.82 - ETA: 0s - loss: 0.3226 - accuracy: 0.8153 - f1_metric: 0.81 - ETA: 0s - loss: 0.3163 - accuracy: 0.8199 - f1_metric: 0.8199\n",
      "Epoch 00111: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3134 - accuracy: 0.8208 - f1_metric: 0.8220 - val_loss: 0.4865 - val_accuracy: 0.8224 - val_f1_metric: 0.8221 - lr: 1.0000e-04\n",
      "Epoch 112/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.2574 - accuracy: 0.8398 - f1_metric: 0.83 - ETA: 0s - loss: 0.2951 - accuracy: 0.8326 - f1_metric: 0.83 - ETA: 0s - loss: 0.3006 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3080 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.3081 - accuracy: 0.8418 - f1_metric: 0.84 - ETA: 0s - loss: 0.3103 - accuracy: 0.8331 - f1_metric: 0.83 - ETA: 0s - loss: 0.3124 - accuracy: 0.8267 - f1_metric: 0.82 - ETA: 0s - loss: 0.3247 - accuracy: 0.8244 - f1_metric: 0.82 - ETA: 0s - loss: 0.3257 - accuracy: 0.8248 - f1_metric: 0.82 - ETA: 0s - loss: 0.3229 - accuracy: 0.8208 - f1_metric: 0.82 - ETA: 0s - loss: 0.3173 - accuracy: 0.8210 - f1_metric: 0.82 - ETA: 0s - loss: 0.3165 - accuracy: 0.8237 - f1_metric: 0.82 - ETA: 0s - loss: 0.3167 - accuracy: 0.8235 - f1_metric: 0.82 - ETA: 0s - loss: 0.3133 - accuracy: 0.8253 - f1_metric: 0.82 - ETA: 0s - loss: 0.3122 - accuracy: 0.8259 - f1_metric: 0.82 - ETA: 0s - loss: 0.3137 - accuracy: 0.8251 - f1_metric: 0.8252\n",
      "Epoch 00112: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3137 - accuracy: 0.8251 - f1_metric: 0.8252 - val_loss: 0.5152 - val_accuracy: 0.7990 - val_f1_metric: 0.7996 - lr: 1.0000e-04\n",
      "Epoch 113/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3511 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3154 - accuracy: 0.7943 - f1_metric: 0.79 - ETA: 0s - loss: 0.3256 - accuracy: 0.8086 - f1_metric: 0.80 - ETA: 0s - loss: 0.3199 - accuracy: 0.8109 - f1_metric: 0.81 - ETA: 0s - loss: 0.3182 - accuracy: 0.8173 - f1_metric: 0.81 - ETA: 0s - loss: 0.3112 - accuracy: 0.8223 - f1_metric: 0.82 - ETA: 0s - loss: 0.3110 - accuracy: 0.8238 - f1_metric: 0.82 - ETA: 0s - loss: 0.3085 - accuracy: 0.8242 - f1_metric: 0.82 - ETA: 0s - loss: 0.3060 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3059 - accuracy: 0.8227 - f1_metric: 0.82 - ETA: 0s - loss: 0.3023 - accuracy: 0.8233 - f1_metric: 0.82 - ETA: 0s - loss: 0.2992 - accuracy: 0.8276 - f1_metric: 0.82 - ETA: 0s - loss: 0.2959 - accuracy: 0.8308 - f1_metric: 0.83 - ETA: 0s - loss: 0.2981 - accuracy: 0.8331 - f1_metric: 0.83 - ETA: 0s - loss: 0.2988 - accuracy: 0.8346 - f1_metric: 0.83 - ETA: 0s - loss: 0.2998 - accuracy: 0.8358 - f1_metric: 0.8371\n",
      "Epoch 00113: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2998 - accuracy: 0.8358 - f1_metric: 0.8371 - val_loss: 0.4823 - val_accuracy: 0.8241 - val_f1_metric: 0.8242 - lr: 1.0000e-04\n",
      "Epoch 114/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2514 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2748 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.2722 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.2759 - accuracy: 0.8368 - f1_metric: 0.83 - ETA: 0s - loss: 0.2857 - accuracy: 0.8268 - f1_metric: 0.82 - ETA: 0s - loss: 0.2866 - accuracy: 0.8260 - f1_metric: 0.82 - ETA: 0s - loss: 0.2811 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.2816 - accuracy: 0.8304 - f1_metric: 0.83 - ETA: 0s - loss: 0.2858 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.2887 - accuracy: 0.8345 - f1_metric: 0.83 - ETA: 0s - loss: 0.2890 - accuracy: 0.8391 - f1_metric: 0.83 - ETA: 0s - loss: 0.2890 - accuracy: 0.8335 - f1_metric: 0.83 - ETA: 0s - loss: 0.2932 - accuracy: 0.8318 - f1_metric: 0.83 - ETA: 0s - loss: 0.3020 - accuracy: 0.8302 - f1_metric: 0.83 - ETA: 0s - loss: 0.3009 - accuracy: 0.8305 - f1_metric: 0.83 - ETA: 0s - loss: 0.2997 - accuracy: 0.8315 - f1_metric: 0.8316\n",
      "Epoch 00114: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.3010 - accuracy: 0.8312 - f1_metric: 0.8308 - val_loss: 0.5121 - val_accuracy: 0.8049 - val_f1_metric: 0.8045 - lr: 1.0000e-04\n",
      "Epoch 115/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2903 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3206 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.3095 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3027 - accuracy: 0.8301 - f1_metric: 0.83 - ETA: 0s - loss: 0.3105 - accuracy: 0.8196 - f1_metric: 0.81 - ETA: 0s - loss: 0.3034 - accuracy: 0.8203 - f1_metric: 0.82 - ETA: 0s - loss: 0.2961 - accuracy: 0.8208 - f1_metric: 0.82 - ETA: 0s - loss: 0.3169 - accuracy: 0.8117 - f1_metric: 0.81 - ETA: 0s - loss: 0.3118 - accuracy: 0.8159 - f1_metric: 0.81 - ETA: 0s - loss: 0.3114 - accuracy: 0.8175 - f1_metric: 0.81 - ETA: 0s - loss: 0.3032 - accuracy: 0.8227 - f1_metric: 0.82 - ETA: 0s - loss: 0.3050 - accuracy: 0.8257 - f1_metric: 0.82 - ETA: 0s - loss: 0.3057 - accuracy: 0.8259 - f1_metric: 0.82 - ETA: 0s - loss: 0.3098 - accuracy: 0.8211 - f1_metric: 0.82 - ETA: 0s - loss: 0.3101 - accuracy: 0.8199 - f1_metric: 0.81 - ETA: 0s - loss: 0.3098 - accuracy: 0.8196 - f1_metric: 0.8193\n",
      "Epoch 00115: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3103 - accuracy: 0.8172 - f1_metric: 0.8151 - val_loss: 0.4886 - val_accuracy: 0.8224 - val_f1_metric: 0.8223 - lr: 1.0000e-04\n",
      "Epoch 116/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3034 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2950 - accuracy: 0.8219 - f1_metric: 0.82 - ETA: 0s - loss: 0.3058 - accuracy: 0.8320 - f1_metric: 0.83 - ETA: 0s - loss: 0.3049 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3008 - accuracy: 0.8255 - f1_metric: 0.82 - ETA: 0s - loss: 0.3032 - accuracy: 0.8302 - f1_metric: 0.83 - ETA: 0s - loss: 0.3033 - accuracy: 0.8247 - f1_metric: 0.82 - ETA: 0s - loss: 0.3001 - accuracy: 0.8259 - f1_metric: 0.82 - ETA: 0s - loss: 0.3014 - accuracy: 0.8301 - f1_metric: 0.83 - ETA: 0s - loss: 0.3021 - accuracy: 0.8287 - f1_metric: 0.82 - ETA: 0s - loss: 0.2998 - accuracy: 0.8303 - f1_metric: 0.83 - ETA: 0s - loss: 0.2994 - accuracy: 0.8306 - f1_metric: 0.83 - ETA: 0s - loss: 0.3019 - accuracy: 0.8286 - f1_metric: 0.82 - ETA: 0s - loss: 0.3013 - accuracy: 0.8312 - f1_metric: 0.83 - ETA: 0s - loss: 0.3008 - accuracy: 0.8313 - f1_metric: 0.83 - ETA: 0s - loss: 0.3009 - accuracy: 0.8319 - f1_metric: 0.83 - ETA: 0s - loss: 0.3043 - accuracy: 0.8325 - f1_metric: 0.8326\n",
      "Epoch 00116: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3038 - accuracy: 0.8312 - f1_metric: 0.8300 - val_loss: 0.4982 - val_accuracy: 0.8149 - val_f1_metric: 0.8149 - lr: 1.0000e-04\n",
      "Epoch 117/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3090 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2927 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.2977 - accuracy: 0.8194 - f1_metric: 0.81 - ETA: 0s - loss: 0.2883 - accuracy: 0.8216 - f1_metric: 0.82 - ETA: 0s - loss: 0.2946 - accuracy: 0.8208 - f1_metric: 0.82 - ETA: 0s - loss: 0.3078 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.3045 - accuracy: 0.8195 - f1_metric: 0.81 - ETA: 0s - loss: 0.3007 - accuracy: 0.8210 - f1_metric: 0.82 - ETA: 0s - loss: 0.3043 - accuracy: 0.8190 - f1_metric: 0.81 - ETA: 0s - loss: 0.3092 - accuracy: 0.8183 - f1_metric: 0.81 - ETA: 0s - loss: 0.3124 - accuracy: 0.8163 - f1_metric: 0.81 - ETA: 0s - loss: 0.3148 - accuracy: 0.8145 - f1_metric: 0.81 - ETA: 0s - loss: 0.3183 - accuracy: 0.8153 - f1_metric: 0.81 - ETA: 0s - loss: 0.3175 - accuracy: 0.8153 - f1_metric: 0.81 - ETA: 0s - loss: 0.3167 - accuracy: 0.8147 - f1_metric: 0.81 - ETA: 0s - loss: 0.3138 - accuracy: 0.8146 - f1_metric: 0.81 - ETA: 0s - loss: 0.3165 - accuracy: 0.8110 - f1_metric: 0.81 - ETA: 0s - loss: 0.3175 - accuracy: 0.8064 - f1_metric: 0.8063\n",
      "Epoch 00117: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.3175 - accuracy: 0.8064 - f1_metric: 0.8063 - val_loss: 0.5292 - val_accuracy: 0.7940 - val_f1_metric: 0.7937 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/3000\n",
      "41/44 [==========================>...] - ETA: 0s - loss: 0.3207 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2914 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3329 - accuracy: 0.8099 - f1_metric: 0.80 - ETA: 0s - loss: 0.3330 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3414 - accuracy: 0.7995 - f1_metric: 0.79 - ETA: 0s - loss: 0.3274 - accuracy: 0.8036 - f1_metric: 0.80 - ETA: 0s - loss: 0.3285 - accuracy: 0.8033 - f1_metric: 0.80 - ETA: 0s - loss: 0.3265 - accuracy: 0.8043 - f1_metric: 0.80 - ETA: 0s - loss: 0.3197 - accuracy: 0.8110 - f1_metric: 0.81 - ETA: 0s - loss: 0.3237 - accuracy: 0.8138 - f1_metric: 0.81 - ETA: 0s - loss: 0.3230 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3241 - accuracy: 0.8145 - f1_metric: 0.81 - ETA: 0s - loss: 0.3205 - accuracy: 0.8149 - f1_metric: 0.81 - ETA: 0s - loss: 0.3213 - accuracy: 0.8160 - f1_metric: 0.81 - ETA: 0s - loss: 0.3199 - accuracy: 0.8146 - f1_metric: 0.81 - ETA: 0s - loss: 0.3186 - accuracy: 0.8159 - f1_metric: 0.8151\n",
      "Epoch 00118: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.3150 - accuracy: 0.8157 - f1_metric: 0.8163 - val_loss: 0.4985 - val_accuracy: 0.8082 - val_f1_metric: 0.8088 - lr: 1.0000e-04\n",
      "Epoch 119/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2594 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2839 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3160 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.3073 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3133 - accuracy: 0.8221 - f1_metric: 0.82 - ETA: 0s - loss: 0.3201 - accuracy: 0.8252 - f1_metric: 0.82 - ETA: 0s - loss: 0.3178 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3131 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3107 - accuracy: 0.8269 - f1_metric: 0.82 - ETA: 0s - loss: 0.3139 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3107 - accuracy: 0.8292 - f1_metric: 0.82 - ETA: 0s - loss: 0.3114 - accuracy: 0.8251 - f1_metric: 0.82 - ETA: 0s - loss: 0.3065 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3045 - accuracy: 0.8321 - f1_metric: 0.83 - ETA: 0s - loss: 0.3111 - accuracy: 0.8322 - f1_metric: 0.83 - ETA: 0s - loss: 0.3086 - accuracy: 0.8348 - f1_metric: 0.83 - ETA: 0s - loss: 0.3094 - accuracy: 0.8330 - f1_metric: 0.8320\n",
      "Epoch 00119: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3073 - accuracy: 0.8323 - f1_metric: 0.8311 - val_loss: 0.5138 - val_accuracy: 0.8065 - val_f1_metric: 0.8040 - lr: 1.0000e-04\n",
      "Epoch 120/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.3261 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3513 - accuracy: 0.8021 - f1_metric: 0.80 - ETA: 0s - loss: 0.2990 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.3125 - accuracy: 0.8105 - f1_metric: 0.81 - ETA: 0s - loss: 0.2968 - accuracy: 0.8082 - f1_metric: 0.80 - ETA: 0s - loss: 0.3103 - accuracy: 0.8029 - f1_metric: 0.80 - ETA: 0s - loss: 0.3043 - accuracy: 0.8115 - f1_metric: 0.81 - ETA: 0s - loss: 0.2979 - accuracy: 0.8189 - f1_metric: 0.81 - ETA: 0s - loss: 0.2988 - accuracy: 0.8156 - f1_metric: 0.81 - ETA: 0s - loss: 0.3025 - accuracy: 0.8168 - f1_metric: 0.81 - ETA: 0s - loss: 0.3003 - accuracy: 0.8177 - f1_metric: 0.81 - ETA: 0s - loss: 0.3061 - accuracy: 0.8206 - f1_metric: 0.82 - ETA: 0s - loss: 0.3060 - accuracy: 0.8227 - f1_metric: 0.82 - ETA: 0s - loss: 0.3016 - accuracy: 0.8271 - f1_metric: 0.82 - ETA: 0s - loss: 0.3066 - accuracy: 0.8267 - f1_metric: 0.82 - ETA: 0s - loss: 0.3047 - accuracy: 0.8268 - f1_metric: 0.82 - ETA: 0s - loss: 0.3080 - accuracy: 0.8277 - f1_metric: 0.82 - ETA: 0s - loss: 0.3087 - accuracy: 0.8281 - f1_metric: 0.8283\n",
      "Epoch 00120: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.3070 - accuracy: 0.8287 - f1_metric: 0.8294 - val_loss: 0.5237 - val_accuracy: 0.7990 - val_f1_metric: 0.7974 - lr: 1.0000e-04\n",
      "Epoch 121/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3027 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.3293 - accuracy: 0.8031 - f1_metric: 0.80 - ETA: 0s - loss: 0.3160 - accuracy: 0.7969 - f1_metric: 0.79 - ETA: 0s - loss: 0.3064 - accuracy: 0.8047 - f1_metric: 0.80 - ETA: 0s - loss: 0.2981 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3081 - accuracy: 0.8115 - f1_metric: 0.81 - ETA: 0s - loss: 0.3071 - accuracy: 0.8090 - f1_metric: 0.80 - ETA: 0s - loss: 0.2996 - accuracy: 0.8132 - f1_metric: 0.81 - ETA: 0s - loss: 0.3014 - accuracy: 0.8118 - f1_metric: 0.81 - ETA: 0s - loss: 0.2998 - accuracy: 0.8155 - f1_metric: 0.81 - ETA: 0s - loss: 0.2963 - accuracy: 0.8192 - f1_metric: 0.81 - ETA: 0s - loss: 0.2978 - accuracy: 0.8180 - f1_metric: 0.81 - ETA: 0s - loss: 0.2971 - accuracy: 0.8196 - f1_metric: 0.81 - ETA: 0s - loss: 0.2955 - accuracy: 0.8214 - f1_metric: 0.82 - ETA: 0s - loss: 0.2959 - accuracy: 0.8218 - f1_metric: 0.82 - ETA: 0s - loss: 0.2971 - accuracy: 0.8238 - f1_metric: 0.82 - ETA: 0s - loss: 0.2998 - accuracy: 0.8241 - f1_metric: 0.8236\n",
      "Epoch 00121: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3033 - accuracy: 0.8240 - f1_metric: 0.8234 - val_loss: 0.4776 - val_accuracy: 0.8275 - val_f1_metric: 0.8280 - lr: 1.0000e-04\n",
      "Epoch 122/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.3166 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.3183 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.3218 - accuracy: 0.8108 - f1_metric: 0.81 - ETA: 0s - loss: 0.3067 - accuracy: 0.8112 - f1_metric: 0.81 - ETA: 0s - loss: 0.3109 - accuracy: 0.8103 - f1_metric: 0.81 - ETA: 0s - loss: 0.3122 - accuracy: 0.8171 - f1_metric: 0.81 - ETA: 0s - loss: 0.3100 - accuracy: 0.8133 - f1_metric: 0.81 - ETA: 0s - loss: 0.3095 - accuracy: 0.8153 - f1_metric: 0.81 - ETA: 0s - loss: 0.3081 - accuracy: 0.8145 - f1_metric: 0.81 - ETA: 0s - loss: 0.3024 - accuracy: 0.8206 - f1_metric: 0.82 - ETA: 0s - loss: 0.3100 - accuracy: 0.8190 - f1_metric: 0.81 - ETA: 0s - loss: 0.3066 - accuracy: 0.8252 - f1_metric: 0.82 - ETA: 0s - loss: 0.3079 - accuracy: 0.8254 - f1_metric: 0.82 - ETA: 0s - loss: 0.3075 - accuracy: 0.8252 - f1_metric: 0.82 - ETA: 0s - loss: 0.3075 - accuracy: 0.8232 - f1_metric: 0.82 - ETA: 0s - loss: 0.3091 - accuracy: 0.8244 - f1_metric: 0.8254\n",
      "Epoch 00122: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3091 - accuracy: 0.8244 - f1_metric: 0.8254 - val_loss: 0.5072 - val_accuracy: 0.8023 - val_f1_metric: 0.8040 - lr: 1.0000e-04\n",
      "Epoch 123/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2857 - accuracy: 0.8125 - f1_metric: 0.81 - ETA: 0s - loss: 0.2987 - accuracy: 0.8151 - f1_metric: 0.81 - ETA: 0s - loss: 0.2818 - accuracy: 0.8301 - f1_metric: 0.83 - ETA: 0s - loss: 0.2818 - accuracy: 0.8328 - f1_metric: 0.83 - ETA: 0s - loss: 0.2876 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2854 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.2856 - accuracy: 0.8309 - f1_metric: 0.83 - ETA: 0s - loss: 0.2903 - accuracy: 0.8313 - f1_metric: 0.83 - ETA: 0s - loss: 0.2975 - accuracy: 0.8267 - f1_metric: 0.82 - ETA: 0s - loss: 0.2984 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2966 - accuracy: 0.8287 - f1_metric: 0.82 - ETA: 0s - loss: 0.2943 - accuracy: 0.8270 - f1_metric: 0.82 - ETA: 0s - loss: 0.3010 - accuracy: 0.8261 - f1_metric: 0.82 - ETA: 0s - loss: 0.2994 - accuracy: 0.8272 - f1_metric: 0.82 - ETA: 0s - loss: 0.2976 - accuracy: 0.8290 - f1_metric: 0.82 - ETA: 0s - loss: 0.2962 - accuracy: 0.8261 - f1_metric: 0.82 - ETA: 0s - loss: 0.2946 - accuracy: 0.8293 - f1_metric: 0.82 - ETA: 0s - loss: 0.2979 - accuracy: 0.8287 - f1_metric: 0.8288\n",
      "Epoch 00123: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.2979 - accuracy: 0.8287 - f1_metric: 0.8288 - val_loss: 0.4867 - val_accuracy: 0.8199 - val_f1_metric: 0.8204 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/3000\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2823 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2930 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2844 - accuracy: 0.8281 - f1_metric: 0.82 - ETA: 0s - loss: 0.2886 - accuracy: 0.8313 - f1_metric: 0.83 - ETA: 0s - loss: 0.2964 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.2955 - accuracy: 0.8404 - f1_metric: 0.84 - ETA: 0s - loss: 0.2976 - accuracy: 0.8340 - f1_metric: 0.83 - ETA: 0s - loss: 0.3046 - accuracy: 0.8322 - f1_metric: 0.83 - ETA: 0s - loss: 0.3071 - accuracy: 0.8303 - f1_metric: 0.82 - ETA: 0s - loss: 0.3007 - accuracy: 0.8325 - f1_metric: 0.83 - ETA: 0s - loss: 0.2984 - accuracy: 0.8265 - f1_metric: 0.82 - ETA: 0s - loss: 0.2934 - accuracy: 0.8261 - f1_metric: 0.82 - ETA: 0s - loss: 0.2932 - accuracy: 0.8253 - f1_metric: 0.82 - ETA: 0s - loss: 0.2922 - accuracy: 0.8256 - f1_metric: 0.82 - ETA: 0s - loss: 0.2957 - accuracy: 0.8273 - f1_metric: 0.82 - ETA: 0s - loss: 0.2951 - accuracy: 0.8294 - f1_metric: 0.8293\n",
      "Epoch 00124: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.2951 - accuracy: 0.8294 - f1_metric: 0.8293 - val_loss: 0.4718 - val_accuracy: 0.8275 - val_f1_metric: 0.8280 - lr: 1.0000e-04\n",
      "Epoch 125/3000\n",
      "42/44 [===========================>..] - ETA: 0s - loss: 0.2325 - accuracy: 0.8594 - f1_metric: 0.85 - ETA: 0s - loss: 0.2522 - accuracy: 0.8698 - f1_metric: 0.86 - ETA: 0s - loss: 0.2857 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.2839 - accuracy: 0.8628 - f1_metric: 0.86 - ETA: 0s - loss: 0.2926 - accuracy: 0.8464 - f1_metric: 0.84 - ETA: 0s - loss: 0.3002 - accuracy: 0.8427 - f1_metric: 0.84 - ETA: 0s - loss: 0.2940 - accuracy: 0.8498 - f1_metric: 0.85 - ETA: 0s - loss: 0.2911 - accuracy: 0.8492 - f1_metric: 0.84 - ETA: 0s - loss: 0.2960 - accuracy: 0.8485 - f1_metric: 0.84 - ETA: 0s - loss: 0.2918 - accuracy: 0.8516 - f1_metric: 0.85 - ETA: 0s - loss: 0.2947 - accuracy: 0.8459 - f1_metric: 0.84 - ETA: 0s - loss: 0.2957 - accuracy: 0.8403 - f1_metric: 0.84 - ETA: 0s - loss: 0.2950 - accuracy: 0.8395 - f1_metric: 0.83 - ETA: 0s - loss: 0.2958 - accuracy: 0.8412 - f1_metric: 0.84 - ETA: 0s - loss: 0.2999 - accuracy: 0.8418 - f1_metric: 0.84 - ETA: 0s - loss: 0.2982 - accuracy: 0.8400 - f1_metric: 0.8401\n",
      "Epoch 00125: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 25ms/step - loss: 0.2974 - accuracy: 0.8405 - f1_metric: 0.8403 - val_loss: 0.5068 - val_accuracy: 0.8015 - val_f1_metric: 0.8009 - lr: 1.0000e-04\n",
      "Epoch 126/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3307 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.3074 - accuracy: 0.8385 - f1_metric: 0.83 - ETA: 0s - loss: 0.2962 - accuracy: 0.8472 - f1_metric: 0.84 - ETA: 0s - loss: 0.2991 - accuracy: 0.8451 - f1_metric: 0.84 - ETA: 0s - loss: 0.3022 - accuracy: 0.8426 - f1_metric: 0.84 - ETA: 0s - loss: 0.2971 - accuracy: 0.8438 - f1_metric: 0.84 - ETA: 0s - loss: 0.2876 - accuracy: 0.8429 - f1_metric: 0.84 - ETA: 0s - loss: 0.2892 - accuracy: 0.8378 - f1_metric: 0.83 - ETA: 0s - loss: 0.2897 - accuracy: 0.8356 - f1_metric: 0.83 - ETA: 0s - loss: 0.3105 - accuracy: 0.8275 - f1_metric: 0.82 - ETA: 0s - loss: 0.3175 - accuracy: 0.8265 - f1_metric: 0.82 - ETA: 0s - loss: 0.3140 - accuracy: 0.8251 - f1_metric: 0.82 - ETA: 0s - loss: 0.3161 - accuracy: 0.8217 - f1_metric: 0.82 - ETA: 0s - loss: 0.3137 - accuracy: 0.8205 - f1_metric: 0.82 - ETA: 0s - loss: 0.3136 - accuracy: 0.8201 - f1_metric: 0.82 - ETA: 0s - loss: 0.3129 - accuracy: 0.8201 - f1_metric: 0.82 - ETA: 0s - loss: 0.3126 - accuracy: 0.8183 - f1_metric: 0.8183\n",
      "Epoch 00126: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.3122 - accuracy: 0.8190 - f1_metric: 0.8195 - val_loss: 0.4850 - val_accuracy: 0.8233 - val_f1_metric: 0.8231 - lr: 1.0000e-04\n",
      "Epoch 127/3000\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.7656 - f1_metric: 0.76 - ETA: 0s - loss: 0.2612 - accuracy: 0.8333 - f1_metric: 0.83 - ETA: 0s - loss: 0.2802 - accuracy: 0.8229 - f1_metric: 0.82 - ETA: 0s - loss: 0.2819 - accuracy: 0.8262 - f1_metric: 0.82 - ETA: 0s - loss: 0.2821 - accuracy: 0.8366 - f1_metric: 0.83 - ETA: 0s - loss: 0.2866 - accuracy: 0.8304 - f1_metric: 0.83 - ETA: 0s - loss: 0.2953 - accuracy: 0.8318 - f1_metric: 0.83 - ETA: 0s - loss: 0.2894 - accuracy: 0.8359 - f1_metric: 0.83 - ETA: 0s - loss: 0.2838 - accuracy: 0.8404 - f1_metric: 0.84 - ETA: 0s - loss: 0.2894 - accuracy: 0.8381 - f1_metric: 0.83 - ETA: 0s - loss: 0.2901 - accuracy: 0.8368 - f1_metric: 0.83 - ETA: 0s - loss: 0.2925 - accuracy: 0.8378 - f1_metric: 0.83 - ETA: 0s - loss: 0.2905 - accuracy: 0.8387 - f1_metric: 0.83 - ETA: 0s - loss: 0.2910 - accuracy: 0.8366 - f1_metric: 0.83 - ETA: 0s - loss: 0.2916 - accuracy: 0.8353 - f1_metric: 0.83 - ETA: 0s - loss: 0.2897 - accuracy: 0.8357 - f1_metric: 0.83 - ETA: 0s - loss: 0.2902 - accuracy: 0.8349 - f1_metric: 0.83 - ETA: 0s - loss: 0.2921 - accuracy: 0.8342 - f1_metric: 0.83 - ETA: 0s - loss: 0.2941 - accuracy: 0.8307 - f1_metric: 0.8310\n",
      "Epoch 00127: val_f1_metric did not improve from 0.83508\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.2940 - accuracy: 0.8305 - f1_metric: 0.8305 - val_loss: 0.4854 - val_accuracy: 0.8233 - val_f1_metric: 0.8232 - lr: 1.0000e-04\n",
      "Epoch 00127: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras evaluate= [0.4690910577774048, 0.8251256346702576, 0.8277374505996704]\n",
      "size of test set 995\n",
      "TP class counts (array([0, 1, 2], dtype=int64), array([ 44,  55, 722], dtype=int64))\n",
      "True class counts (array([0, 1, 2], dtype=int64), array([ 59,  60, 876], dtype=int64))\n",
      "Pred class counts (array([0, 1, 2], dtype=int64), array([119, 134, 742], dtype=int64))\n",
      "baseline acc: 88.04020100502512\n",
      "[[ 44   0  15]\n",
      " [  0  55   5]\n",
      " [ 75  79 722]]\n",
      "F1 score (weighted) 0.8492301168723444\n",
      "F1 score (macro) 0.6512840528990362\n",
      "F1 score (micro) 0.8251256281407036\n",
      "cohen's Kappa 0.4672464089213839\n",
      "precision of class 0 = 0.75\n",
      "precision of class 1 = 0.92\n",
      "precision of class 2 = 0.82\n",
      "precision avg 0.83\n",
      "labels already calculated\n",
      "TODAY WE SHOULD SELL !!!\n"
     ]
    }
   ],
   "source": [
    "#Applyng CNN for predicting Petrobras BUY SELL HOLD pattern\n",
    "# source https://towardsdatascience.com/stock-market-action-prediction-with-convnet-8689238feae3\n",
    "\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pandas_datareader import data as wb\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "\n",
    "\n",
    "# Load Pebrobras data from 2010 01 01 until now\n",
    "\n",
    "dataset=wb.DataReader('PETR4.SA', data_source='yahoo', start='2000-1-1')    #2005 best so far\n",
    "\n",
    "dataset = dataset.reset_index() #turn index into column\n",
    "\n",
    "###renaming columns to match the original code\n",
    "\n",
    "dataset.rename(columns={'Date':'timestamp'}, inplace=True)\n",
    "dataset.rename(columns={'Open':'open'}, inplace=True)\n",
    "dataset.rename(columns={'High':'high'}, inplace=True)\n",
    "dataset.rename(columns={'Close':'close'}, inplace=True)\n",
    "dataset.rename(columns={'Volume':'volume'}, inplace=True)\n",
    "dataset.rename(columns={'Low':'low'}, inplace=True)\n",
    "dataset.rename(columns={'Adj Close':'adjusted_close'}, inplace=True)\n",
    "\n",
    "colorder=['timestamp', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
    "\n",
    "dataset=dataset[colorder]\n",
    "\n",
    "#####create new features to construct the image\n",
    "\n",
    "DataGenerator('petra',dfinput=dataset.copy())  #save to a directory named 'outputs'\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "df=pd.read_csv('outputs\\\\df_petra.csv')\n",
    "\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "\n",
    "if 'dividend_amount' in df.columns:\n",
    "    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################# NORMALIZATION AND DEFINITION OF TRAIN TEST AND VALIDATION \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "list_features = list(df.loc[:, 'open':'eom_26'].columns)\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'open':'eom_26'].values, df['labels'].values, train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['labels'].values)\n",
    "                                                    \n",
    "\n",
    "if 0.7*x_train.shape[0] < 2500:\n",
    "    train_split = 0.8\n",
    "else:\n",
    "    train_split = 0.7\n",
    "# train_split = 0.7\n",
    "print('train_split =',train_split)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=train_split, test_size=1-train_split, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "                                                \n",
    "from pickle import dump\n",
    "\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)             \n",
    "\n",
    "x_main = x_train.copy()\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))\n",
    "\n",
    "dump(mm_scaler, open('scaler.pkl', 'wb'))  #saving scaler for using on new data\n",
    "\n",
    "\n",
    "num_features = 225  # should be a perfect square\n",
    "selection_method = 'all'\n",
    "topk = 330 if selection_method == 'all' else num_features\n",
    "# if train_split >= 0.8:\n",
    "#     topk = 400\n",
    "# else:\n",
    "#     topk = 300\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "if selection_method == 'anova' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(f_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)  \n",
    "    selected_features_anova = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(selected_features_anova)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    print(\"****************************************\")\n",
    "    \n",
    "if selection_method == 'mutual_info' or selection_method == 'all':\n",
    "    select_k_best = SelectKBest(mutual_info_classif, k=topk)\n",
    "    if selection_method != 'all':\n",
    "        x_train = select_k_best.fit_transform(x_main, y_train)\n",
    "        x_cv = select_k_best.transform(x_cv)\n",
    "        x_test = select_k_best.transform(x_test)\n",
    "    else:\n",
    "        select_k_best.fit(x_main, y_train)\n",
    "    selected_features_mic = itemgetter(*select_k_best.get_support(indices=True))(list_features)\n",
    "    print(len(selected_features_mic), selected_features_mic)\n",
    "    print(select_k_best.get_support(indices=True))\n",
    "    \n",
    "    \n",
    "    \n",
    "if selection_method == 'all':\n",
    "    common = list(set(selected_features_anova).intersection(selected_features_mic))\n",
    "    print(\"common selected featues\", len(common), common)\n",
    "    if len(common) < num_features:\n",
    "        raise Exception('number of common features found {} < {} required features. Increase \"topk variable\"'.format(len(common), num_features))\n",
    "    feat_idx = []\n",
    "    for c in common:\n",
    "        feat_idx.append(list_features.index(c))\n",
    "    feat_idx = sorted(feat_idx[0:num_features])            # This is to ensure that related features are in close proximity in the image, since I had appended similar type of indicators closely. Feature selection significantly improved the performance of the model.\n",
    "    print(feat_idx)\n",
    "        \n",
    "    \n",
    "if selection_method == 'all':\n",
    "    x_train = x_train[:, feat_idx]\n",
    "    x_cv = x_cv[:, feat_idx]\n",
    "    x_test = x_test[:, feat_idx]\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, \n",
    "                                                             y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))                                                                                                                        \n",
    "                                                             \n",
    "_labels, _counts = np.unique(y_train, return_counts=True)\n",
    "print(\"percentage of class 0 = {}, class 1 = {}\".format(_counts[0]/len(y_train) * 100, _counts[1]/len(y_train) * 100))        \n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "def get_sample_weights(y):\n",
    "    y = y.astype(int)  # compute_class_weight needs int labels\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y), y)    \n",
    "    print(\"real class weights are {}\".format(class_weights), np.unique(y))\n",
    "    print(\"value_counts\", np.unique(y, return_counts=True))\n",
    "    sample_weights = y.copy().astype(float)\n",
    "    for i in np.unique(y):\n",
    "        sample_weights[sample_weights == i] = class_weights[i]  # if i == 2 else 0.8 * class_weights[i]\n",
    "        # sample_weights = np.where(sample_weights == i, class_weights[int(i)], y_)\n",
    "    return sample_weights\n",
    "\n",
    "def reshape_as_image(x, img_width, img_height):\n",
    "    x_temp = np.zeros((len(x), img_height, img_width))\n",
    "    for i in range(x.shape[0]):\n",
    "        # print(type(x), type(x_temp), x.shape)\n",
    "        x_temp[i] = np.reshape(x[i], (img_height, img_width))\n",
    "    return x_temp\n",
    "\n",
    "def f1_weighted(y_true, y_pred):\n",
    "    y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)  # can use conf_mat[0, :], tf.slice()\n",
    "    # precision = TP/TP+FP, recall = TP/TP+FN\n",
    "    rows, cols = conf_mat.get_shape()\n",
    "    size = y_true_class.get_shape()[0]\n",
    "    precision = tf.constant([0, 0, 0])  # change this to use rows/cols as size\n",
    "    recall = tf.constant([0, 0, 0])\n",
    "    class_counts = tf.constant([0, 0, 0])\n",
    "    def get_precision(i, conf_mat):\n",
    "        print(\"prec check\", conf_mat, conf_mat[i, i], tf.reduce_sum(conf_mat[:, i]))\n",
    "        precision[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[:, i]))\n",
    "        recall[i].assign(conf_mat[i, i] / tf.reduce_sum(conf_mat[i, :]))\n",
    "        tf.add(i, 1)\n",
    "        return i, conf_mat, precision, recall\n",
    "    def tf_count(i):\n",
    "        elements_equal_to_value = tf.equal(y_true_class, i)\n",
    "        as_ints = tf.cast(elements_equal_to_value, tf.int32)\n",
    "        count = tf.reduce_sum(as_ints)\n",
    "        class_counts[i].assign(count)\n",
    "        tf.add(i, 1)\n",
    "        return count\n",
    "    def condition(i, conf_mat):\n",
    "        return tf.less(i, 3)\n",
    "    i = tf.constant(3)\n",
    "    i, conf_mat = tf.while_loop(condition, get_precision, [i, conf_mat])\n",
    "    i = tf.constant(3)\n",
    "    c = lambda i: tf.less(i, 3)\n",
    "    b = tf_count(i)\n",
    "    tf.while_loop(c, b, [i])\n",
    "    weights = tf.math.divide(class_counts, size)\n",
    "    numerators = tf.math.multiply(tf.math.multiply(precision, recall), tf.constant(2))\n",
    "    denominators = tf.math.add(precision, recall)\n",
    "    f1s = tf.math.divide(numerators, denominators)\n",
    "    weighted_f1 = tf.reduce_sum(f.math.multiply(f1s, weights))\n",
    "    return weighted_f1\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    this calculates precision & recall \n",
    "    \"\"\"\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # mistake: y_pred of 0.3 is also considered 1\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    # y_true_class = tf.math.argmax(y_true, axis=1, output_type=tf.dtypes.int32)\n",
    "    # y_pred_class = tf.math.argmax(y_pred, axis=1, output_type=tf.dtypes.int32)\n",
    "    # conf_mat = tf.math.confusion_matrix(y_true_class, y_pred_class)\n",
    "    # tf.Print(conf_mat, [conf_mat], \"confusion_matrix\")\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "get_custom_objects().update({\"f1_metric\": f1_metric, \"f1_weighted\": f1_weighted})\n",
    "\n",
    "\n",
    "sample_weights = get_sample_weights(y_train)\n",
    "print(\"Test sample_weights\")\n",
    "rand_idx = np.random.randint(0, 1000, 30)\n",
    "print(y_train[rand_idx])\n",
    "print(sample_weights[rand_idx])\n",
    "\n",
    "one_hot_enc = OneHotEncoder(sparse=False, categories='auto')  # , categories='auto'\n",
    "y_train = one_hot_enc.fit_transform(y_train.reshape(-1, 1))\n",
    "print(\"y_train\",y_train.shape)\n",
    "y_cv = one_hot_enc.transform(y_cv.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dim = int(np.sqrt(num_features))\n",
    "x_train = reshape_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "print(\"final shape of x, y train/test {} {} {} {}\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = rows = 3\n",
    "for i in range(1, columns*rows +1):\n",
    "    index = np.random.randint(len(x_train))\n",
    "    img = x_train[index]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('image_'+str(index)+'_class_'+str(np.argmax(y_train[index])), fontsize=10)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "params = {'batch_size': 80, 'conv2d_layers': {'conv2d_do_1': 0.015, 'conv2d_filters_1': 25, 'conv2d_kernel_size_1': 2, 'conv2d_mp_1': 2, 'conv2d_strides_1': 1, 'kernel_regularizer_1': 0.01, 'conv2d_do_2': 0.015, 'conv2d_filters_2': 12, 'conv2d_kernel_size_2': 2, 'conv2d_mp_2': 2, 'conv2d_strides_2': 2, 'kernel_regularizer_2': 0.01, 'layers': 'two'}, 'dense_layers': {'dense_do_1': 0.015, 'dense_nodes_1': 100, 'kernel_regularizer_1': 0.01, 'layers': 'one'}, 'epochs': 3000, 'lr': 0.001, 'optimizer': 'adam'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from functools import *\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "def f1_custom(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=1)\n",
    "    y_p = np.argmax(y_pred, axis=1)\n",
    "    f1_score(y_t, y_p, labels=None, average='weighted', sample_weight=None, zero_division='warn')\n",
    "\n",
    "def create_model_cnn(params):\n",
    "    model = Sequential()\n",
    "    print(\"Training with params {}\".format(params))\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    # x_train, y_train = get_data_cnn(df, df.head(1).iloc[0][\"timestamp\"])[0:2]\n",
    "    conv2d_layer1 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_1\"],\n",
    "                           params[\"conv2d_layers\"][\"conv2d_kernel_size_1\"],\n",
    "                           strides=params[\"conv2d_layers\"][\"conv2d_strides_1\"],\n",
    "                           kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_1\"]), \n",
    "                           padding='valid',activation=\"relu\", use_bias=True,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           input_shape=(x_train[0].shape[0],\n",
    "                                        x_train[0].shape[1], x_train[0].shape[2]))\n",
    "    model.add(conv2d_layer1)\n",
    "    if params[\"conv2d_layers\"]['conv2d_mp_1'] == 1:\n",
    "        model.add(MaxPool2D(pool_size=2))\n",
    "    model.add(Dropout(params['conv2d_layers']['conv2d_do_1']))\n",
    "    if params[\"conv2d_layers\"]['layers'] == 'two':\n",
    "        conv2d_layer2 = Conv2D(params[\"conv2d_layers\"][\"conv2d_filters_2\"],\n",
    "                               params[\"conv2d_layers\"][\"conv2d_kernel_size_2\"],\n",
    "                               strides=params[\"conv2d_layers\"][\"conv2d_strides_2\"],\n",
    "                               kernel_regularizer=regularizers.l2(params[\"conv2d_layers\"][\"kernel_regularizer_2\"]),\n",
    "                               padding='valid',activation=\"relu\", use_bias=True,\n",
    "                               kernel_initializer='glorot_uniform')\n",
    "        model.add(conv2d_layer2)\n",
    "        if params[\"conv2d_layers\"]['conv2d_mp_2'] == 1:\n",
    "            model.add(MaxPool2D(pool_size=2))\n",
    "        model.add(Dropout(params['conv2d_layers']['conv2d_do_2']))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(params['dense_layers'][\"dense_nodes_1\"], activation='relu'))\n",
    "    model.add(Dropout(params['dense_layers']['dense_do_1']))\n",
    "    if params['dense_layers'][\"layers\"] == 'two':\n",
    "        model.add(Dense(params['dense_layers'][\"dense_nodes_2\"], activation='relu', \n",
    "                        kernel_regularizer=params['dense_layers'][\"kernel_regularizer_1\"]))\n",
    "        model.add(Dropout(params['dense_layers']['dense_do_2']))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    if params[\"optimizer\"] == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    elif params[\"optimizer\"] == 'sgd':\n",
    "        optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    elif params[\"optimizer\"] == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_metric])\n",
    "    # from keras.utils.vis_utils import plot_model use this too for diagram with plot\n",
    "    # model.summary(print_fn=lambda x: print(x + '\\n'))\n",
    "    return model\n",
    "\n",
    "def check_baseline(pred, y_test):\n",
    "    print(\"size of test set\", len(y_test))\n",
    "    e = np.equal(pred, y_test)\n",
    "    print(\"TP class counts\", np.unique(y_test[e], return_counts=True))\n",
    "    print(\"True class counts\", np.unique(y_test, return_counts=True))\n",
    "    print(\"Pred class counts\", np.unique(pred, return_counts=True))\n",
    "    holds = np.unique(y_test, return_counts=True)[1][2]  # number 'hold' predictions\n",
    "    print(\"baseline acc:\", (holds/len(y_test)*100))\n",
    "    \n",
    "    \n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "\n",
    "model = create_model_cnn(params)      \n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras_petrobras')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_f1_metric', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='max', period=1)  # val_f1_metric\n",
    "                      \n",
    "#training model\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=64, shuffle=True,\n",
    "                            # validation_split=0.3,\n",
    "                            validation_data=(x_cv, y_cv),\n",
    "                            callbacks=[mcp, rlp, es]\n",
    "                            , sample_weight=sample_weights)\n",
    "                            \n",
    "                            \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train_loss', 'val_loss', 'f1', 'val_f1'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "test_res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"keras evaluate=\", test_res)\n",
    "pred = model.predict(x_test)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "check_baseline(pred_classes, y_test_classes)\n",
    "conf_mat = confusion_matrix(y_test_classes, pred_classes)\n",
    "print(conf_mat)\n",
    "labels = [0,1,2]\n",
    "# ax = sns.heatmap(conf_mat, xticklabels=labels, yticklabels=labels, annot=True)\n",
    "# ax.xaxis.set_ticks_position('top')\n",
    "f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='weighted', sample_weight=None)\n",
    "print(\"F1 score (weighted)\", f1_weighted)\n",
    "print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='macro', sample_weight=None))\n",
    "print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "         average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "prec = []\n",
    "for i, row in enumerate(conf_mat):\n",
    "    prec.append(np.round(row[i]/np.sum(row), 2))\n",
    "    print(\"precision of class {} = {}\".format(i, prec[i]))\n",
    "print(\"precision avg\", sum(prec)/len(prec))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TESTING FOR TODAY\n",
    "\n",
    "import os\n",
    "                            \n",
    "dataset=wb.DataReader('PETR4.SA', data_source='yahoo', start='2020-02-05')    \n",
    "\n",
    "dataset = dataset.reset_index() #turn index into column\n",
    "\n",
    "\n",
    "best_model_path = os.path.join('.', 'best_model_keras_petrobras')\n",
    "\n",
    "\n",
    "#renaming columns to match the code\n",
    "\n",
    "dataset.rename(columns={'Date':'timestamp'}, inplace=True)\n",
    "dataset.rename(columns={'Open':'open'}, inplace=True)\n",
    "dataset.rename(columns={'High':'high'}, inplace=True)\n",
    "dataset.rename(columns={'Close':'close'}, inplace=True)\n",
    "dataset.rename(columns={'Volume':'volume'}, inplace=True)\n",
    "dataset.rename(columns={'Low':'low'}, inplace=True)\n",
    "dataset.rename(columns={'Adj Close':'adjusted_close'}, inplace=True)                            \n",
    "                            \n",
    "colorder=['timestamp', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
    "\n",
    "dataset=dataset[colorder]\n",
    "                            \n",
    "                            \n",
    "DataGenerator('petra2',dfinput=dataset.copy())\n",
    "\n",
    "df=pd.read_csv('outputs\\\\df_petra2.csv')\n",
    "\n",
    "df['labels'] = df['labels'].astype(np.int8)\n",
    "\n",
    "if 'dividend_amount' in df.columns:\n",
    "    df.drop(columns=['dividend_amount', 'split_coefficient'], inplace=True)\n",
    "                            \n",
    "list_features = list(df.loc[:, 'open':'eom_26'].columns)\n",
    "                            \n",
    "x_new=df.loc[:,list_features].values  \n",
    "y_new=df.loc[:,'labels'].values\n",
    "                            \n",
    "#load scaler\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "loaded_scaler=load(open('scaler.pkl', 'rb'))\n",
    "\n",
    "x_new_norm=loaded_scaler.transform(x_new) \n",
    "\n",
    "x_new_data=x_new_norm[:,feat_idx]\n",
    "                            \n",
    "dim = int(np.sqrt(num_features))\n",
    "\n",
    "x_new_data= reshape_as_image(x_new_data, dim, dim)\n",
    "\n",
    "x_new_data= np.stack((x_new_data,) * 3, axis=-1)\n",
    "\n",
    "\n",
    "#model = load_model(best_model_path)\n",
    "\n",
    "\n",
    "pred = model.predict(x_new_data)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "\n",
    "confusion_matrix(y_new, pred_classes)\n",
    "\n",
    "\n",
    "labels_output={1: 'BUY', 0: 'SELL', 2: 'HOLD'}\n",
    "\n",
    "\n",
    "print('TODAY WE SHOULD',labels_output[pred_classes[-1]],'!!!')\n",
    "                            \n",
    "\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 0], dtype=int8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cdict = {0: 'red', 1: 'blue', 2: 'green'}\n",
    "labels_output={1: 'Buy', 0: 'Sell', 2: 'Hold'}\n",
    "\n",
    "from matplotlib import dates\n",
    "df.timestamp=dates.datestr2num(df.timestamp)\n",
    "df.timestamp=dates.num2date(df.timestamp)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in np.unique(y_new):\n",
    "  ix=np.where(y_new==i)\n",
    "  ax.scatter(df.iloc[ix]['timestamp'],df.iloc[ix]['close'],c=cdict[i], label=labels_output[i])\n",
    "\n",
    "ax.xaxis.set_tick_params(rotation=30, labelsize=10)\n",
    "ax.set_ylabel('Preo (reais)',fontsize=12)\n",
    "ax.legend()\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
